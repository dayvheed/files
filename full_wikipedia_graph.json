{
  "nodes": [
    {
      "id": "455220",
      "title": "Data engineering",
      "url": "https://en.wikipedia.org/wiki/Data_engineering",
      "summary": "Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science; which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing.\n\n"
    },
    {
      "id": "2381958",
      "title": "Conceptual model",
      "url": "https://en.wikipedia.org/wiki/Conceptual_model",
      "summary": "The term conceptual model refers to any model that is formed after a conceptualization or generalization process. Conceptual models are often abstractions of things in the real world, whether physical or social. Semantic studies are relevant to various stages of concept formation. Semantics is fundamentally a study of concepts, the meaning that thinking beings give to various elements of their experience.\n\n"
    },
    {
      "id": "639009",
      "title": "Agile software development",
      "url": "https://en.wikipedia.org/wiki/Agile_software_development",
      "summary": "In software development, agile practices (sometimes written \"Agile\") include requirements, discovery and solutions improvement through the collaborative effort of self-organizing and cross-functional teams with their customer(s)/end user(s). Popularized in the 2001 Manifesto for Agile Software Development, these values and principles were derived from, and underpin, a broad range of software development frameworks, including Scrum and Kanban.While there is much anecdotal evidence that adopting agile practices and values improves the effectiveness of software professionals, teams and organizations, the empirical evidence is mixed and hard to find."
    },
    {
      "id": "775",
      "title": "Algorithm",
      "url": "https://en.wikipedia.org/wiki/Algorithm",
      "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.  For example, social media recommender systems rely on heuristics in such a way that, although widely characterized as \"algorithms\" in 21st century popular media, cannot deliver correct results due to the nature of the problem.\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    {
      "id": "90451",
      "title": "Amazon (company)",
      "url": "https://en.wikipedia.org/wiki/Amazon_(company)",
      "summary": "Amazon.com, Inc., doing business as Amazon ( AM-\u0259-zon, UK also  AM-\u0259-z\u0259n), is an American multinational technology company focusing on e-commerce, cloud computing, online advertising, digital streaming, and artificial intelligence. It is considered one of the Big Five American technology companies; the other four are Alphabet (parent company of Google), Apple, Meta (parent company of Facebook), and Microsoft.\nAmazon was founded on July 5, 1994 by Jeff Bezos from his garage in Bellevue, Washington. The company initially was an online marketplace for books, but incrementally expanded into a multitude of product categories, a strategy that has earned it the moniker \"The Everything Store\".The company has multiple subsidiaries, including Amazon Web Services, providing cloud computing, Zoox, a self-driving car division, Kuiper Systems, a satellite Internet provider, and Amazon Lab126, a computer hardware R&D provider. Other subsidiaries include Ring, Twitch, IMDb, and Whole Foods Market. Its acquisition of Whole Foods in August 2017 for US$13.4 billion substantially increased its market share and presence as a physical retailer.Amazon has a reputation as a disruptor of industries through technological innovation and aggressive reinvestment of profits into capital expenditures. As of 2023, it is the world's largest online retailer and marketplace, smart speaker provider, cloud computing service through AWS, live-streaming service through Twitch, and Internet company as measured by revenue and market share. In 2021, it surpassed Walmart as the world's largest retailer outside of China, driven in large part by its paid subscription plan, Amazon Prime, which has close to 200 million subscribers worldwide. It is the second-largest private employer in the United States.As of October 2023, Amazon is the 12th-most visited website in the world with 82% of its traffic coming from the United States.Amazon also distributes a variety of downloadable and streaming content through its Amazon Prime Video, MGM+, Amazon Music, Twitch, and Audible units. It publishes books through its publishing arm, Amazon Publishing, film and television content through Amazon MGM Studios, including the Metro-Goldwyn-Mayer studio which acquired in March 2022. It also produces consumer electronics\u2014most notably, Kindle e-readers, Echo devices, Fire tablets, and Fire TVs.\nAmazon has been criticized for customer data collection practices, a toxic work culture, censorship, tax avoidance, and anti-competitive behavior.\n\n"
    },
    {
      "id": "61928892",
      "title": "Apache Airflow",
      "url": "https://en.wikipedia.org/wiki/Apache_Airflow",
      "summary": "Apache Airflow is an open-source workflow management platform for data engineering pipelines. It started at Airbnb in October 2014 as a solution to manage the company's increasingly complex workflows. Creating Airflow allowed Airbnb to programmatically author and schedule their workflows and monitor them via the built-in Airflow user interface. From the beginning, the project was made open source, becoming an Apache Incubator project in March 2016 and a top-level Apache Software Foundation project in January 2019.\nAirflow is written in Python, and workflows are created via Python scripts. Airflow is designed under the principle of \"configuration as code\". While other \"configuration as code\" workflow platforms exist using markup languages like XML, using Python allows developers to import libraries and classes to help them create their workflows."
    },
    {
      "id": "42164234",
      "title": "Apache Spark",
      "url": "https://en.wikipedia.org/wiki/Apache_Spark",
      "summary": "Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since."
    },
    {
      "id": "1164",
      "title": "Artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
      "summary": "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of other living beings, primarily of humans. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\nAI technology is widely used throughout industry, government, and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), interacting via human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Alan Turing was the first person to conduct substantial research in the field that he called Machine Intelligence. Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\n\n"
    },
    {
      "id": "27051151",
      "title": "Big data",
      "url": "https://en.wikipedia.org/wiki/Big_data",
      "summary": "Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.\"\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17\u00d7260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than \u20ac100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\""
    },
    {
      "id": "1364506",
      "title": "Binary data",
      "url": "https://en.wikipedia.org/wiki/Binary_data",
      "summary": "Binary data is data whose unit can take on only two possible states. These are often labelled as 0 and 1 in accordance with the binary numeral system and Boolean algebra.\nBinary data occurs in many different technical and scientific fields, where it can be called by different names including bit (binary digit) in computer science, truth value in mathematical logic and related domains and binary variable in statistics.\n\n"
    },
    {
      "id": "2371482",
      "title": "Business analysis",
      "url": "https://en.wikipedia.org/wiki/Business_analysis",
      "summary": "Business analysis is a professional discipline focused on identifying business needs and determining solutions to business problems. Solutions may include a software-systems development component, process improvements, or organizational changes, and may involve extensive analysis, strategic planning and policy development. A person dedicated to carrying out these tasks within an organization is called a business analyst or BA.Business analysts are not found solely within projects for developing software systems. They may also work across the organisation, solving business problems in consultation with business stakeholders. Whilst most of the work that business analysts do today relates to software development / solutions, this is due to the ongoing massive changes businesses all over the world are experiencing in their attempts to digitise.Although there are different role definitions, depending upon the organization, there does seem to be an area of common ground where most\nbusiness analysts work. The responsibilities appear to be:\n\nTo investigate business systems, taking a holistic view of the situation. This may include examining elements of the organisation structures and staff development issues as well as current processes and IT systems.\nTo evaluate actions to improve the operation of a business system. Again, this may require an examination of organisational structure and staff development needs, to ensure that they are in line with any proposed process redesign and IT system development.\nTo document the business requirements for the IT system support using appropriate documentation standards.In line with this, the core business analyst role could be defined as an internal consultancy role that has the responsibility for investigating business situations, identifying and evaluating options for improving business systems, defining requirements and ensuring the effective use of information systems in meeting the needs of the business."
    },
    {
      "id": "168387",
      "title": "Business intelligence",
      "url": "https://en.wikipedia.org/wiki/Business_intelligence",
      "summary": "Business intelligence comprises the strategies and technologies used by enterprises for the data analysis and management of business information. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. \nBI tools can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability, and help them take strategic decisions.Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an \"intelligence\" that cannot be derived from any singular set of data.Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as \"BI/DW\"\nor as \"BIDW\". A data warehouse contains a copy of analytical data that facilitates decision support.\n\n"
    },
    {
      "id": "1040299",
      "title": "Clive Finkelstein",
      "url": "https://en.wikipedia.org/wiki/Clive_Finkelstein",
      "summary": "Clive Finkelstein (born ca. 1939 died 9/12/2021) is an Australian computer scientist, known as the \"Father\" of information engineering methodology."
    },
    {
      "id": "19541494",
      "title": "Cloud computing",
      "url": "https://en.wikipedia.org/wiki/Cloud_computing",
      "summary": "Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each of which is a data center. Cloud computing relies on sharing of resources to achieve coherence and typically uses a pay-as-you-go model, which can help in reducing capital expenses but may also lead to unexpected operating expenses for users."
    },
    {
      "id": "5300",
      "title": "Computer data storage",
      "url": "https://en.wikipedia.org/wiki/Computer_data_storage",
      "summary": "Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.:\u200a15\u201316\u200aThe central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy,:\u200a468\u2013473\u200a which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally, the fast technologies are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".\nEven the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.\n\n"
    },
    {
      "id": "7077",
      "title": "Computer file",
      "url": "https://en.wikipedia.org/wiki/Computer_file",
      "summary": "In computing, a computer file is a resource for recording data on a computer storage device, primarily identified by its filename. Just as words can be written on paper, so can data be written to a computer file. Files can be shared with and transferred between computers and mobile devices via removable media, networks, or the Internet.\nDifferent types of computer files are designed for different purposes. A file may be designed to store an image, a written message, a video, a program, or any wide variety of other kinds of data. Certain files can store multiple data types at once.\nBy using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times.\nFiles are typically organized in a file system, which tracks file locations on the disk and enables user access.\n\n"
    },
    {
      "id": "5323",
      "title": "Computer science",
      "url": "https://en.wikipedia.org/wiki/Computer_science",
      "summary": "Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). Though more often considered an academic discipline, computer science is closely related to computer programming.Algorithms and data structures are central to computer science.\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human\u2013computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\n\n"
    },
    {
      "id": "7398",
      "title": "Computer security",
      "url": "https://en.wikipedia.org/wiki/Computer_security",
      "summary": "Computer security, cybersecurity, digital security or information technology security (IT security) is the protection of computer systems and networks from attacks by malicious actors that may result in unauthorized information disclosure, theft of, or damage to hardware, software, or data, as well as from the disruption or misdirection of the services they provide.The field is significant due to the expanded reliance on computer systems, the Internet, and wireless network standards such as Bluetooth and Wi-Fi. Also, due to the growth of smart devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is one of the most significant challenges of the contemporary world, due to both the complexity of information systems and the societies they support. Security is of especially high importance for systems that govern large-scale systems with far-reaching physical effects, such as power distribution, elections, and finance.\n\n"
    },
    {
      "id": "2720954",
      "title": "Data analysis",
      "url": "https://en.wikipedia.org/wiki/Data_analysis",
      "summary": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.Data mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.\n\n"
    },
    {
      "id": "34296790",
      "title": "Data infrastructure",
      "url": "https://en.wikipedia.org/wiki/Data_infrastructure",
      "summary": "A data infrastructure is a digital infrastructure promoting data sharing and consumption. \nSimilarly to other infrastructures, it is a structure needed for the operation of a society as well as the services and facilities necessary for an economy to function, the data economy in this case.\n\n"
    },
    {
      "id": "46626475",
      "title": "Data lake",
      "url": "https://en.wikipedia.org/wiki/Data_lake",
      "summary": "A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and binary data (images, audio, video). A data lake can be established \"on premises\" (within an organization's data centers) or \"in the cloud\" (using cloud services from vendors such as Amazon, Microsoft, Oracle Cloud, or Google).\n\n"
    },
    {
      "id": "82871",
      "title": "Data model",
      "url": "https://en.wikipedia.org/wiki/Data_model",
      "summary": "A data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner.\nThe corresponding professional activity is called generally data modeling or, more specifically, database design.\nData models are typically specified by a data expert, data specialist, data scientist, data librarian, or a data scholar. \nA data modeling language and notation are often represented in graphical form as diagrams.A data model can sometimes be referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models.\nA data model explicitly determines the structure of data; conversely, structured data is data organized according to an explicit data model or data structure. Structured data is in contrast to unstructured data and semi-structured data.\n\n"
    },
    {
      "id": "759422",
      "title": "Data modeling",
      "url": "https://en.wikipedia.org/wiki/Data_modeling",
      "summary": "Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. It may be applied as part of broader Model-driven engineering (MDD) concept.\n\n"
    },
    {
      "id": "41961",
      "title": "Data processing",
      "url": "https://en.wikipedia.org/wiki/Data_processing",
      "summary": "Data processing is the collection and manipulation of digital data to produce meaningful information. Data processing is a form of information processing, which is the modification (processing) of information in any manner detectable by an observer.\n\n"
    },
    {
      "id": "237536",
      "title": "Information privacy",
      "url": "https://en.wikipedia.org/wiki/Information_privacy",
      "summary": "Information privacy is the relationship between the collection and dissemination of data, technology, the public expectation of privacy, contextual information norms, and the legal and political issues surrounding them.  It is also known as data privacy or data protection.\n\n"
    },
    {
      "id": "35458904",
      "title": "Data science",
      "url": "https://en.wikipedia.org/wiki/Data_science",
      "summary": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.A  data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.\n\n"
    },
    {
      "id": "7990",
      "title": "Data warehouse",
      "url": "https://en.wikipedia.org/wiki/Data_warehouse",
      "summary": "In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis and is considered a core component of business intelligence. Data warehouses are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise. This is beneficial for companies as it enables them to interrogate and draw insights from their data and make decisions.\nThe data stored in the warehouse is uploaded from the operational systems (such as marketing or sales). The data may pass through an operational data store and may require data cleansing for additional operations to ensure data quality before it is used in the data warehouse for reporting. \nExtract, transform, load (ETL) and extract, load, transform (ELT) are the two main approaches used to build a data warehouse system.\n\n"
    },
    {
      "id": "8078610",
      "title": "Database administration",
      "url": "https://en.wikipedia.org/wiki/Database_administration",
      "summary": "Database administration is the function of managing and maintaining database management systems (DBMS) software. Mainstream DBMS software such as Oracle, IBM Db2 and Microsoft SQL Server need ongoing management. As such, corporations that use DBMS software often hire specialized information technology personnel called database administrators or DBAs.\n\n"
    },
    {
      "id": "1040387",
      "title": "Database design",
      "url": "https://en.wikipedia.org/wiki/Database_design",
      "summary": "Database design is the organization of data according to a database model. The designer determines what data must be stored and how the data elements interrelate. With this information, they can begin to fit the data to the database model.\nA database management system manages the data accordingly.\nDatabase design involves classifying data and identifying interrelationships. This theoretical representation of the data is called an ontology.\n\n"
    },
    {
      "id": "8377",
      "title": "Database",
      "url": "https://en.wikipedia.org/wiki/Database",
      "summary": "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\nSmall databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.\nComputer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.\n\n"
    },
    {
      "id": "1079396",
      "title": "Dataflow programming",
      "url": "https://en.wikipedia.org/wiki/Dataflow_programming",
      "summary": "In computer programming, dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. Dataflow programming languages share some features of functional languages, and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing. Some authors use the term datastream instead of dataflow to avoid confusion with dataflow computing or dataflow architecture, based on an indeterministic machine paradigm. Dataflow programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.\n\n"
    },
    {
      "id": "32472154",
      "title": "Deep learning",
      "url": "https://en.wikipedia.org/wiki/Deep_learning",
      "summary": "Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog. ANNs are generally seen as low quality models for brain function."
    },
    {
      "id": "204002",
      "title": "Directed acyclic graph",
      "url": "https://en.wikipedia.org/wiki/Directed_acyclic_graph",
      "summary": "In mathematics, particularly graph theory, and computer science, a directed acyclic graph (DAG) is a directed graph with no directed cycles. That is, it consists of vertices and edges (also called arcs), with each edge directed from one vertex to another, such that following those directions will never form a closed loop. A directed graph is a DAG if and only if it can be topologically ordered, by arranging the vertices as a linear ordering that is consistent with all edge directions. DAGs have numerous scientific and computational applications, ranging from biology (evolution, family trees, epidemiology) to information science (citation networks) to computation (scheduling).\nDirected acyclic graphs are sometimes instead called acyclic directed graphs or acyclic digraphs."
    },
    {
      "id": "19721986",
      "title": "Directed graph",
      "url": "https://en.wikipedia.org/wiki/Directed_graph",
      "summary": "In mathematics, and more specifically in graph theory, a directed graph (or digraph) is a graph that is made up of a set of vertices connected by directed edges, often called arcs.\n\n"
    },
    {
      "id": "422994",
      "title": "Digital object identifier",
      "url": "https://en.wikipedia.org/wiki/Digital_object_identifier",
      "summary": "A digital object identifier (DOI) is a persistent identifier or handle used to uniquely identify various objects, standardized by the International Organization for Standardization (ISO). DOIs are an implementation of the Handle System; they also fit within the URI system (Uniform Resource Identifier). They are widely used to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. \nA DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL where the object is located. Thus, by being actionable and interoperable, a DOI differs from ISBNs or ISRCs which are identifiers only. The DOI system uses the indecs Content Model for representing metadata.\nThe DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL. It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link, leaving the DOI useless.The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. The DOI system is implemented through a federation of registration agencies coordinated by the IDF. By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations, and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations."
    },
    {
      "id": "239516",
      "title": "Extract, transform, load",
      "url": "https://en.wikipedia.org/wiki/Extract,_transform,_load",
      "summary": "In computing, extract, transform, load (ETL) is a three-phase process where data is extracted, transformed (cleaned, sanitized, scrubbed) and loaded into an output data container. The data can be collated from one or more sources and it can also be output to one or more destinations. ETL processing is typically executed using software applications but it can also be done manually by system operators. ETL software typically automates the entire process and can be run manually or on reccurring schedules either as single jobs or aggregated into a batch of jobs.\n\nA properly designed ETL system extracts data from source systems and enforces data type and data validity standards and ensures it conforms structurally to the requirements of the output. Some ETL systems can also deliver data in a presentation-ready format so that application developers can build applications and end users can make decisions.The ETL process is often used in data warehousing. ETL systems commonly integrate data from multiple applications (systems), typically developed and supported by different vendors or hosted on separate computer hardware. The separate systems containing the original data are frequently managed and operated by different stakeholders. For example, a cost accounting system may combine data from payroll, sales, and purchasing.\nData extraction involves extracting data from homogeneous or heterogeneous sources; data transformation processes data by data cleaning and transforming it into a proper storage format/structure for the purposes of querying and analysis; finally, data loading describes the insertion of data into the final target database such as an operational data store, a data mart, data lake or a data warehouse.\n\n"
    },
    {
      "id": "5603080",
      "title": "Face book",
      "url": "https://en.wikipedia.org/wiki/Face_book",
      "summary": "A face book or facebook is a paper or online directory of individuals' photographs and names published by some American universities. In particular, the term denotes publications of this type distributed by university administrations at the start of the academic year, with the intention of helping students to get to know each other."
    },
    {
      "id": "1010280",
      "title": "File system",
      "url": "https://en.wikipedia.org/wiki/File_system",
      "summary": "In computing, a file system or filesystem (often abbreviated to fs) is a method and data structure that the operating system uses to control how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stopped and the next began, or where any piece of data was located when it was time to retrieve it. By separating the data into pieces and giving each piece a name, the data are easily isolated and identified. Taking its name from the way a paper-based data management system is named, each group of data is called a \"file\". The structure and logic rules used to manage the groups of data and their names is called a \"file system.\"\nThere are many kinds of file systems, each with unique structure and logic, properties of speed, flexibility, security, size and more. Some file systems have been designed to be used for specific applications. For example, the ISO 9660 and UDF file systems are designed specifically for optical discs.\nFile systems can be used on many types of storage devices using various media. Introduced by IBM in 1956, HDDs (hard disk drives) beginning in the early 1960s were, and still are, the dominant secondary storage device for general-purpose computers and are projected to remain so for the foreseeable future. Other kinds of media that are used include SSDs, magnetic tapes, and optical discs. In some cases, such as with tmpfs, the computer's main memory (random-access memory, RAM) is used to create a temporary file system for short-term use.\n\nSome file systems are used on local data storage devices; others provide file access via a network protocol (for example, NFS, SMB, or 9P clients).  Some file systems are \"virtual\", meaning that the supplied \"files\" (called virtual files) are computed on request (such as procfs and sysfs) or are merely a mapping into a different file system used as a backing store.  The file system manages access to both the content of files and the metadata about those files.  It is responsible for arranging storage space; reliability, efficiency, and tuning with regard to the physical storage medium are important design considerations."
    },
    {
      "id": "13777",
      "title": "Hard disk drive",
      "url": "https://en.wikipedia.org/wiki/Hard_disk_drive",
      "summary": "A hard disk drive (HDD), hard disk, hard drive, or fixed disk, is an electro-mechanical data storage device that stores and retrieves digital data using magnetic storage with one or more rigid rapidly rotating platters coated with magnetic material. The platters are paired with magnetic heads, usually arranged on a moving actuator arm, which read and write data to the platter surfaces. Data is accessed in a random-access manner, meaning that individual blocks of data can be stored and retrieved in any order. HDDs are a type of non-volatile storage, retaining stored data when powered off. Modern HDDs are typically in the form of a small rectangular box.\nIntroduced by IBM in 1956, HDDs were the dominant secondary storage device for general-purpose computers beginning in the early 1960s. HDDs maintained this position into the modern era of servers and personal computers, though personal computing devices produced in large volume, like mobile phones and tablets, rely on flash memory storage devices. More than 224 companies have produced HDDs historically, though after extensive industry consolidation, most units are manufactured by Seagate, Toshiba, and Western Digital. HDDs dominate the volume of storage produced (exabytes per year) for servers. Though production is growing slowly (by exabytes shipped), sales revenues and unit shipments are declining, because solid-state drives (SSDs) have higher data-transfer rates, higher areal storage density, somewhat better reliability, and much lower latency and access times.The revenues for SSDs, most of which use NAND flash memory, slightly exceeded those for HDDs in 2018. Flash storage products had more than twice the revenue of hard disk drives as of 2017. Though SSDs have four to nine times higher cost per bit, they are replacing HDDs in applications where speed, power consumption, small size, high capacity and durability are important. As of 2019, the cost per bit of SSDs is falling, and the price premium over HDDs has narrowed.The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of 1000: a 1-terabyte (TB) drive has a capacity of 1,000 gigabytes (GB; where 1 gigabyte = 1 000 megabytes = 1 000 000 kilobytes (1 million) = 1 000 000 000 bytes (1 billion)). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. There can be confusion regarding storage capacity, since capacities are stated in decimal gigabytes (powers of 1000) by HDD manufacturers, whereas the most commonly used operating systems report capacities in powers of 1024, which results in a smaller number than advertised. Performance is specified as the time required to move the heads to a track or cylinder (average access time), the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally, the speed at which the data is transmitted (data rate).\nThe two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as PATA (Parallel ATA), SATA (Serial ATA), USB or SAS (Serial Attached SCSI) cables.\n\n"
    },
    {
      "id": "185529",
      "title": "Scalability",
      "url": "https://en.wikipedia.org/wiki/Scalability",
      "summary": "Scalability is the property of a system to handle a growing amount of work.  One definition for software systems specifies that this may be done by adding resources to the system.In an economic context, a scalable business model implies that a company can increase sales given increased resources. For example, a package delivery system is scalable because more packages can be delivered by adding more delivery vehicles. However, if all packages had to first pass through a single warehouse for sorting, the system would not be as scalable, because one warehouse can handle only a limited number of packages.In computing, scalability is a characteristic of computers, networks, algorithms, networking protocols, programs and applications. An example is a search engine, which must support increasing numbers of users, and the number of topics it indexes. Webscale is a computer architectural approach that brings the capabilities of large-scale cloud computing companies into enterprise data centers.In distributed systems, there are several definitions according to the authors, some considering the concepts of scalability a sub-part of elasticity, others as being distinct.\nIn mathematics, scalability mostly refers to closure under scalar multiplication.\nIn industrial engineering and manufacturing, scalability refers to the capacity of a process, system, or organization to handle a growing workload, adapt to increasing demands, and maintain operational efficiency. A scalable system can effectively manage increased production volumes, new product lines, or expanding markets without compromising quality or performance. In this context, scalability is a vital consideration for businesses aiming to meet customer expectations, remain competitive, and achieve sustainable growth. Factors influencing scalability include the flexibility of the production process, the adaptability of the workforce, and the integration of advanced technologies. By implementing scalable solutions, companies can optimize resource utilization, reduce costs, and streamline their operations. Scalability in industrial engineering and manufacturing enables businesses to respond to fluctuating market conditions, capitalize on emerging opportunities, and thrive in an ever-evolving global landscape."
    },
    {
      "id": "56938",
      "title": "Institute of Electrical and Electronics Engineers",
      "url": "https://en.wikipedia.org/wiki/Institute_of_Electrical_and_Electronics_Engineers",
      "summary": "The Institute of Electrical and Electronics Engineers (IEEE) is a 501(c)(3) professional association for electronics engineering, electrical engineering, and other related disciplines.\nThe IEEE has a corporate office in New York City and an operations center in Piscataway, New Jersey. The IEEE was formed in 1963 as an amalgamation of the American Institute of Electrical Engineers and the Institute of Radio Engineers."
    },
    {
      "id": "20276016",
      "title": "Incremental computing",
      "url": "https://en.wikipedia.org/wiki/Incremental_computing",
      "summary": "Incremental computing, also known as incremental computation, is a software feature which, whenever a piece of data changes, attempts to save time by only recomputing those outputs which depend on the changed data. When incremental computing is successful, it can be significantly faster than computing new outputs naively. For example, a spreadsheet software package might use incremental computation in its recalculation feature, to update only those cells containing formulas which depend (directly or indirectly) on the changed cells.\nWhen incremental computing is implemented by a tool that can implement it for a variety of different pieces of code automatically, that tool is an example of a program analysis tool for optimization."
    },
    {
      "id": "58644759",
      "title": "Information engineering",
      "url": "https://en.wikipedia.org/wiki/Information_engineering",
      "summary": "Information engineering is the engineering discipline that deals with the generation, distribution, analysis, and use of information, data, and knowledge in systems. The field first became identifiable in the early 21st century.\n\nThe components of information engineering include more theoretical fields such as machine learning, artificial intelligence, control theory, signal processing, and information theory, and more applied fields such as computer vision, natural language processing, bioinformatics, medical image computing, cheminformatics, autonomous robotics, mobile robotics, and telecommunications. Many of these originate from computer science, as well as other branches of engineering such as computer engineering, electrical engineering, and bioengineering.\n\nThe field of information engineering is based heavily on mathematics, particularly probability, statistics, calculus, linear algebra, optimization, differential equations, variational calculus, and complex analysis.\nInformation engineers often hold a degree in information engineering or a related area, and are often part of a professional body such as the Institution of Engineering and Technology or Institute of Measurement and Control. They are employed in almost all industries due to the widespread use of information engineering.\n\n"
    },
    {
      "id": "36674345",
      "title": "Information technology",
      "url": "https://en.wikipedia.org/wiki/Information_technology",
      "summary": "Information technology (IT) is a set of related fields that encompass computer systems, software, programming languages and data and information processing and storage. IT forms part of information and communications technology (ICT). An information technology system (IT system) is generally an information system, a communications system, or, more specifically speaking, a computer system \u2014 including all hardware, software, and peripheral equipment \u2014 operated by a limited group of IT users, and an IT project usually refers to the commissioning and implementation of an IT system.Although humans have been storing, retrieving, manipulating, and communicating information since the earliest writing systems were developed, the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that \"the new technology does not yet have a single established name. We shall call it information technology (IT).\" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce.Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC \u2014 1450 AD), mechanical (1450 \u2014 1840), electromechanical (1840 \u2014 1940), and electronic (1940 to present).Information technology is also a branch of computer science, which can be defined as the overall study of procedure, structure, and the processing of various types of data.  As this field continues to evolve across the world, its overall priority and importance has also grown, which is where we begin to see the introduction of computer science-related courses in K-12 education."
    },
    {
      "id": "146738",
      "title": "Interest",
      "url": "https://en.wikipedia.org/wiki/Interest",
      "summary": "In finance and economics, interest is payment from a borrower or deposit-taking financial institution to a lender or depositor of an amount above repayment of the principal sum (that is, the amount borrowed), at a particular rate. It is distinct from a fee which the borrower may pay to the lender or some third party. It is also distinct from dividend which is paid by a company to its shareholders (owners) from its profit or reserve, but not at a particular rate decided beforehand, rather on a pro rata basis as a share in the reward gained by risk taking entrepreneurs when the revenue earned exceeds the total costs.For example, a customer would usually pay interest to borrow from a bank, so they pay the bank an amount which is more than the amount they borrowed; or a customer may earn interest on their savings, and so they may withdraw more than they originally deposited. In the case of savings, the customer is the lender, and the bank plays the role of the borrower.\nInterest differs from profit, in that interest is received by a lender, whereas profit is received by the owner of an asset, investment or enterprise. (Interest may be part or the whole of the profit on an investment, but the two concepts are distinct from each other from an accounting perspective.)\nThe rate of interest is equal to the interest amount paid or received over a particular period divided by the principal sum borrowed or lent (usually expressed as a percentage).\nCompound interest means that interest is earned on prior interest in addition to the principal. Due to compounding, the total amount of debt grows exponentially, and its mathematical study led to the discovery of the number e. In practice, interest is most often calculated on a daily, monthly, or yearly basis, and its impact is influenced greatly by its compounding rate."
    },
    {
      "id": "1716320",
      "title": "James Martin (author)",
      "url": "https://en.wikipedia.org/wiki/James_Martin_(author)",
      "summary": "James Martin (19 October 1933 \u2013 24 June 2013) was an English information technology consultant and author, known for his work on information technology engineering.\n\n"
    },
    {
      "id": "59252",
      "title": "Marketing",
      "url": "https://en.wikipedia.org/wiki/Marketing",
      "summary": "Marketing is the act of satisfying and retaining customers. It is one of the primary components of business management and commerce.Marketing is typically conducted by the seller, typically a retailer or manufacturer. Products can be marketed to other businesses (B2B) or directly to consumers (B2C). Sometimes tasks are contracted to dedicated marketing firms, like a media, market research, or advertising agency. Sometimes, a trade association or government agency (such as the Agricultural Marketing Service) advertises on behalf of an entire industry or locality, often a specific type of food (e.g. Got Milk?), food from a specific area, or a city or region as a tourism destination.\nMarket orientations are philosophies concerning the factors that should go into market planning. The marketing mix, which outlines the specifics of the product and how it will be sold, including the channels that will be used to advertise the prodcut, is affected by the environment surrounding the product, the results of marketing research and market research, and the characteristics of the product's target market. Once these factors are determined, marketers must then decide what methods of promoting the product, including use of coupons and other price inducements."
    },
    {
      "id": "18831",
      "title": "Mathematics",
      "url": "https://en.wikipedia.org/wiki/Mathematics",
      "summary": "Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.\nMost mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or\u2014in modern mathematics\u2014entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and\u2014in case of abstraction from nature\u2014some basic properties that are considered true starting points of the theory under consideration.Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was primarily divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.\n\n"
    },
    {
      "id": "18933632",
      "title": "Metadata",
      "url": "https://en.wikipedia.org/wiki/Metadata",
      "summary": "Metadata (or metainformation) is \"data that provides information about other data\", but not the content of the data itself, such as the text of a message or the image itself. There are many distinct types of metadata, including:\n\nDescriptive metadata \u2013 the descriptive information about a resource. It is used for discovery and identification. It includes elements such as title, abstract, author, and keywords.\nStructural metadata \u2013 metadata about containers of data and indicates how compound objects are put together, for example, how pages are ordered to form chapters. It describes the types, versions, relationships, and other characteristics of digital materials.\nAdministrative metadata \u2013 the information to help manage a resource, like resource type, permissions, and when and how it was created.\nReference metadata \u2013 the information about the contents and quality of statistical data.\nStatistical metadata \u2013 also called process data, may describe processes that collect, process, or produce statistical data.\nLegal metadata \u2013 provides information about the creator, copyright holder, and public licensing, if provided.Metadata is not strictly bound to one of these categories, as it can describe a piece of data in many other ways.\n\n"
    },
    {
      "id": "19001",
      "title": "Microsoft",
      "url": "https://en.wikipedia.org/wiki/Microsoft",
      "summary": "Microsoft Corporation is an American multinational technology corporation headquartered in Redmond, Washington, United States. Microsoft's best-known software products are the Windows line of operating systems, the Microsoft 365 suite of productivity applications, and the Edge web browser. Its flagship hardware products are the Xbox video game consoles and the Microsoft Surface lineup of touchscreen personal computers. Microsoft ranked No. 14 in the 2022 Fortune 500 rankings of the largest United States corporations by total revenue; it was the world's largest software maker by revenue as of 2022. It is considered one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Apple, and Meta (parent company of Facebook).\nMicrosoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Windows. The company's 1986 initial public offering (IPO) and subsequent rise in its share price created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market and has made several corporate acquisitions, the largest being the acquisition of Activision Blizzard for $68.7 billion in October 2023, followed by its acquisition of LinkedIn for $26.2 billion in December 2016, and its acquisition of Skype Technologies for $8.5 billion in May 2011.As of 2015, Microsoft is market-dominant in the IBM PC compatible operating system market and the office software suite market, although it has lost the majority of the overall operating system market to Android. The company also produces a wide range of other consumer and enterprise software for desktops, laptops, tabs, gadgets, and servers, including Internet search (with Bing), the digital services market (through MSN), mixed reality (HoloLens), cloud computing (Azure), and software development (Visual Studio).\nSteve Ballmer replaced Gates as CEO in 2000 and later envisioned a \"devices and services\" strategy. This unfolded with Microsoft acquiring Danger Inc. in 2008, entering the personal computer production market for the first time in June 2012 with the launch of the Microsoft Surface line of tablet computers, and later forming Microsoft Mobile through the acquisition of Nokia's devices and services division. Since Satya Nadella took over as CEO in 2014, the company has scaled back on hardware and instead focused on cloud computing, a move that helped the company's shares reach their highest value since December 1999. Under Nadella's direction, the company has also heavily expanded its gaming business to support the Xbox brand, establishing the Microsoft Gaming division in 2022, dedicated to operating Xbox in addition to its three subsidiaries (publishers). Microsoft Gaming is the third-largest gaming company in the world by revenue as of 2023.Earlier dethroned by Apple in 2010, in 2018, Microsoft reclaimed its position as the most valuable publicly traded company in the world. In April 2019, Microsoft reached a trillion-dollar market cap, becoming the third U.S. public company to be valued at over $1 trillion after Apple and Amazon, respectively. As of 2023, Microsoft has the third-highest global brand valuation.\nMicrosoft has been criticized for its monopolistic practices and the company's software has been criticized for problems with ease of use, robustness, and security."
    },
    {
      "id": "175537",
      "title": "Netflix",
      "url": "https://en.wikipedia.org/wiki/Netflix",
      "summary": "Netflix is an American subscription video on-demand over-the-top streaming service. The service primarily distributes original and acquired films and television shows from various genres, and it is available internationally in multiple languages.Launched on January 16, 2007, nearly a decade after Netflix, Inc. began its pioneering DVD\u2011by\u2011mail movie rental service, Netflix is the most-subscribed video on demand streaming media service, with 260.28 million paid memberships in more than 190 countries as of January 2024. By 2022, \"Netflix Original\" productions accounted for half of its library in the United States and the namesake company had ventured into other categories, such as video game publishing of mobile games via its flagship service. As of October 2023, Netflix is the 24th most-visited website in the world with 23.66% of its traffic coming from the United States, followed by the United Kingdom at 5.84% and Brazil at 5.64%."
    },
    {
      "id": "37256799",
      "title": "NewSQL",
      "url": "https://en.wikipedia.org/wiki/NewSQL",
      "summary": "NewSQL is a class of relational database management systems that seek to provide the scalability of NoSQL systems for online transaction processing (OLTP) workloads while maintaining the ACID guarantees of a traditional database system.Many enterprise systems that handle high-profile data (e.g., financial and order processing systems) are too large for conventional relational databases, but have transactional and consistency requirements that are not practical for NoSQL systems. The only options previously available for these organizations were to either purchase more powerful computers or to develop custom middleware that distributes requests over conventional DBMS. Both approaches feature high infrastructure costs and/or development costs. NewSQL systems attempt to reconcile the conflicts.\n\n"
    },
    {
      "id": "23968131",
      "title": "NoSQL",
      "url": "https://en.wikipedia.org/wiki/NoSQL",
      "summary": "NoSQL (originally referring to \"non-SQL\" or \"non-relational\") is an approach to database design that focuses on providing a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Instead of the typical tabular structure of a relational database, NoSQL databases house data within one data structure. Since this non-relational database design does not require a\u202fschema, it offers rapid\u202fscalability\u202fto manage\u202flarge and typically unstructured data sets.  NoSQL systems are also sometimes called \"Not only SQL\" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.Non-relational databases have existed since the late 1960s, but the name \"NoSQL\" was only coined in the early 2000s, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications.Motivations for this approach include simplicity of design, simpler \"horizontal\" scaling to clusters of machines (which is a problem for relational databases), finer control over availability, and limiting the object-relational impedance mismatch. The data structures used by NoSQL databases (e.g. key\u2013value pair, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as \"more flexible\" than relational database tables.Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance), lack of ability to perform ad hoc joins across tables, lack of standardized interfaces, and huge previous investments in existing relational databases. Most NoSQL stores lack true ACID transactions, although a few databases have made them central to their designs.\nInstead, most NoSQL databases offer a concept of \"eventual consistency\", in which database changes are propagated to all nodes \"eventually\" (typically within milliseconds), so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale read. Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss. Some NoSQL systems provide concepts such as write-ahead logging to avoid data loss. For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Relational databases \"do not allow referential integrity constraints to span databases\". Few systems maintain both ACID transactions and X/Open XA standards for distributed transaction processing. Interactive relational databases share conformational relay analysis techniques as a common feature. Limitations within the interface environment are overcome using semantic virtualization protocols, such that NoSQL services are accessible to most operating systems.\n\n"
    },
    {
      "id": "2063278",
      "title": "Object\u2013relational impedance mismatch",
      "url": "https://en.wikipedia.org/wiki/Object%E2%80%93relational_impedance_mismatch",
      "summary": "Object\u2013relational impedance mismatch creates difficulties going from data in relational data stores (relational database management system [\u201cRDBMS\u201d]) to usage in domain-driven object models. Object-orientation (OO) is the default method for business-centric design in programming languages. The problem lies in neither relational nor OO, but in the conceptual difficulty mapping between the two logic models. Both are logical models implementable differently on database servers, programming languages, design patterns, or other technologies. Issues range from application to enterprise scale, whenever stored relational data is used in domain-driven object models, and vice versa. Object-oriented data stores can trade this problem for other implementation difficulties.\nThe term impedance mismatch comes from impedance matching in electrical engineering .\n\n"
    },
    {
      "id": "40572678",
      "title": "Object storage",
      "url": "https://en.wikipedia.org/wiki/Object_storage",
      "summary": "Object storage (also known as object-based storage or blob storage) is a computer data storage approach that manages data as \"blobs\" or \"objects\", as opposed to other storage architectures like file systems which manages data as a file hierarchy, and block storage which manages data as blocks within sectors and tracks. Each object is typically associated with a variable amount of metadata, and a globally unique identifier. Object storage can be implemented at multiple levels, including the device level (object-storage device), the system level, and the interface level. In each case, object storage seeks to enable capabilities not addressed by other storage architectures, like interfaces that are directly programmable by the application, a namespace that can span multiple instances of physical hardware, and data-management functions like data replication and data distribution at object-level granularity.\nObject storage systems allow retention of massive amounts of unstructured data in which data is written once and read once (or many times). Object storage is used for purposes such as storing objects like videos and photos on Facebook, songs on Spotify, or files in online collaboration services, such as Dropbox. One of the limitations with object storage is that it is not intended for transactional data, as object storage was not designed to replace NAS file access and sharing; it does not support the locking and sharing mechanisms needed to maintain a single, accurately updated version of a file.\n\n"
    },
    {
      "id": "189239",
      "title": "Online analytical processing",
      "url": "https://en.wikipedia.org/wiki/Online_analytical_processing",
      "summary": "Online analytical processing, or OLAP (), is an approach to answer multi-dimensional analytical (MDA) queries swiftly in computing. OLAP is part of the broader category of business intelligence, which also encompasses relational databases, report writing and data mining.  Typical applications of OLAP include business reporting for sales, marketing, management reporting, business process management (BPM), budgeting and forecasting, financial reporting and similar areas, with new applications emerging, such as agriculture.The term OLAP was created as a slight modification of the traditional database term online transaction processing (OLTP).OLAP tools enable users to analyse multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.:\u200a402\u2013403\u200a Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region's sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP cube and view (dicing) the slices from different viewpoints.  These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson, or by date, or by customer, or by product, or by region, etc.).\nDatabases configured for OLAP use a multidimensional data model, allowing for complex analytical and ad hoc queries with a rapid execution time.  They borrow aspects of navigational databases, hierarchical databases and relational databases.\nOLAP is typically contrasted to OLTP (online transaction processing), which is generally characterized by much less complex queries, in a larger volume, to process transactions rather than for the purpose of business intelligence or reporting. Whereas OLAP systems are mostly optimized for read, OLTP has to process all kinds of queries (read, insert, update and delete)."
    },
    {
      "id": "2329992",
      "title": "Online transaction processing",
      "url": "https://en.wikipedia.org/wiki/Online_transaction_processing",
      "summary": "Online transaction processing (OLTP) is a type of database system used in transaction-oriented applications, such as many operational systems. \"Online\" refers to that such systems are expected to respond to user requests and process them in real-time (process transactions). The term is contrasted with online analytical processing (OLAP) which instead focuses on data analysis (for example planning and management systems).\n\n"
    },
    {
      "id": "23862",
      "title": "Python (programming language)",
      "url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
      "summary": "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community."
    },
    {
      "id": "25873",
      "title": "Relational database",
      "url": "https://en.wikipedia.org/wiki/Relational_database",
      "summary": "A relational database is a (most commonly digital) database  based on the relational model of data, as proposed by E. F. Codd in 1970. A database management system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems are equipped with the option of using SQL (Structured Query Language) for querying and updating the database.\n\n"
    },
    {
      "id": "29414838",
      "title": "Rust (programming language)",
      "url": "https://en.wikipedia.org/wiki/Rust_(programming_language)",
      "summary": "Rust is a multi-paradigm, general-purpose programming language that emphasizes performance, type safety, and concurrency. It enforces memory safety\u2014meaning that all references point to valid memory\u2014without a garbage collector. To simultaneously enforce memory safety and prevent data races, its \"borrow checker\" tracks the object lifetime of all references in a program during compilation. Rust was influenced by ideas from functional programming, including immutability, higher-order functions, and algebraic data types. It is popular for systems programming.Software developer Graydon Hoare created Rust as a personal project while working at Mozilla Research in 2006. Mozilla officially sponsored the project in 2009. In the years following the first stable release in May 2015, Rust was adopted by companies including Amazon, Discord, Dropbox, Google (Alphabet), Meta, and Microsoft. In December 2022, it became the first language other than C and assembly to be supported in the development of the Linux kernel.\nRust has been noted for its rapid adoption, and has been studied in programming language theory research."
    },
    {
      "id": "48455863",
      "title": "Semantic Scholar",
      "url": "https://en.wikipedia.org/wiki/Semantic_Scholar",
      "summary": "Semantic Scholar is a research tool for scientific literature powered by artificial intelligence. It is developed at the Allen Institute for AI and was publicly released in November 2015. Semantic Scholar uses modern techniques in natural language processing to support the research process, for example by providing automatically generated summaries of scholarly papers. The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human\u2013computer interaction, and information retrieval.Semantic Scholar began as a database for the topics of computer science, geoscience, and neuroscience. In 2017, the system began including biomedical literature in its corpus. As of September 2022, it includes over 200 million publications from all fields of science.\n\n"
    },
    {
      "id": "1393991",
      "title": "Social Science Research Network",
      "url": "https://en.wikipedia.org/wiki/Social_Science_Research_Network",
      "summary": "The Social Science Research Network (SSRN) is a repository for preprints devoted to the rapid dissemination of scholarly research in the social sciences, humanities, life sciences, and health sciences, among others. Elsevier bought SSRN from Social Science Electronic Publishing Inc. in May 2016. It is not an electronic journal, but rather an eLibrary and search engine."
    },
    {
      "id": "69894",
      "title": "Wales",
      "url": "https://en.wikipedia.org/wiki/Wales",
      "summary": "Wales (Welsh: Cymru [\u02c8k\u0259m.r\u0268] ) is a country that is part of the United Kingdom. It is bordered by the Irish Sea to the north and west, England to the east, the Bristol Channel to the south, and the Celtic Sea to the south-west. As of the 2021 census, it had a population of 3,107,494. It has a total area of 21,218 square kilometres (8,192 sq mi) and over 2,700 kilometres (1,680 mi) of coastline. It is largely mountainous with its higher peaks in the north and central areas, including Snowdon (Yr Wyddfa), its highest summit. The country lies within the north temperate zone and has a changeable, maritime climate. The capital and largest city is Cardiff.\nA distinct Welsh culture emerged among the Celtic Britons after the Roman withdrawal from Britain in the 5th century, and Wales was briefly united under Gruffydd ap Llywelyn in 1055. After over 200 years of war, the conquest of Wales by King Edward I of England was completed by 1283, though Owain Glynd\u0175r led the Welsh Revolt against English rule in the early 15th century, and briefly re-established an independent Welsh state with its own national parliament (Welsh: senedd). In the 16th century the whole of Wales was annexed by England and incorporated within the English legal system under the Laws in Wales Acts 1535 and 1542. Distinctive Welsh politics developed in the 19th century. Welsh Liberalism, exemplified in the early 20th century by David Lloyd George, was displaced by the growth of socialism and the Labour Party. Welsh national feeling grew over the century: a nationalist party, Plaid Cymru, was formed in 1925, and the Welsh Language Society in 1962. A governing system of Welsh devolution is employed in Wales, of which the most major step was the formation of the Senedd (Welsh Parliament, formerly the National Assembly for Wales) in 1998, responsible for a range of devolved policy matters.\nAt the dawn of the Industrial Revolution, development of the mining and metallurgical industries transformed the country from an agricultural society into an industrial one; the South Wales Coalfield's exploitation caused a rapid expansion of Wales's population. Two-thirds of the population live in South Wales, including Cardiff, Swansea, Newport and the nearby valleys. The eastern region of North Wales has about a sixth of the overall population, with Wrexham being the largest northern city. The remaining parts of Wales are sparsely populated. Now that the country's traditional extractive and heavy industries have gone or are in decline, the economy is based on the public sector, light and service industries, and tourism. Agriculture in Wales is largely livestock based, making Wales a net exporter of animal produce, contributing towards national agricultural self-sufficiency.\nThe country has a distinct national and cultural identity and from the late 19th century onwards Wales acquired its popular image as the \"land of song\", in part due to the eisteddfod tradition and rousing choir singing. Both Welsh and English are official languages. A majority of the population in most areas speaks English whilst the majority of the population in parts of the north and west speak Welsh, with a total of 538,300 Welsh speakers across the entire country."
    },
    {
      "id": "3254510",
      "title": "Scala (programming language)",
      "url": "https://en.wikipedia.org/wiki/Scala_(programming_language)",
      "summary": "Scala ( SKAH-lah) is a strong statically typed high-level general-purpose programming language that supports both object-oriented programming and functional programming. Designed to be concise, many of Scala's design decisions are intended to address criticisms of Java.Scala source code can be compiled to Java bytecode and run on a Java virtual machine (JVM). Scala can also be compiled to JavaScript to run in a browser, or directly to a native executable. On the JVM Scala provides language interoperability with Java so that libraries written in either language may be referenced directly in Scala or Java code. Like Java, Scala is object-oriented, and uses a syntax termed curly-brace which is similar to the language C. Since Scala 3, there is also an option to use the off-side rule (indenting) to structure blocks, and its use is advised. Martin Odersky has said that this turned out to be the most productive change introduced in Scala 3.Unlike Java, Scala has many features of functional programming languages (like Scheme, Standard ML, and Haskell), including currying, immutability, lazy evaluation, and pattern matching. It also has an advanced type system supporting algebraic data types, covariance and contravariance, higher-order types (but not higher-rank types), anonymous types, operator overloading, optional parameters, named parameters, raw strings, and an experimental exception-only version of algebraic effects that can be seen as a more powerful version of Java's checked exceptions.The name Scala is a portmanteau of scalable and language, signifying that it is designed to grow with the demands of its users."
    },
    {
      "id": "22135297",
      "title": "Semi-structured data",
      "url": "https://en.wikipedia.org/wiki/Semi-structured_data",
      "summary": "Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure.\nIn semi-structured data, the entities belonging to the same class may have different attributes even though they are grouped together, and the attributes' order is not important.\nSemi-structured data are increasingly occurring since the advent of the Internet where full-text documents and databases are not the only forms of data anymore, and different applications need a medium for exchanging information. In object-oriented databases, one often finds semi-structured data.\n\n"
    },
    {
      "id": "5309",
      "title": "Software",
      "url": "https://en.wikipedia.org/wiki/Software",
      "summary": "Software is a collection of programs and data that tell a computer how to perform specific tasks. Software often includes associated software documentation. This is in contrast to hardware, from which the system is built and which actually performs the work.\nAt the lowest programming level, executable code consists of machine language instructions supported by an individual processor\u2014typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer\u2014an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction or is interrupted by the operating system. As of 2024, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.\nThe majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler, an interpreter, or a combination of the two. Software may also be written in a low-level assembly language that has a strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler."
    },
    {
      "id": "27010",
      "title": "Software engineering",
      "url": "https://en.wikipedia.org/wiki/Software_engineering",
      "summary": "Software engineering is an engineering-based approach to software development.\nA software engineer is a person who applies the engineering design process to design, develop, test, maintain, and evaluate computer software. The term programmer is sometimes used as a synonym, but may emphasize software implementation over design and can also lack connotations of engineering education or skills.Engineering techniques are used to inform the software development process, which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management, which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.\n\n"
    },
    {
      "id": "7366298",
      "title": "Solid-state drive",
      "url": "https://en.wikipedia.org/wiki/Solid-state_drive",
      "summary": "A solid-state drive (SSD) is a solid-state storage device that uses integrated circuit assemblies to store data persistently, typically using flash memory, and functions as secondary storage in the hierarchy of computer storage. It is also sometimes called a semiconductor storage device, a solid-state device, or a solid-state disk, even though SSDs lack the physical spinning disks and movable read-write heads used in hard disk drives (HDDs) and floppy disks. SSD also has rich internal parallelism for data processing.In comparison to hard disk drives and similar electromechanical media which use moving parts, SSDs are typically more resistant to physical shock, run silently, and have higher input/output rates and lower latency. SSDs store data in semiconductor cells. As of 2019, cells can contain between 1 and 4 bits of data. SSD storage devices vary in their properties according to the number of bits stored in each cell, with single-bit cells (\"Single Level Cells\" or \"SLC\") being generally the most reliable, durable, fast, and expensive type, compared with 2- and 3-bit cells (\"Multi-Level Cells/MLC\" and \"Triple-Level Cells/TLC\"), and finally, quad-bit cells (\"QLC\") being used for consumer devices that do not require such extreme properties and are the cheapest per gigabyte (GB) of the four. In addition, 3D XPoint memory (sold by Intel under the Optane brand) stores data by changing the electrical resistance of cells instead of storing electrical charges in cells, and SSDs made from RAM can be used for high speed, when data persistence after power loss is not required, or may use battery power to retain data when its usual power source is unavailable. Hybrid drives or solid-state hybrid drives (SSHDs), such as Intel's Hystor and Apple's Fusion Drive, combine features of SSDs and HDDs in the same unit using both flash memory and spinning magnetic disks in order to improve the performance of frequently-accessed data. Bcache achieves a similar effect purely in software, using combinations of dedicated regular SSDs and HDDs.\nSSDs based on NAND flash will slowly leak charge over time if left for long periods without power. This causes worn-out drives (that have exceeded their endurance rating) to start losing data typically after one year (if stored at 30 \u00b0C) to two years (at 25 \u00b0C) in storage; for new drives it takes longer. Therefore, SSDs are not suitable for archival storage. 3D XPoint is a possible exception to this rule; it is a relatively new technology with unknown long-term data-retention characteristics.\nSSDs can use traditional HDD interfaces and form factors, or newer interfaces and form factors that exploit specific advantages of the flash memory in SSDs. Traditional interfaces (e.g. SATA and SAS) and standard HDD form factors allow such SSDs to be used as drop-in replacements for HDDs in computers and other devices. Newer form factors such as mSATA, M.2, U.2, NF1/M.3/NGSFF, XFM Express (Crossover Flash Memory, form factor XT2) and EDSFF (formerly known as Ruler SSD) and higher speed interfaces such as NVM Express (NVMe) over PCI Express (PCIe) can further increase performance over HDD performance. SSDs have a limited lifetime number of writes, and also slow down as they reach their full storage capacity."
    },
    {
      "id": "26685",
      "title": "Statistics",
      "url": "https://en.wikipedia.org/wiki/Statistics",
      "summary": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when an it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems."
    },
    {
      "id": "8286675",
      "title": "System",
      "url": "https://en.wikipedia.org/wiki/System",
      "summary": "A system is a group of interacting or interrelated elements that act according to a set of rules to form a unified whole. A system, surrounded and influenced by its environment, is described by its boundaries, structure /system/\nand purpose and is expressed in its functioning. Systems are the subjects of study of systems theory and other systems sciences.\nSystems have several common properties and characteristics, including structure, function(s), behavior and interconnectivity."
    },
    {
      "id": "12098689",
      "title": "Systems analyst",
      "url": "https://en.wikipedia.org/wiki/Systems_analyst",
      "summary": "A systems analyst, also known as business technology analyst, is an information technology (IT) professional who specializes in analyzing, designing and implementing information systems. Systems analysts assess the suitability of information systems in terms of their intended outcomes and liaise with end users, software vendors and programmers in order to achieve these outcomes. A systems analyst is a person who uses analysis and design techniques to solve business problems using information technology. Systems analysts may serve as change agents who identify the organizational improvements needed, design systems to implement those changes, and train and motivate others to use the systems.\n\n"
    },
    {
      "id": "21101351",
      "title": "T. William Olle",
      "url": "https://en.wikipedia.org/wiki/T._William_Olle",
      "summary": "T. William (Bill) Olle (born 1933 and died March 2019) was a British computer scientist and consultant and President of T. William Olle Associates, England.\n\n"
    },
    {
      "id": "651800",
      "title": "Terry Halpin",
      "url": "https://en.wikipedia.org/wiki/Terry_Halpin",
      "summary": "Terence Aidan (Terry) Halpin (born 1950s) is an Australian computer scientist who is known for his formalization of the Object Role Modeling notation."
    },
    {
      "id": "45554839",
      "title": "Tony Morgan (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Tony_Morgan_(computer_scientist)",
      "summary": "Antony J. (Tony) Morgan (born c. 1944) is a British computer scientist, data modeling consultant, and Professor in computer science at INTI International University. He is known for his work on (2002) \"Business rules and information systems,\" and the 2010 \"Information modeling and relational databases,\" co-authored with Terry Halpin."
    },
    {
      "id": "23538754",
      "title": "Wayback Machine",
      "url": "https://en.wikipedia.org/wiki/Wayback_Machine",
      "summary": "The Wayback Machine is a digital archive of the World Wide Web founded by the Internet Archive, an American nonprofit organization based in San Francisco, California. Created in 1996 and launched to the public in 2001, it allows the user to go \"back in time\" to see how websites looked in the past. Its founders, Brewster Kahle and Bruce Gilliat, developed the Wayback Machine to provide \"universal access to all knowledge\" by preserving archived copies of defunct web pages.Launched on May 10, 1996, the Wayback Machine had saved more than 38.2 billion web pages at the end of 2009. As of January 3, 2024, the Wayback Machine has archived more than 860 billion web pages and well over 99 petabytes of data."
    },
    {
      "id": "12506378",
      "title": "Workflow management system",
      "url": "https://en.wikipedia.org/wiki/Workflow_management_system",
      "summary": "A workflow management system (WfMS or WFMS) provides an infrastructure for the set-up, performance, and monitoring of a defined sequence of tasks arranged as a workflow application."
    },
    {
      "id": "14924067",
      "title": "Academic discipline",
      "url": "https://en.wikipedia.org/wiki/Academic_discipline",
      "summary": "An academic discipline or academic field is a subdivision of knowledge that is taught and researched at the college or university level. Disciplines are defined (in part) and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties within colleges and universities to which their practitioners belong. Academic disciplines are conventionally divided into the humanities, including language, art and cultural studies, and the scientific disciplines, such as physics, chemistry, and biology; the social sciences are sometimes considered a third category.\nIndividuals associated with academic disciplines are commonly referred to as experts or specialists. Others, who may have studied liberal arts or systems theory rather than concentrating in a specific academic discipline, are classified as generalists.\nWhile academic disciplines in and of themselves are more or less focused practices, scholarly approaches such as multidisciplinarity/interdisciplinarity, transdisciplinarity, and cross-disciplinarity integrate aspects from multiple academic disciplines, therefore addressing any problems that may arise from narrow concentration within specialized fields of study. For example, professionals may encounter trouble communicating across academic disciplines because of differences in language, specified concepts, or methodology.\nSome researchers believe that academic disciplines may, in the future, be replaced by what is known as Mode 2 or \"post-academic science\", which involves the acquisition of cross-disciplinary knowledge through the collaboration of specialists from various academic disciplines.\nIt is also known as a field of study, field of inquiry, research field and branch of knowledge. The different terms are used in different countries and fields.\n\n"
    },
    {
      "id": "341988",
      "title": "American Statistical Association",
      "url": "https://en.wikipedia.org/wiki/American_Statistical_Association",
      "summary": "The American Statistical Association (ASA) is the main professional organization for statisticians and related professionals in the United States. It was founded in Boston, Massachusetts on November 27, 1839, and is the second-oldest continuously operating professional society in the U.S. behind the Massachusetts Medical Society, founded in 1781). ASA services statisticians, quantitative scientists, and users of statistics across many academic areas and applications. The association publishes a variety of journals and sponsors several international conferences every year.\n\n"
    },
    {
      "id": "1134",
      "title": "Analysis",
      "url": "https://en.wikipedia.org/wiki/Analysis",
      "summary": "Analysis (pl.: analyses) is the process of breaking a complex topic or substance into smaller parts in order to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.The word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analysis, \"a breaking-up\" or \"an untying;\" from ana- \"up, throughout\" and lysis \"a loosening\"). From it also comes the word's plural, analyses.\nAs a formal concept, the method has variously been ascribed to Alhazen, Ren\u00e9 Descartes (Discourse on the Method), and Galileo Galilei. It has also been ascribed to Isaac Newton, in the form of a practical method of physical discovery (which he did not name).\nThe converse of analysis is synthesis: putting the pieces back together again in a new or different whole."
    },
    {
      "id": "25745941",
      "title": "Andrew Gelman",
      "url": "https://en.wikipedia.org/wiki/Andrew_Gelman",
      "summary": "Andrew Eric Gelman (born February 11, 1965) is an American statistician and professor of statistics and political science at Columbia University.\nGelman received bachelor of science degrees in mathematics and in physics from MIT, where he was a National Merit Scholar, in 1986. He then received a master of science in 1987 and a doctor of philosophy in 1990, both in statistics from Harvard University, under the supervision of Donald Rubin."
    },
    {
      "id": "38751",
      "title": "ArXiv",
      "url": "https://en.wikipedia.org/wiki/ArXiv",
      "summary": "arXiv (pronounced as \"archive\"\u2014the X represents the Greek letter chi \u27e8\u03c7\u27e9) is an open-access repository of electronic preprints and postprints (known as e-prints) approved for posting after moderation, but not peer review. It consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance and economics, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, had hit a million by the end of 2014 and two million by the end of 2021. As of April 2021, the submission rate is about 16,000 articles per month."
    },
    {
      "id": "1673865",
      "title": "Astronomical survey",
      "url": "https://en.wikipedia.org/wiki/Astronomical_survey",
      "summary": "An astronomical survey is a general map or image of a region of the sky (or of the whole sky) that lacks a specific observational target.  Alternatively, an astronomical survey may comprise a set of images, spectra, or other observations of objects that share a common type or feature. Surveys are often restricted to one band of the electromagnetic spectrum due to instrumental limitations, although multiwavelength surveys can be made by using multiple detectors, each sensitive to a different bandwidth.Surveys have generally been performed as part of the production of an astronomical catalog. They may also search for transient astronomical events. They often use wide-field astrographs.\n\n"
    },
    {
      "id": "30863191",
      "title": "Basic research",
      "url": "https://en.wikipedia.org/wiki/Basic_research",
      "summary": "Basic research, also called pure research, fundamental research, basic science, or pure science, is a type of scientific research with the aim of improving scientific theories for better understanding and prediction of natural or other phenomena.  In contrast, applied research uses scientific theories to develop technology or techniques which can be used to intervene and alter natural or other phenomena. Though often driven simply by curiosity, basic research often fuels the technological innovations of applied science.  The two aims are often practiced simultaneously in coordinated research and development.\nIn addition to innovations, basic research also serves to provide insight into nature around us and allows us to respect its innate value. The development of this respect is what drives conservation efforts. Through learning about the environment, conservation efforts can be strengthened using research as a basis. Technological innovations can unintentionally be created through this as well, as seen with examples such as kingfishers' beaks affecting the design for high speed bullet train in Japan.\n\n"
    },
    {
      "id": "20877649",
      "title": "Ben Fry",
      "url": "https://en.wikipedia.org/wiki/Ben_Fry",
      "summary": "Benjamin Fry is an American designer who has expertise in data visualization.\n\n"
    },
    {
      "id": "24437894",
      "title": "Boston",
      "url": "https://en.wikipedia.org/wiki/Boston",
      "summary": "Boston (US: ), officially the City of Boston, is the capital and most populous city in Massachusetts, a state in the United States. The city serves as the cultural and financial center of the New England region of the Northeastern United States. It has an area of 48.4 sq mi (125 km2) and a population of 675,647 as of 2020. The Greater Boston metropolitan statistical area, surrounding the city, is the eleventh-largest in the country.Boston is one of the United States' oldest cities. It was founded on the Shawmut Peninsula in 1630 by Puritan settlers. The city was named after Boston, Lincolnshire, England. During the American Revolution, Boston was home to several key events. These included the Boston Massacre, the Boston Tea Party, the hanging of Paul Revere's lantern signal in Old North Church, the Battle of Bunker Hill, and the siege of Boston. Following American independence from Great Britain, the city continued to play an important role as a port, manufacturing hub, and center for education and culture.The city expanded significantly beyond the original peninsula through filling in land and annexing neighboring towns. It now attracts many tourists, with Faneuil Hall alone drawing more than 20 million visitors per year. Boston's many firsts include the United States' first public park (Boston Common, 1634), the first public school (Boston Latin School, 1635), and the first subway system (Tremont Street subway, 1897).In the 21st century, Boston emerged as a global leader in higher education and academic research. Some of the city's colleges and universities include Boston University and Northeastern University in the city proper. Furthermore, Greater Boston's many colleges and universities include globally-ranked Harvard and MIT in neighboring Cambridge.Boston has become the largest biotechnology hub in the world. The city is also a national leader in scientific research, law, medicine, engineering, and business. With nearly 5,000 startup companies, the city is considered a global pioneer in innovation and entrepreneurship, and more recently in artificial intelligence. Boston's economy also includes finance, professional and business services, information technology, and government activities. Households in the city claim the highest average rate of philanthropy in the United States. Furthermore, Boston's businesses and institutions rank among the top in the country overall for environmental sustainability and new investment."
    },
    {
      "id": "30068",
      "title": "The Boston Globe",
      "url": "https://en.wikipedia.org/wiki/The_Boston_Globe",
      "summary": "The Boston Globe is an American daily newspaper founded and based in Boston, Massachusetts. The newspaper has won a total of 27 Pulitzer Prizes.Its reported daily circulation had fallen to under 69,000 copies per day as of June 2022. It reported 300,000 print and digital subscribers in 2017. The Boston Globe is the oldest and largest daily newspaper in Boston.Founded in 1872, the paper was mainly controlled by Irish Catholic interests before being sold to Charles H. Taylor and his family. After being privately held until 1973, it was sold to The New York Times in 1993 for $1.1 billion, making it one of the most expensive print purchases in U.S. history. The newspaper was purchased in 2013 by Boston Red Sox and Liverpool F.C. owner John W. Henry for $70 million from The New York Times Company, having lost over 90% of its value in 20 years.\nThe newspaper has been noted as \"one of the nation's most prestigious papers.\" In 1967, The Boston Globe became the first major paper in the U.S. to come out against the Vietnam War. The paper's 2002 coverage of the Roman Catholic Church sex abuse scandal received international media attention and served as the basis for the 2015 American drama film Spotlight.The editor of The Boston Globe is Nancy Barnes, who took the helm in February 2023.The chief print rival of The Boston Globe is the Boston Herald, which has a smaller circulation that is declining more rapidly."
    },
    {
      "id": "39206",
      "title": "Business",
      "url": "https://en.wikipedia.org/wiki/Business",
      "summary": "Business is the practice of making one's living or making money by producing or buying and selling products (such as goods and services). It is also \"any activity or enterprise entered into for profit.\"A business entity is not necessarily separate from the owner and the creditors can hold the owner liable for debts the business has acquired. The taxation system for businesses is different from that of the corporates. A business structure does not allow for corporate tax rates. The proprietor is personally taxed on all income from the business.\nThe term is also often used colloquially (but not by lawyers or public officials) to refer to a company, such as a corporation or cooperative.\nCorporations, in contrast with sole proprietors and partnerships, are separate legal entities and provide limited liability for their owners/members, as well as being subject to corporate tax rates. A corporation is more complicated and expensive to set up, but offers more protection and benefits for the owners/members."
    },
    {
      "id": "158442",
      "title": "Buzzword",
      "url": "https://en.wikipedia.org/wiki/Buzzword",
      "summary": "A buzzword is a word or phrase, new or already existing, that becomes popular for a period of time. Buzzwords often derive from technical terms yet often have much of the original technical meaning removed through fashionable use, being simply used to impress others. Some buzzwords retain their true technical meaning when used in the correct contexts, for example artificial intelligence.\nBuzzwords often originate in jargon, acronyms, or neologisms. Examples of overworked business buzzwords include synergy, vertical, dynamic, cyber and strategy. \nIt has been stated that businesses could not operate without buzzwords, as they are the shorthands or internal shortcuts that make perfect sense to people informed of the context. However, a useful buzzword can become co-opted into general popular speech and lose its usefulness. According to management professor Robert Kreitner, \"Buzzwords are the literary equivalent of Gresham's law. They will drive out good ideas.\"\nBuzzwords, or buzz-phrases such as \"all on the same page\", can also be seen in business as a way to make people feel like there is a mutual understanding. As most workplaces use a specialized jargon, which could be argued is another form of buzzwords, it allows quicker communication. Indeed, many new hires feel more like \"part of the team\" the quicker they learn the buzzwords of their new workplace. Buzzwords permeate people's working lives so much that many don't realize that they are using them. The vice president of CSC Index, Rich DeVane, notes that buzzwords describe not only a trend, but also what can be considered a \"ticket of entry\" with regards to being considered as a successful organization \u2013 \"What people find tiresome is each consulting firm's attempt to put a different spin on it. That's what gives bad information.\"Buzzwords also feature prominently in politics, where they can result in a process which \"privileges rhetoric over reality, producing policies that are 'operationalized' first and only 'conceptualized' at a later date\". The resulting political speech is known for \"eschewing reasoned debate (as characterized by the use of evidence and structured argument), instead employing language exclusively for the purposes of control and manipulation\"."
    },
    {
      "id": "26880450",
      "title": "C. F. Jeff Wu",
      "url": "https://en.wikipedia.org/wiki/C._F._Jeff_Wu",
      "summary": "Chien-Fu Jeff Wu (born 1949) is the Coca-Cola Chair in Engineering Statistics and Professor in the H. Milton Stewart School of Industrial and Systems Engineering at the Georgia Institute of Technology.  He is known for his work on the convergence of the EM algorithm, resampling methods such as the bootstrap and jackknife, and industrial statistics, including design of experiments, and robust parameter design (Taguchi methods).\nBorn in Taiwan, Wu earned a B.Sc. in Mathematics from National Taiwan University in 1971, and a Ph.D. in Statistics from University of California, Berkeley in 1976.  He has been a faculty member at the University of Wisconsin, Madison (1977\u20131988), the University of Waterloo (1988\u20131993; GM-NSERC chair in quality and productivity), the University of Michigan (1995\u20132003; chair of Department of Statistics 1995\u201398; H.C. Carver professor of statistics, 1997\u20132003) and currently the Georgia Institute of Technology.  He has supervised 50 Ph.D. students and published around 185 peer-reviewed articles and two books. He has received several awards, including the COPSS Presidents' Award in 1987,\nthe Shewhart Medal in 2008,\nthe COPSS R. A. Fisher Lectureship in 2011,\nand the Deming Lecturer Award in 2012.  He gave the inaugural Akaike Memorial Lecture in 2016. He has been elected as a fellow of the American Statistical Association, the Institute of Mathematical Statistics, the American Society for Quality and the Institute for Operations Research and the Management Sciences.  In 2000 he was elected as a member of Academia Sinica. In 2004, he was elected as a member of the National Academy of Engineering.  He received the Shewhart Medal of the American Society for Quality and an honorary degree from the University of Waterloo in 2008. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, he used the term Data Science for the first time as an alternative name for statistics.  Later, in November 1997, he gave the inaugural lecture entitled \"Statistics = Data Science?\" for his appointment to the H. C. Carver Professorship at the University of Michigan.\nHe popularized the term \"data science\" and advocated that statistics be renamed data science and statisticians data scientists.\nHe also presented his lecture entitled \"Statistics = Data Science?\" as the first of his 1998 P.C. Mahalanobis Memorial Lectures. These lectures honor Prasanta Chandra Mahalanobis, an Indian scientist and statistician and founder of the Indian Statistical Institute.\nIn Mile, Yunnan, China, a conference was held in July 2014 celebrating Professor Wu's 65th birthday. In 2014 he gave the Bradley Lecture at the University of Georgia. In 2016 he was the inaugural recipient of the Akaike Memorial Lecture Award.  In 2017 Jeff Wu received the George Box Medal from ENBIS. In 2020, Jeff Wu received Georgia Institute of Technology\u2019s highest award given to a faculty member: the Class of 1934 Distinguished Professor Award.  In the same year, he also got the Sigma Xi\u2019s Monie A. Ferst Award which, since 1977, has honored science and engineering teachers who have inspired their students to significant research achievements. In 2020 he delivered the CANSSI/Fields Distinguished Lectures Series in Statistical Sciences."
    },
    {
      "id": "6310",
      "title": "Columbia University",
      "url": "https://en.wikipedia.org/wiki/Columbia_University",
      "summary": "Columbia University, officially Columbia University in the City of New York, is a private Ivy League research university in New York City. Established in 1754 as King's College on the grounds of Trinity Church in Manhattan, it is the oldest institution of higher education in New York and the fifth-oldest in the United States.\nColumbia was established as a colonial college by royal charter under George II of Great Britain. It was renamed Columbia College in 1784 following the American Revolution, and in 1787 was placed under a private board of trustees headed by former students Alexander Hamilton and John Jay. In 1896, the campus was moved to its current location in Morningside Heights and renamed Columbia University.\nColumbia is organized into twenty schools, including four undergraduate schools and 16 graduate schools. The university's research efforts include the Lamont\u2013Doherty Earth Observatory, the Goddard Institute for Space Studies, and accelerator laboratories with Big Tech firms such as Amazon and IBM. Columbia is a founding member of the Association of American Universities and was the first school in the United States to grant the MD degree. The university also administers and annually awards the Pulitzer Prize.\nColumbia scientists and scholars have played a pivotal role in scientific breakthroughs including brain-computer interface; the laser and maser; nuclear magnetic resonance; the first nuclear pile; the first nuclear fission reaction in the Americas; the first evidence for plate tectonics and continental drift; and much of the initial research and planning for the Manhattan Project during World War II.\nAs of December 2021, its alumni, faculty, and staff have included five of the Seven Founding Fathers of the United States of America; four U.S. presidents; 34 foreign heads of state; two secretaries-general of the United Nations; ten justices of the United States Supreme Court; 103 Nobel laureates; 125 National Academy of Sciences members; 53 living billionaires; 23 Olympic medalists; 33 Academy Award winners; and 125 Pulitzer Prize  recipients."
    },
    {
      "id": "63552467",
      "title": "Comet NEOWISE",
      "url": "https://en.wikipedia.org/wiki/Comet_NEOWISE",
      "summary": "C/2020 F3 (NEOWISE) or Comet NEOWISE is a long period comet with a near-parabolic orbit discovered on March 27, 2020, by astronomers during the NEOWISE mission of the Wide-field Infrared Survey Explorer (WISE) space telescope. At that time, it was an 18th-magnitude object, located 2 AU (300 million km; 190 million mi) away from the Sun and 1.7 AU (250 million km; 160 million mi) away from Earth.NEOWISE is known for being the brightest comet in the northern hemisphere since Comet Hale\u2013Bopp in 1997. It was widely photographed by professional and amateur observers and was even spotted by people living near city centers and areas with light pollution. While it was too close to the Sun to be observed at perihelion, it emerged from perihelion around magnitude 0.5 to 1, making it bright enough to be visible to the naked eye. Under dark skies, it could be seen with the naked eye and remained visible to the naked eye throughout July 2020. By July 30, the comet was about magnitude 5, when binoculars were required near urban areas to locate the comet. \nFor observers in the Northern Hemisphere, the comet could be seen on the northwestern horizon, below the Big Dipper. North of 45 degrees north, the comet was visible all night in mid-July 2020. On July 30, Comet NEOWISE entered the constellation of Coma Berenices, below the bright star Arcturus."
    },
    {
      "id": "7671",
      "title": "Committee on Data of the International Science Council",
      "url": "https://en.wikipedia.org/wiki/Committee_on_Data_of_the_International_Science_Council",
      "summary": "The Committee on Data of the International Science Council (CODATA) was established in 1966 as the Committee on Data for Science and Technology, originally part of the International Council of Scientific Unions, now part of the International Science Council (ISC). Since November 2023 its president is the Catalan researcher Merc\u00e8 Crosas.CODATA exists to promote global collaboration to advance open science and to improve the availability and usability of data for all areas of research. CODATA supports the principle that data produced by research and susceptible to being used for research should be as open as possible and as closed as necessary. CODATA works also to advance the interoperability and the usability of such data; research data should be FAIR (findable, accessible, interoperable and reusable). By promoting the policy, technological, and cultural changes that are essential to promote open science, CODATA helps advance ISC's vision and mission of advancing science as a global public good.\nThe CODATA Strategic Plan 2015 and Prospectus of Strategy and Achievement 2016 identify three priority areas:\n\npromoting principles, policies and practices for open data and open science;\nadvancing the frontiers of data science;\nbuilding capacity for open science by improving data skills and the functions of national science systems needed to support open data.CODATA achieves these objectives through a number of standing committees and strategic executive led initiatives, and through its task groups and working groups. CODATA also works closely with member unions and associations of ISC to promote the efforts on open data and open science."
    },
    {
      "id": "5177",
      "title": "Communication",
      "url": "https://en.wikipedia.org/wiki/Communication",
      "summary": "Communication is commonly defined as the transmission of information. Its precise definition is disputed and there are disagreements about whether unintentional or failed transmissions are included and whether communication not only transmits meaning but also creates it. Models of communication are simplified overviews of its main components and their interactions. Many models include the idea that a source uses a coding system to express information in the form of a message. The message is sent through a channel to a receiver who has to decode it to understand it. The main field of inquiry investigating communication is called communication studies.\nA common way to classify communication is by whether information is exchanged between humans, members of other species, or non-living entities such as computers. For human communication, a central contrast is between verbal and non-verbal communication. Verbal communication involves the exchange of messages in linguistic form, including spoken and written messages as well as sign language. Non-verbal communication happens without the use of a linguistic system, for example, using body language, touch, and facial expressions. Another distinction is between interpersonal communication, which happens between distinct persons, and intrapersonal communication, which is communication with oneself. Communicative competence is the ability to communicate well and applies to the skills of formulating messages and understanding them.\nNon-human forms of communication include animal and plant communication. Researchers in this field often refine their definition of communicative behavior by including the criteria that observable responses are present and that the participants benefit from the exchange. Animal communication is used in areas like courtship and mating, parent\u2013offspring relations, navigation, and self-defense. Communication through chemicals is particularly important for the relatively immobile plants. For example, maple trees release so-called volatile organic compounds into the air to warn other plants of a herbivore attack. Most communication takes place between members of the same species. The reason is that its purpose is usually some form of cooperation, which is not as common between different species. Interspecies communication happens mainly in cases of symbiotic relationships. For instance, many flowers use symmetrical shapes and distinctive colors to signal to insects where nectar is located. Humans engage in interspecies communication when interacting with pets and working animals.\nHuman communication has a long history and how people exchange information has changed over time. These changes were usually triggered by the development of new communication technologies. Examples are the invention of writing systems, the development of mass printing, the use of radio and television, and the invention of the internet. The technological advances also led to new forms of communication, such as the exchange of data between computers."
    },
    {
      "id": "37438",
      "title": "Complex system",
      "url": "https://en.wikipedia.org/wiki/Complex_system",
      "summary": "A complex system is a system composed of many components which may interact with each other. Examples of complex systems are Earth's global climate, organisms, the human brain, infrastructure such as power grid, transportation or communication systems, complex software and electronic systems, social and economic organizations (like cities), an ecosystem, a living cell, and ultimately the entire universe.Complex systems are systems whose behavior is intrinsically difficult to model due to the dependencies, competitions, relationships, or other types of interactions between their parts or between a given system and its environment. Systems that are \"complex\" have distinct properties that arise from these relationships, such as nonlinearity, emergence, spontaneous order, adaptation, and feedback loops, among others. Because such systems appear in a wide variety of fields, the commonalities among them have become the topic of their independent area of research. In many cases, it is useful to represent such a system as a network where the nodes represent the components and links to their interactions.\nThe term complex systems often refers to the study of complex systems, which is an approach to science that investigates how relationships between a system's parts give rise to its collective behaviors and how the system interacts and forms relationships with its environment. The study of complex systems regards collective, or system-wide, behaviors as the fundamental object of study; for this reason, complex systems can be understood as an alternative paradigm to reductionism, which attempts to explain systems in terms of their constituent parts and the individual interactions between them.\nAs an interdisciplinary domain, complex systems draws contributions from many different fields, such as the study of self-organization and critical phenomena from physics, that of spontaneous order from the social sciences, chaos from mathematics, adaptation from biology, and many others. Complex systems is therefore often used as a broad term encompassing a research approach to problems in many diverse disciplines, including statistical physics, information theory, nonlinear dynamics, anthropology, computer science, meteorology, sociology, economics, psychology, and biology.\n\n"
    },
    {
      "id": "1181008",
      "title": "Computational science",
      "url": "https://en.wikipedia.org/wiki/Computational_science",
      "summary": "Computational science, also known as scientific computing, technical computing or scientific computation (SC), is a division of science that uses advanced computing capabilities to understand and solve complex physical problems. This includes \n\nAlgorithms (numerical and non-numerical): mathematical models, computational models, and computer simulations developed to solve sciences (e.g, physical, biological, and social), engineering, and humanities problems\nComputer hardware that develops and optimizes the advanced system hardware, firmware, networking, and data management components needed to solve computationally demanding problems\nThe computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information scienceIn practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiments, which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs and application software that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.\n\n"
    },
    {
      "id": "5311",
      "title": "Computer programming",
      "url": "https://en.wikipedia.org/wiki/Computer_programming",
      "summary": "Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\nAuxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process \u2013 with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.\n\n"
    },
    {
      "id": "45443335",
      "title": "DJ Patil",
      "url": "https://en.wikipedia.org/wiki/DJ_Patil",
      "summary": "Dhanurjay \"DJ\" Patil (born August 3, 1974) is an American mathematician and computer scientist who served as the Chief Data Scientist of the United States Office of Science and Technology Policy from 2015 to 2017. He is the Head of Technology for Devoted Health. \nHe previously served as the Vice President of Product at RelateIQ, which was acquired by Salesforce.com, as Chief Product Officer of Color Labs, and as Head of Data Products and Chief Scientist of LinkedIn. His father, Suhas Patil, is a venture capitalist and the founder of Cirrus Logic.\n\n"
    },
    {
      "id": "2234333",
      "title": "Data (computer science)",
      "url": "https://en.wikipedia.org/wiki/Data_(computer_science)",
      "summary": "In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data. Data requires interpretation to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation. In modern (post-1960) computer systems, all data is digital. \nData exists in three states: data at rest, data in transit and data in use. Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices.\nPhysical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures.\n\n"
    },
    {
      "id": "2588620",
      "title": "Data archaeology",
      "url": "https://en.wikipedia.org/wiki/Data_archaeology",
      "summary": "There are two conceptualisations of data archaeology, the technical definition and the social science definition.\nData archaeology (also data archeology) in the technical sense refers to the art and science of recovering computer data encoded and/or encrypted in now obsolete media or formats. Data archaeology can also refer to recovering information from damaged electronic formats after natural disasters or human error.\nIt entails the rescue and recovery of old data trapped in outdated, archaic or obsolete storage formats such as floppy disks, magnetic tape, punch cards and transforming/transferring that data to more usable formats.\nData archaeology in the social sciences usually involves an investigation into the source and history of datasets and the construction of these datasets. It involves mapping out the entire lineage of data, its nature and characteristics, its quality and veracity and how these affect the analysis and interpretation of the dataset.\nThe findings of performing data archaeology affect the level to which the conclusions parsed from data analysis can be trusted.The term data archaeology originally appeared in 1993 as part of the Global Oceanographic Data Archaeology and Rescue Project (GODAR). The original impetus for data archaeology came from the need to recover computerised records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of climate change. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the Nimbus 2 satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.NASA also utilises the services of data archaeologists to recover information stored on 1960s-era vintage computer tape, as exemplified by the Lunar Orbiter Image Recovery Project (LOIRP)."
    },
    {
      "id": "51443362",
      "title": "Data augmentation",
      "url": "https://en.wikipedia.org/wiki/Data_augmentation",
      "summary": "Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model, achieved by training models on several slightly-modified copies of existing data.\n\n"
    },
    {
      "id": "3575651",
      "title": "Data cleansing",
      "url": "https://en.wikipedia.org/wiki/Data_cleansing",
      "summary": "Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.\nAfter cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.\nThe actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of \"varying file formats, naming conventions, and columns\", and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (\"st, rd, etc.\" to \"street, road, etcetera\").\n\n"
    },
    {
      "id": "11501746",
      "title": "Data collection",
      "url": "https://en.wikipedia.org/wiki/Data_collection",
      "summary": "Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities, and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture evidence that allows data analysis to lead to the formulation of credible answers to the questions that have been posed.\nRegardless of the field of or preference for defining data (quantitative or qualitative), accurate data collection is essential to maintain research integrity. The selection of appropriate data collection instruments (existing, modified, or newly developed) and delineated instructions for their correct use reduce the likelihood of errors.\n\n"
    },
    {
      "id": "8013",
      "title": "Data compression",
      "url": "https://en.wikipedia.org/wiki/Data_compression",
      "summary": "In information theory, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder.\nThe process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding: encoding is done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal.\nCompression is useful because it reduces the resources required to store and transmit data. Computational resources are consumed in the compression and decompression processes. Data compression is subject to a space-time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data."
    },
    {
      "id": "1040512",
      "title": "Data corruption",
      "url": "https://en.wikipedia.org/wiki/Data_corruption",
      "summary": "Data corruption refers to errors in computer data that occur during writing, reading, storage, transmission, or processing, which introduce unintended changes to the original data. Computer, transmission, and storage systems use a number of measures to provide end-to-end data integrity, or lack of errors.\nIn general, when data corruption occurs, a file containing that data will produce unexpected results when accessed by the system or the related application. Results could range from a minor loss of data to a system crash. For example, if a document file is corrupted, when a person tries to open that file with a document editor they may get an error message, thus the file might not be opened or might open with some of the data corrupted (or in some cases, completely corrupted, leaving the document unintelligible). The adjacent image is a corrupted image file in which most of the information has been lost.\nSome types of malware may intentionally corrupt files as part of their payloads, usually by overwriting them with inoperative or garbage code, while a non-malicious virus may also unintentionally corrupt files when it accesses them. If a virus or trojan with this payload method manages to alter files critical to the running of the computer's operating system software or physical hardware, the entire system may be rendered unusable.\nSome programs can give a suggestion to repair the file automatically (after the error), and some programs cannot repair it. It depends on the level of corruption, and the built-in functionality of the application to handle the error. There are various causes of the corruption."
    },
    {
      "id": "23943140",
      "title": "Data curation",
      "url": "https://en.wikipedia.org/wiki/Data_curation",
      "summary": "Data curation is the organization and integration of data collected from various sources. It involves annotation, publication and presentation of the data so that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes \"all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data\". In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database.In the modern era of big data, the curation of data has become more prominent, particularly for software processing high volume and complex data systems.  The term is also used in historical occasions and the humanities, where increasing cultural and scholarly data from digital humanities projects requires the expertise and analytical practices of data curation. In broad terms, curation means a range of activities and processes done to create, manage, maintain, and validate a component. Specifically, data curation is the attempt to determine what information is worth saving and for how long.\n\n"
    },
    {
      "id": "372381",
      "title": "Data degradation",
      "url": "https://en.wikipedia.org/wiki/Data_degradation",
      "summary": "Data degradation is the gradual corruption of computer data due to an accumulation of non-critical failures in a data storage device. The phenomenon is also known as data decay, data rot or bit rot. This process leads to the slow deterioration of data quality over time, even if the data is not actively being used or accessed."
    },
    {
      "id": "42906439",
      "title": "Data editing",
      "url": "https://en.wikipedia.org/wiki/Data_editing",
      "summary": "Data editing is defined as the process involving the review and adjustment of collected survey data. Data editing helps define guidelines that will reduce potential bias and ensure consistent estimates leading to a clear analysis of the data set by correct inconsistent data using the methods later in this article. The purpose is to control the quality of the collected data. Data editing can be performed manually, with the assistance of a computer or a combination of both.\n\n"
    },
    {
      "id": "12097860",
      "title": "Data extraction",
      "url": "https://en.wikipedia.org/wiki/Data_extraction",
      "summary": "Data extraction is the act or process of retrieving data out of (usually unstructured or poorly structured) data sources for further data processing or data storage (data migration). The import into the intermediate extracting system is thus usually followed by data transformation and possibly the addition of metadata prior to export to another stage in the data workflow.\nUsually, the term data extraction is applied when (experimental) data is first imported into a computer from primary sources, like measuring or recording devices. Today's electronic devices will usually present an electrical connector (e.g. USB) through which 'raw data' can be streamed into a personal computer.\n\n"
    },
    {
      "id": "2690471",
      "title": "Data farming",
      "url": "https://en.wikipedia.org/wiki/Data_farming",
      "summary": "Data farming  is the process of using designed computational experiments to \u201cgrow\u201d data, which can then be analyzed using statistical and visualization techniques to obtain insight into complex systems.  These methods can be applied to any computational model.\nData farming differs from Data mining, as the following metaphors indicate: \n\nMiners seek valuable nuggets of ore buried in the earth, but have no control over what is out there or how hard it is to extract the nuggets from their surroundings. ...  Similarly, data miners seek to uncover valuable nuggets of information buried within massive amounts of data.  Data-mining techniques use statistical and graphical measures to try to identify interesting correlations or clusters in the data set.\nFarmers cultivate the land to maximize their yield.  They manipulate the environment to their advantage using irrigation, pest control, crop rotation, fertilizer, and more.  Small-scale designed experiments let them determine whether these treatments are effective.  Similarly, data farmers manipulate simulation models to their advantage, using large-scale designed experimentation to grow data from their models in a manner that easily lets them extract useful information. ...the results can reveal root cause-and-effect relationships between the model input factors and the model responses, in addition to rich graphical and statistical views of these relationships.\n\nA NATO modeling and simulation task group has documented the data farming process in the Final Report of MSG-088.\nHere, data farming uses collaborative processes in combining rapid scenario prototyping, simulation modeling, design of experiments, high performance computing, and analysis and visualization in an iterative loop-of-loops Archived 2015-08-29 at the Wayback Machine.\n\n"
    },
    {
      "id": "9328883",
      "title": "Data format management",
      "url": "https://en.wikipedia.org/wiki/Data_format_management",
      "summary": "Data format management (DFM) is the application of a systematic approach to the selection and use of the data formats used to encode information for storage on a computer. \nIn practical terms, data format management is the analysis of data formats and their associated technical, legal or economic attributes which can either enhance or detract from the ability of a digital asset or a given information systems to meet specified objectives.\nData format management is necessary as the amount of information and number of people creating it grows. This is especially the case as the information with which users are working is difficult to generate, store, costly to acquire, or to be shared.\nData format management as an analytic tool or approach is data format neutral.    \nHistorically individuals, organization and businesses have been categorized by their type of computer or their operating system.  Today, however, it is primarily productivity software, such as spreadsheet or word processor programs, and the way these programs store information that also defines an entity. For instance, when browsing the web it is not important which kind of computer is responsible for hosting a site, only that the information it publishes is in a format that is readable by the viewing browser. In this instance the data format of the published information has more to do with defining compatibilities than the underlying hardware or operating system. \nSeveral initiatives have been established to record those data formats commonly used and the software available to read them, for example the Pronom project at the UK National Archives.\n\n"
    },
    {
      "id": "7176679",
      "title": "Data fusion",
      "url": "https://en.wikipedia.org/wiki/Data_fusion",
      "summary": "Data fusion is the process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source.\nData fusion processes are often categorized as low, intermediate, or high, depending on the processing stage at which fusion takes place. Low-level data fusion combines several sources of raw data to produce new raw data. The expectation is that fused data is more informative and synthetic than the original inputs.\nFor example, sensor fusion is also known as (multi-sensor) data fusion and is a subset of information fusion.\nThe concept of data fusion has origins in the evolved capacity of humans and animals to incorporate information from multiple senses to improve their ability to survive. For example, a combination of sight, touch, smell, and taste may indicate whether a substance is edible.\n\n"
    },
    {
      "id": "4780372",
      "title": "Data integration",
      "url": "https://en.wikipedia.org/wiki/Data_integration",
      "summary": "Data integration involves combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains.  Data integration appears with increasing frequency as the volume, complexity (that is, big data) and the need to share existing data explodes.  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved. Data integration encourages collaboration between internal as well as external users. The data being integrated must be received from a heterogeneous database system and transformed to a single coherent data store that provides synchronous data across a network of files for clients. A common use of data integration is in data mining when analyzing and extracting information from existing databases that can be useful for Business information.\n\n"
    },
    {
      "id": "40995",
      "title": "Data integrity",
      "url": "https://en.wikipedia.org/wiki/Data_integrity",
      "summary": "Data integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle. It is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation is a prerequisite for data integrity.\n\n"
    },
    {
      "id": "24810701",
      "title": "PANGAEA (data library)",
      "url": "https://en.wikipedia.org/wiki/PANGAEA_(data_library)",
      "summary": "PANGAEA - Data Publisher for Earth & Environmental Science is a digital data library and a data publisher for earth system science. Data can be georeferenced in time (date/time or geological age) and space (latitude, longitude, depth/height).\nScientific data are archived with related metainformation in a relational database (Sybase) through an editorial system. Data are in Open Access and are distributed through web services in standard formats on the Internet through various search engines and portals. Data set descriptions (metadata) are conform to the ISO 19115 standard and are served in various further formats (e.g. Directory Interchange Format, Dublin Core). They include a bibliographic citation and are persistently identified using Digital Object Identifiers (DOI). Identifier provision and long-term availability of data sets via library catalogs is ensured through a cooperation with the German National Library of Science and Technology (TIB). Retrieval of data sets is provided through a full text search engine (based on Apache Lucene / panFMP). For efficient data compilations a data warehouse is operated. Data descriptions are available through various protocols (OAI-PMH, Web Catalog Service).\nPANGAEA is hosted by the Alfred Wegener Institute, Helmholtz Centre for Polar and Marine Research (AWI), Bremerhaven and the MARUM \u2013 Center for Marine Environmental Sciences, Bremen in Germany. The system is used by various international research projects from public funding as data repository and by the World Data Center for Marine Environmental Sciences (WDC-MARE) as long-term archive. The system was initially developed since 1987 and is operational on the Internet since 1995.\nThe MediaWiki software is used to operate a wiki as PANGAEA manual and reference.\nPANGAEA is also listed in re3data.org.\n\n"
    },
    {
      "id": "44783487",
      "title": "Data lineage",
      "url": "https://en.wikipedia.org/wiki/Data_lineage",
      "summary": "Data lineage includes the data origin, what happens to it, and where it moves over time. Data lineage provides visibility and simplifies tracing errors back to the root cause in a data analytics process.It also enables replaying specific portions or inputs of the data flow for step-wise debugging or regenerating lost output. Database systems use such information, called data provenance, to address similar validation and debugging challenges. Data provenance refers to records of the inputs, entities, systems, and processes that influence data of interest, providing a historical record of the data and its origins. The generated evidence supports forensic activities such as data-dependency analysis, error/compromise detection and recovery, auditing, and compliance analysis. \"Lineage is a simple type of why provenance.\"Data lineage can be represented visually to discover the data flow/movement from its source to destination via various changes and hops on its way in the enterprise environment, how the data gets transformed along the way, how the representation and parameters change, and how the data splits or converges after each hop. A simple representation of the Data Lineage can be shown with dots and lines, where dot represents a data container for data points and lines connecting them represents the transformations the data point undergoes, between the data containers.\nRepresentation broadly depends on the scope of the metadata management and reference point of interest. Data lineage provides sources of the data and intermediate data flow hops from the reference point with backward data lineage, leading to the final destination's data points and its intermediate data flows with forward data lineage. These views can be combined with end-to-end lineage for a reference point that provides a complete audit trail of that data point of interest from sources to their final destinations. As the data points or hops increase, the complexity of such representation becomes incomprehensible. Thus, the best feature of the data lineage view would be to be able to simplify the view by temporarily masking unwanted peripheral data points. Tools that have the masking feature enable scalability of the view and enhance analysis with the best user experience for both technical and business users. Data lineage also enables companies to trace sources of specific business data for the purposes of tracking errors, implementing changes in processes, and implementing system migrations to save significant amounts of time and resources, thereby tremendously improving BI efficiency.The scope of the data lineage determines the volume of metadata required to represent its data lineage. Usually, data governance, and data management determines the scope of the data lineage based on their regulations, enterprise data management strategy, data impact, reporting attributes, and critical data elements of the organization.\nData lineage provides the audit trail of the data points at the highest granular level, but presentation of the lineage may be done at various zoom levels to simplify the vast information, similar to analytic web maps. Data Lineage can be visualized at various levels based on the granularity of the view. At a very high level data lineage provides what systems the data interacts before it reaches destination. As the granularity increases it goes up to the data point level where it can provide the details of the data point and its historical behavior, attribute properties, and trends and data quality of the data passed through that specific data point in the data lineage.\nData governance plays a key role in metadata management for guidelines, strategies, policies, implementation. Data quality, and master data management helps in enriching the data lineage with more business value. Even though the final representation of data lineage is provided in one interface but the way the metadata is harvested and exposed to the data lineage graphical user interface could be entirely different. Thus, data lineage can be broadly divided into three categories based on the way metadata is harvested: data lineage involving software packages for structured data, programming languages, and big data.\nData lineage information includes technical metadata involving data transformations. Enriched data lineage information may include data quality test results, reference data values, data models, business vocabulary, data stewards, program management information, and enterprise information systems linked to the data points and transformations. Masking feature in the data lineage visualization allows the tools to incorporate all the enrichments that matter for the specific use case. To represent disparate systems into one common view, \"metadata normalization\" or standardization may be necessary.\n\n"
    },
    {
      "id": "1381282",
      "title": "Data loss",
      "url": "https://en.wikipedia.org/wiki/Data_loss",
      "summary": "Data loss is an error condition in information systems in which information is destroyed by failures (like failed spindle motors or head crashes on hard drives) or neglect (like mishandling, careless handling or storage under unsuitable conditions) in storage, transmission, or processing. Information systems implement backup and disaster recovery equipment and processes to prevent data loss or restore lost data. Data loss can also occur if the physical medium containing the data is lost or stolen. \nData loss is distinguished from data unavailability, which may arise from a network outage. Although the two have substantially similar consequences for users, data unavailability is temporary, while data loss may be permanent. Data loss is also distinct from data breach, an incident where data falls into the wrong hands, although the term data loss has been used in those incidents.\n\n"
    },
    {
      "id": "759312",
      "title": "Data management",
      "url": "https://en.wikipedia.org/wiki/Data_management",
      "summary": "Data management comprises all disciplines related to handling data as a valuable resource, it is  the practice of managing an organization\u2019s data so it can be analyzed for decision making.\n\n"
    },
    {
      "id": "1135408",
      "title": "Data migration",
      "url": "https://en.wikipedia.org/wiki/Data_migration",
      "summary": "Data migration is the process of selecting, preparing, extracting, and transforming data and permanently transferring it from one computer storage system to another. Additionally, the validation of migrated data for completeness and the decommissioning of legacy data storage are considered part of the entire data migration process. Data migration is a key consideration for any system implementation, upgrade, or consolidation, and it is typically performed in such a way as to be as automated as possible, freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, application migration, website consolidation, disaster recovery, and data center relocation.\n\n"
    },
    {
      "id": "49882988",
      "title": "Data philanthropy",
      "url": "https://en.wikipedia.org/wiki/Data_philanthropy",
      "summary": "Data philanthropy describes a form of collaboration in which private sector companies share data for public benefit. There are multiple uses of data philanthropy being explored from humanitarian, corporate, human rights, and academic use. Since introducing the term in 2011, the United Nations Global Pulse has advocated for a global \"data philanthropy movement\".\n\n"
    },
    {
      "id": "12386904",
      "title": "Data Preprocessing",
      "url": "https://en.wikipedia.org/wiki/Data_Preprocessing",
      "summary": "Data preprocessing can refer to manipulation, filtration or augmentation of data before it is analyzed, and is often an important step in the data mining process. Data collection methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, amongst other issues.\nThe preprocessing pipeline used can often have large effects on the conclusions drawn from the downstream analysis. Thus, representation and quality of data is necessary before running any analysis. \nOften, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is a high proportion of irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase may be more difficult. Data preparation and filtering steps can take a considerable amount of processing time. Examples of methods used in data preprocessing include cleaning, instance selection, normalization, one-hot encoding, data transformation, feature extraction and feature selection.\n\n"
    },
    {
      "id": "55810614",
      "title": "Data preservation",
      "url": "https://en.wikipedia.org/wiki/Data_preservation",
      "summary": "Data preservation is the act of conserving and maintaining both the safety and integrity of data. Preservation is done through formal activities that are governed by policies, regulations and strategies directed towards protecting and prolonging the existence and authenticity of data and its metadata. Data can be described as the elements or units in which knowledge and information is created,\n and metadata are the summarizing subsets of the elements of data; or the data about the data. The main goal of data preservation is to protect data from being lost or destroyed and to contribute to the reuse and progression of the data.\n\n"
    },
    {
      "id": "42813835",
      "title": "Data publishing",
      "url": "https://en.wikipedia.org/wiki/Data_publishing",
      "summary": "Data publishing (also data publication) is the act of releasing research data in published form for use by others. It is a practice consisting in preparing certain data or data set(s) for public use thus to make them available to everyone to use as they wish. \nThis practice is an integral part of the open science movement. \nThere is a large and multidisciplinary consensus on the benefits resulting from this practice.The main goal is to elevate data to be first class research outputs. There are a number of initiatives underway as well as points of consensus and issues still in contention.There are several distinct ways to make research data available, including: \n\npublishing data as supplemental material associated with a research article, typically with the data files hosted by the publisher of the article\nhosting data on a publicly available website, with files available for download\nhosting data in a repository that has been developed to support data publication, e.g. figshare, Dryad, Dataverse, Zenodo. A large number of general and specialty (such as by research topic) data repositories exist. For example, the UK Data Service enables users to deposit data collections and re-share these for research purposes.\npublishing a data paper about the dataset, which may be published as a preprint, in a regular journal, or in a data journal that is dedicated to supporting data papers. The data may be hosted by the journal or hosted separately in a data repository.Publishing data allows researchers to both make their data available to others to use, and enables datasets to be cited similarly to other research publication types (such as articles or books), thereby enabling producers of datasets to gain academic credit for their work.\nThe motivations for publishing data may range for a desire to make research more accessible, to enable citability of datasets, or research funder or publisher mandates that require open data publishing. The UK Data Service is one key organisation working with others to raise the importance of citing data correctly and helping researchers to do so.\nSolutions to preserve privacy within data publishing has been proposed, including privacy protection algorithms, data \u201dmasking\u201d methods, and regional privacy level calculation algorithm.\n\n"
    },
    {
      "id": "1609808",
      "title": "Data quality",
      "url": "https://en.wikipedia.org/wiki/Data_quality",
      "summary": "Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is \"fit for [its] intended uses in operations, decision making and planning\". Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.\n\n"
    },
    {
      "id": "2160183",
      "title": "Data recovery",
      "url": "https://en.wikipedia.org/wiki/Data_recovery",
      "summary": "In computing, data recovery is a process of retrieving deleted, inaccessible, lost, corrupted, damaged, or formatted data from secondary storage, removable media or files, when the data stored in them cannot be accessed in a usual way.  The data is most often salvaged from storage media such as internal or external hard disk drives (HDDs), solid-state drives (SSDs), USB flash drives, magnetic tapes, CDs, DVDs, RAID subsystems, and other electronic devices. Recovery may be required due to physical damage to the storage devices or logical damage to the file system that prevents it from being mounted by the host operating system (OS).Logical failures occur when the hard drive devices are functional but the user or automated-OS cannot retrieve or access data stored on them. Logical failures can occur due to corruption of the engineering chip, lost partitions, firmware failure, or failures during formatting/re-installation.Data recovery can be a very simple or technical challenge. This is why there are specific software companies specialized in this field.\n\n"
    },
    {
      "id": "23789529",
      "title": "Data reduction",
      "url": "https://en.wikipedia.org/wiki/Data_reduction",
      "summary": "Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The purpose of data reduction can be two-fold: reduce the number of data records by eliminating invalid data or produce summary data and statistics at different aggregation levels for various applications. Data reduction does not necessarily mean loss of information. For example, the body mass index reduces two dimensions (body and mass) into a single measure, without any information being lost in the process.\nWhen information is derived from instrument readings there may also be a transformation from analog to digital form. When the data are already in digital form the 'reduction' of the data typically involves some editing, scaling, encoding, sorting, collating, and producing tabular summaries. When the observations are discrete but the underlying phenomenon is continuous then smoothing and interpolation are often needed. The data reduction is often undertaken in the presence of reading or measurement errors. Some idea of the nature of these errors is needed before the most likely value may be determined.\nAn example in astronomy is the data reduction in the Kepler satellite. This satellite records 95-megapixel images once every six seconds, generating dozens of megabytes of data per second, which is orders-of-magnitudes more than the downlink bandwidth of 550 kB/s. The on-board data reduction encompasses co-adding the raw frames for thirty minutes, reducing the bandwidth by a factor of 300. Furthermore, interesting targets are pre-selected and only the relevant pixels are processed, which is 6% of the total. This reduced data is then sent to Earth where it is processed further.\nResearch has also been carried out on the use of data reduction in wearable (wireless) devices for health monitoring and diagnosis applications. For example, in the context of epilepsy diagnosis, data reduction has been used to increase the battery lifetime of a wearable EEG device by selecting and only transmitting EEG data that is relevant for diagnosis and discarding background activity.\n\n"
    },
    {
      "id": "3095080",
      "title": "Data retention",
      "url": "https://en.wikipedia.org/wiki/Data_retention",
      "summary": "Data retention defines the policies of persistent data and records management for meeting legal and business data archival requirements. Although sometimes interchangeable, it is not to be confused with the Data Protection Act 1998.\nThe different data retention policies weigh legal and privacy concerns economics and need-to-know concerns to determine the retention time, archival rules, data formats, and the permissible means of storage, access, and encryption."
    },
    {
      "id": "46471245",
      "title": "Data scraping",
      "url": "https://en.wikipedia.org/wiki/Data_scraping",
      "summary": "Data scraping is a technique where a computer program extracts data from human-readable output coming from another program."
    },
    {
      "id": "3575656",
      "title": "Data scrubbing",
      "url": "https://en.wikipedia.org/wiki/Data_scrubbing",
      "summary": "Data scrubbing is an error correction technique that uses a background task to periodically inspect main memory or storage for errors, then corrects detected errors using redundant data in the form of different checksums or copies of data. Data scrubbing reduces the likelihood that single correctable errors will accumulate, leading to reduced risks of uncorrectable errors.\nData integrity is a high-priority concern in writing, reading, storage, transmission, or processing of the computer data in computer operating systems and in computer storage and data transmission systems.  However, only a few of the currently existing and used file systems provide sufficient protection against data corruption.To address this issue, data scrubbing provides routine checks of all inconsistencies in data and, in general, prevention of hardware or software failure. This \"scrubbing\" feature occurs commonly in memory, disk arrays, file systems, or FPGAs as a mechanism of error detection and correction.\n\n"
    },
    {
      "id": "1157832",
      "title": "Data security",
      "url": "https://en.wikipedia.org/wiki/Data_security",
      "summary": "Data security  means protecting digital data, such as those in a database, from destructive forces and from the unwanted actions of unauthorized users, such as a cyberattack or a data breach.\n\n"
    },
    {
      "id": "8495",
      "title": "Data set",
      "url": "https://en.wikipedia.org/wiki/Data_set",
      "summary": "A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set.  Data sets can also consist of a collection of documents or files.In the open data discipline, data set is the unit to measure the information released in a public open data repository.  The European data.europa.eu portal aggregates more than a million data sets.\n\n"
    },
    {
      "id": "60798295",
      "title": "Data sonification",
      "url": "https://en.wikipedia.org/wiki/Data_sonification",
      "summary": "Data sonification is the presentation of data as sound using sonification. It is the auditory equivalent of the more established practice of data visualization.\nThe usual process for data sonification is directing digital media of a dataset through a software synthesizer and into a digital-to-analog converter to produce sound for humans to experience.Applications of data sonification include astronomy studies of star creation, interpreting cluster analysis, and geoscience.Various projects describe the production of sonifications as a collaboration between scientists and musicians.A target demographic for using data sonification is the blind community because of the inaccessibility of data visualizations.\n\n"
    },
    {
      "id": "6212365",
      "title": "Data steward",
      "url": "https://en.wikipedia.org/wiki/Data_steward",
      "summary": "A data steward is an oversight or data governance role within an organization, and is responsible for ensuring the quality and fitness for purpose of the organization's data assets, including the metadata for those data assets.  A data steward may share some responsibilities with a data custodian, such as the awareness, accessibility, release, appropriate use, security and management of data.  A data steward would also participate in the development and implementation of data assets.  A data steward may seek to improve the quality and fitness for purpose of other data assets their organization depends upon but is not responsible for.\nData stewards have a specialist role that utilizes an organization's data governance processes, policies, guidelines and responsibilities for administering an organizations' entire data in compliance with policy and/or regulatory obligations. The overall objective of a data steward is the data quality of the data assets, datasets, data records and data elements. This includes documenting metainformation for the data, such as definitions, related rules/governance, physical manifestation, and related data models (most of these properties being specific to an attribute/concept relationship), identifying owners/custodian's various responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules.\nData stewards begin the stewarding process with the identification of the data assets and elements which they will steward, with the ultimate result being standards, controls and data entry.  The steward works closely with business glossary standards analysts (for standards), with data architect/modelers (for standards), with  DQ analysts (for controls) and with operations team members (good-quality data going in per business rules) while entering data.\nData stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.  Master data management often makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.\n\n"
    },
    {
      "id": "28174",
      "title": "Data storage",
      "url": "https://en.wikipedia.org/wiki/Data_storage",
      "summary": "Data storage is the recording (storing) of information (data) in a storage medium. Handwriting, phonographic recording, magnetic tape, and optical discs are all examples of storage media. Biological molecules such as RNA and DNA are considered by some as data storage. Recording may be accomplished with virtually any form of energy. Electronic data storage requires electrical power to store and retrieve data. \nData storage in a digital, machine-readable medium is sometimes called digital data. Computer data storage is one of the core functions of a general-purpose computer. Electronic documents can be stored in much less space than paper documents. Barcodes and magnetic ink character recognition (MICR) are two ways of recording machine-readable data on paper.\n\n"
    },
    {
      "id": "7360695",
      "title": "Data synchronization",
      "url": "https://en.wikipedia.org/wiki/Data_synchronization",
      "summary": "Data synchronization is the process of establishing consistency between source and target data stores, and the continuous harmonization of the data over time. It is fundamental to a wide variety of applications, including file synchronization and mobile device synchronization.\nData synchronization can also be useful in encryption for synchronizing public key servers.\nData synchronization is needed to update and keep multiple copies of a set of data coherent with one another or to maintain data integrity, Figure 3. For example, database replication is used to keep multiple copies of data synchronized with database servers that store data in different locations.\n\n"
    },
    {
      "id": "4080917",
      "title": "Data transformation (computing)",
      "url": "https://en.wikipedia.org/wiki/Data_transformation_(computing)",
      "summary": "In computing, data transformation is the process of converting data from one format or structure into another format or structure. It is a fundamental aspect of most data integration and data management tasks such as data wrangling, data warehousing, data integration and application integration.\nData transformation can be simple or complex based on the required changes to the data between the source (initial) data and the target (final) data. Data transformation is typically performed via a mixture of manual and automated steps. Tools and technologies used for data transformation can vary widely based on the format, structure, complexity, and volume of the data being transformed.\nA master data recast is another form of data transformation where the entire database of data values is transformed or recast without extracting the data from the database.  All data in a well designed database is directly or indirectly related to a limited set of master database tables by a network of foreign key constraints.  Each foreign key constraint is dependent upon a unique database index from the parent database table.  Therefore, when the proper master database table is recast with a different unique index, the directly and indirectly related data are also recast or restated.  The directly and indirectly related data may also still be viewed in the original form since the original unique index still exists with the master data.  Also, the database recast must be done in such a way as to not impact the applications architecture software.\nWhen the data mapping is indirect via a mediating data model, the process is also called data mediation.\n\n"
    },
    {
      "id": "1705399",
      "title": "Data validation",
      "url": "https://en.wikipedia.org/wiki/Data_validation",
      "summary": "In computing, data validation is the process of ensuring data has undergone data cleansing to confirm they have data quality, that is, that they are both correct and useful. It uses routines, often called \"validation rules\", \"validation constraints\", or \"check routines\", that check for correctness, meaningfulness, and security of data that are input to the system. The rules may be implemented through the automated facilities of a data dictionary, or by the inclusion of explicit application program validation logic of the computer and its application.\nThis is distinct from formal verification, which attempts to prove or disprove the correctness of algorithms for implementing a specification or property.\n\n"
    },
    {
      "id": "3461736",
      "title": "Data and information visualization",
      "url": "https://en.wikipedia.org/wiki/Data_and_information_visualization",
      "summary": "Data and information visualization (data viz/vis or info viz/vis) is the practice of designing and creating easy-to-communicate and easy-to-understand graphic or visual representations of a large amount of complex quantitative and qualitative data and information with the help of static, dynamic or interactive visual items. Typically based on data and information collected from a certain domain of expertise, these visualizations are intended for a broader audience to help them visually explore and discover, quickly understand, interpret and gain important insights into otherwise difficult-to-identify structures, relationships, correlations, local and global patterns, trends, variations, constancy, clusters, outliers and unusual groupings within data (exploratory visualization). When intended for the general public (mass communication) to convey a concise version of known, specific information in a clear and engaging manner (presentational or explanatory visualization), it is typically called information graphics.  \nData visualization is concerned with visually presenting sets of primarily quantitative raw data in a schematic form. The visual formats used in data visualization include tables, charts and graphs (e.g. pie charts, bar charts, line charts, area charts, cone charts, pyramid charts, donut charts, histograms, spectrograms, cohort charts, waterfall charts, funnel charts, bullet graphs, etc.), diagrams, plots (e.g. scatter plots, distribution plots, box-and-whisker plots), geospatial maps (such as proportional symbol maps, choropleth maps, isopleth maps and heat maps), figures, correlation matrices, percentage gauges, etc., which sometimes can be combined in a dashboard. \nInformation visualization, on the other hand, deals with multiple, large-scale and complicated datasets which contain quantitative (numerical) data as well as qualitative (non-numerical, i.e. verbal or graphical) and primarily abstract information and its goal is to add value to raw data, improve the viewers' comprehension, reinforce their cognition and help them derive insights and make decisions as they navigate and interact with the computer-supported graphical display. Visual tools used in information visualization include maps (such as tree maps), animations, infographics, Sankey diagrams, flow charts, network diagrams, semantic networks, entity-relationship diagrams, venn diagrams, timelines, mind maps, etc.\nEmerging technologies like virtual, augmented and mixed reality have the potential to make information visualization more immersive, intuitive, interactive and easily manipulable and thus enhance the user's visual perception and cognition. In data and information visualization, the goal is to graphically present and explore abstract, non-physical and non-spatial data collected from databases, information systems, file systems, documents, business and financial data, etc. (presentational and exploratory visualization) which is different from the field of scientific visualization, where the goal is to render realistic images based on physical and spatial scientific data to confirm or reject hypotheses (confirmatory visualization).Effective data visualization is properly sourced, contextualized, simple and uncluttered. The underlying data is accurate and up-to-date to make sure that insights are reliable. Graphical items are well-chosen for the given datasets and aesthetically appealing, with shapes, colors and other visual elements used deliberately in a meaningful and non-distracting manner. The visuals are accompanied by supporting texts (labels and titles). These verbal and graphical components complement each other to ensure clear, quick and memorable understanding. Effective information visualization is aware of the needs and concerns and the level of expertise of the target audience, deliberately guiding them to the intended conclusion. Such effective visualization can be used not only for conveying specialized, complex, big data-driven ideas to a wider group of non-technical audience in a visually appealing, engaging and accessible manner, but also to domain experts and executives for making decisions, monitoring performance, generating new ideas and stimulating research. In addition, data scientists, data analysts and data mining specialists use data visualization to check the quality of data, find errors, unusual gaps and missing values in data, clean data, explore the structures and features of data and assess outputs of data-driven models. In business, data and information visualization can constitute a part of data storytelling, where they are paired with a coherent narrative structure or storyline to contextualize the analyzed data and communicate the insights gained from analyzing the data clearly and memorably with the goal of convincing the audience into making a decision or taking an action in order to create business value. This can be contrasted with the field of statistical graphics, where complex statistical data are communicated graphically in an accurate and precise manner among researchers and analysts with statistical expertise to help them perform exploratory data analysis or to convey the results of such analyses, where visual appeal, capturing attention to a certain issue and storytelling are not as important.The field of data and information visualization is of interdisciplinary nature as it incorporates principles found in the disciplines of descriptive statistics (as early as the 18th century), visual communication, graphic design, cognitive science and, more recently, interactive computer graphics and human-computer interaction. Since effective visualization requires design skills, statistical skills and computing skills, it is argued by authors such as Gershon and Page that it is both an art and a science. The neighboring field of visual analytics marries statistical data analysis, data and information visualization and human analytical reasoning through interactive visual interfaces to help human users reach conclusions, gain actionable insights and make informed decisions which are otherwise difficult for computers to do.\nResearch into how people read and misread various types of visualizations is helping to determine what types and features of visualizations are most understandable and effective in conveying information. On the other hand, unintentionally poor or intentionally misleading and deceptive visualizations (misinformative visualization) can function as powerful tools which disseminate misinformation, manipulate public perception and divert public opinion toward a certain agenda. Thus data visualization literacy has become an important component of data and information literacy in the information age akin to the roles played by textual, mathematical and visual literacy in the past.\n\n"
    },
    {
      "id": "12487489",
      "title": "Data wrangling",
      "url": "https://en.wikipedia.org/wiki/Data_wrangling",
      "summary": "Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.\nThe process of data wrangling may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data wrangling typically follows a set of general steps which begin with extracting the data in a raw form from the data source, \"munging\" the raw data (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use. It is closely aligned with the ETL process. \n\n"
    },
    {
      "id": "265752",
      "title": "Decision-making",
      "url": "https://en.wikipedia.org/wiki/Decision-making",
      "summary": "In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action.\nResearch about decision-making is also published under the label problem solving, particularly in European psychological research."
    },
    {
      "id": "8501",
      "title": "Distributed computing",
      "url": "https://en.wikipedia.org/wiki/Distributed_computing",
      "summary": "A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. Distributed computing is a field of computer science that studies distributed systems. \nThe components of a distributed system interact with one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.\nA computer program that runs within a distributed system is called  a distributed program, and distributed programming is the process of writing such programs. There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing."
    },
    {
      "id": "750101",
      "title": "Domain knowledge",
      "url": "https://en.wikipedia.org/wiki/Domain_knowledge",
      "summary": "Domain knowledge is knowledge of a specific discipline or field  in contrast to general (or domain-independent) knowledge.  The term is often used in reference to a more general discipline\u2014for example, in describing a software engineer who has general knowledge of computer programming as well as domain knowledge about developing programs for a particular industry. People with domain knowledge are often regarded as specialists or experts in their field.\n\n"
    },
    {
      "id": "9545",
      "title": "Empirical research",
      "url": "https://en.wikipedia.org/wiki/Empirical_research",
      "summary": "Empirical research is research using empirical evidence. It is also a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values some research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions that cannot be studied in laboratory settings, particularly in the social sciences and in education.\nIn some fields, quantitative research may begin with a research question (e.g., \"Does listening to vocal music during the learning of a word list have an effect on later memory for these words?\") which is tested through experimentation. Usually, the researcher has a certain theory regarding the topic under investigation. Based on this theory, statements or hypotheses will be proposed (e.g., \"Listening to vocal music has a negative effect on learning a word list.\"). From these hypotheses, predictions about specific events are derived (e.g., \"People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence.\"). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.\n\n"
    },
    {
      "id": "72826783",
      "title": "Exploration",
      "url": "https://en.wikipedia.org/wiki/Exploration",
      "summary": "Exploration is the process of exploring, an activity which has some expectation of discovery. Organised exploration is largely a human activity, but exploratory activity is common to most organisms capable of directed locomotion and the ability to learn, and has been described in, amongst others, social insects foraging behaviour, where feedback from returning individuals affects the activity of other members of the group.Exploration has been defined as:\n\nTo travel somewhere in search of discovery.\nTo examine or investigate something systematically.\nTo examine diagnostically.\nTo (seek) experience first hand.\nTo wander without any particular aim or purpose.In all these definitions there is an implication of novelty, or unfamiliarity or the expectation of discovery in the exploration, whereas a survey implies directed examination, but not necessarily discovery of any previously unknown or unexpected information. The activities are not mutually exclusive, and often occur simultaneously to a variable extent. The same field of investigation or region may be explored at different times by different explorers with different motivations, who may make similar or different discoveries.\nIntrinsic exploration involves activity that is not directed towards a specific goal other than the activity itself.Extrinsic exploration has the same meaning as appetitive behavior. It is directed towards a specific goal.\n\n"
    },
    {
      "id": "46363781",
      "title": "Extract, load, transform",
      "url": "https://en.wikipedia.org/wiki/Extract,_load,_transform",
      "summary": "Extract, load, transform (ELT) is an alternative to extract, transform, load (ETL) used with data lake implementations. In contrast to ETL, in ELT models the data is not transformed on entry to the data lake, but stored in its original raw format. This enables faster loading times. However, ELT requires sufficient processing power within the data processing engine to carry out the transformation on demand, to return the results in a timely manner. Since the data is not processed on entry to the data lake, the query and schema do not need to be defined a priori (although often the schema will be available during load since many data sources are extracts from databases or similar structured data systems and hence have an associated schema). ELT is a data pipeline model.\n\n"
    },
    {
      "id": "46207323",
      "title": "Feature engineering",
      "url": "https://en.wikipedia.org/wiki/Feature_engineering",
      "summary": "Feature engineering or feature extraction  or feature discovery is the process of extracting features (characteristics, properties, attributes) from raw data to support training a downstream statistical model.Other examples of features in physics include the construction of dimensionless numbers such as the Reynolds number in fluid dynamics, the Nusselt number in heat transfer, the Archimedes number in sedimentation, and construction of first approximations of the solution such as analytical strength of materials solutions in mechanics."
    },
    {
      "id": "294894",
      "title": "Forbes",
      "url": "https://en.wikipedia.org/wiki/Forbes",
      "summary": "Forbes () is an American business magazine founded  by B.C. Forbes in 1917 and owned by Hong Kong-based investment group Integrated Whale Media Investments since 2014. Its chairperson and editor-in-chief is Steve Forbes, and its CEO is Mike Federle. It is based in Jersey City, New Jersey. Competitors in the national business magazine category include Fortune and Bloomberg Businessweek. \nPublished eight times a year, Forbes features articles on finance, industry, investing, and marketing topics. It also reports on related subjects such as technology, communications, science, politics, and law. It has an international edition in Asia as well as editions produced under license in 27 countries and regions worldwide. The magazine is known for its lists and rankings, including of the richest Americans (the Forbes 400), the 30 most notable young people under the age of 30 (Forbes 30 under 30), America's Wealthiest Celebrities, the world's top companies (the Forbes Global 2000), Forbes list of the World's Most Powerful People, and The World's Billionaires. The motto of Forbes magazine is \"Change the World\"."
    },
    {
      "id": "12799",
      "title": "Graphic design",
      "url": "https://en.wikipedia.org/wiki/Graphic_design",
      "summary": "Graphic design is a profession, academic discipline and applied art whose activity consists in projecting visual communications intended to transmit specific messages to social groups, with specific objectives. Graphic design is an interdisciplinary branch of design and of the fine arts. Its practice involves creativity, innovation and lateral thinking using manual or digital tools, where it is usual to use text and graphics to communicate visually.\nThe role of the graphic designer in the communication process is that of encoder or interpreter of the message. They work on the interpretation, ordering, and presentation of visual messages. Usually, graphic design uses the aesthetics of typography and the compositional arrangement of the text, ornamentation, and imagery to convey ideas, feelings, and attitudes beyond what language alone expresses. The design work can be based on a customer's demand, a demand that ends up being established linguistically, either orally or in writing, that is, that graphic design transforms a linguistic message into a graphic manifestation.Graphic design has, as a field of application, different areas of knowledge focused on any visual communication system. For example, it can be applied in advertising strategies, or it can also be applied in the aviation world or space exploration. In this sense, in some countries graphic design is related as only associated with the production of sketches and drawings, this is incorrect, since visual communication is a small part of a huge range of types and classes where it can be applied.\nWith origins in Antiquity and the Middle Ages, graphic design as applied art was initially linked to the boom of rise of printing in Europe in the 15th century and the growth of consumer culture in the Industrial Revolution. From there it emerged as a distinct profession in the West, closely associated with advertising in the 19th century and its evolution allowed its consolidation in the 20th century. Given the rapid and massive growth in information exchange today, the demand for experienced designers is greater than ever, particularly because of the development of new technologies and the need to pay attention to human factors beyond the competence of the engineers who develop them."
    },
    {
      "id": "1822696",
      "title": "Harvard Business Review",
      "url": "https://en.wikipedia.org/wiki/Harvard_Business_Review",
      "summary": "Harvard Business Review (HBR) is a general management magazine  published by Harvard Business Publishing, a not-for-profit, independent corporation that is an affiliate of Harvard Business School. HBR is published six times a year and is headquartered in Brighton, Massachusetts.\nHBR covers a wide range of topics that are relevant to various industries, management functions, and geographic locations. These include leadership, negotiation, strategy, operations, marketing, and finance.Harvard Business Review has published articles by Clayton Christensen, Peter F. Drucker, Justin Fox, Michael E. Porter, Rosabeth Moss Kanter, John Hagel III, Thomas H. Davenport, Gary Hamel, C. K. Prahalad, Vijay Govindarajan, Robert S. Kaplan, Rita Gunther McGrath and others. Several management concepts and business terms were first given prominence in HBR.\nHarvard Business Review's worldwide English-language circulation is 250,000. HBR licenses its content for publication in nine international editions."
    },
    {
      "id": "23534602",
      "title": "Human\u2013computer interaction",
      "url": "https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction",
      "summary": "Human\u2013computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a \"Human-computer Interface (HCI)\".\nAs a field of research, human\u2013computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human\u2013Computer Interaction. The first known use was in 1975 by Carlisle. The term is intended to convey that, unlike other tools with specific and limited uses, computers have many uses which often involve an open-ended dialogue between the user and the computer. The notion of dialogue likens human\u2013computer interaction to human-to-human interaction: an analogy that is crucial to theoretical considerations in the field."
    },
    {
      "id": "14919",
      "title": "ISBN",
      "url": "https://en.wikipedia.org/wiki/ISBN",
      "summary": "The International Standard Book Number (ISBN) is a numeric commercial book identifier that is intended to be unique. Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.An ISBN is assigned to each separate edition and variation (except reprintings) of a publication. For example, an e-book, a paperback and a hardcover edition of the same book must each have a different ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.\nThe initial ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the 9-digit SBN code can be converted to a 10-digit ISBN by prefixing it with a zero).\nPrivately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns such books ISBNs on its own initiative.Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.\n\n"
    },
    {
      "id": "234930",
      "title": "ISSN",
      "url": "https://en.wikipedia.org/wiki/ISSN",
      "summary": "An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication (periodical), such as a magazine. The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975. ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.\nWhen a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN). Consequently, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.\n\n"
    },
    {
      "id": "2862975",
      "title": "Informatik",
      "url": "https://en.wikipedia.org/wiki/Informatik",
      "summary": "Informatik formerly known as Inform\u00e4tik is an electro-industrial/futurepop duo from Boston that was formed in 1993 and is represented by Metropolis Records in the US and Dependent Records in Europe. The band were repeat contributors to the \"Mind/Body\" compilation series organized by participants of the rec.music.industrial Usenet group in the mid-nineties. Both members are vegans."
    },
    {
      "id": "14774",
      "title": "Information explosion",
      "url": "https://en.wikipedia.org/wiki/Information_explosion",
      "summary": "The information explosion is the rapid increase in the amount of published information or data and the effects of this abundance. As the amount of available data grows, the problem of managing the information becomes more difficult, which can lead to information overload. The Online Oxford English Dictionary indicates use of the phrase in a March 1964 New Statesman article. The New York Times first used the phrase in its editorial content in an article by Walter Sullivan on June 7, 1964, in which he described the phrase as \"much discussed\". (p11.)  The earliest known use of the phrase was in a speech about television by NBC president Pat Weaver at the Institute of Practitioners of Advertising in London on September 27, 1955. The speech was rebroadcast on radio station WSUI in Iowa and excerpted in the Daily Iowan newspaper two months later.Many sectors are seeing this rapid increase in the amount of information available such as healthcare, supermarkets, and governments. Another sector that is being affected by this phenomenon is journalism. Such a profession, which in the past was responsible for the dissemination of information, may be suppressed by the overabundance of information today.Techniques to gather knowledge from an overabundance of electronic information (e.g., data fusion may help in data mining) have existed since the 1970s. Another common technique to deal with such amount of information is qualitative research. Such approaches aim to organize the information, synthesizing, categorizing and systematizing in order to be more usable and easier to search.\n\n"
    },
    {
      "id": "149354",
      "title": "Information science",
      "url": "https://en.wikipedia.org/wiki/Information_science",
      "summary": "Information science (also known as information studies) is an academic field which is primarily concerned with analysis, collection, classification, manipulation, storage, retrieval, movement, dissemination, and protection of information. Practitioners within and outside the field study the  application and the usage of knowledge in organizations in addition to the interaction between people, organizations, and any existing information systems with the aim of creating, replacing, improving, or understanding the information systems.\nHistorically, information science (informatics) is associated with computer science, data science, psychology, technology, library science, healthcare, and intelligence agencies. However, information science also incorporates aspects of diverse fields such as archival science, cognitive science, commerce, law, linguistics, museology, management, mathematics, philosophy, public policy, and social sciences."
    },
    {
      "id": "15201",
      "title": "Interdisciplinarity",
      "url": "https://en.wikipedia.org/wiki/Interdisciplinarity",
      "summary": "Interdisciplinarity or interdisciplinary studies involves the combination of multiple academic disciplines into one activity (e.g., a research project). It draws knowledge from several other fields like sociology, anthropology, psychology, economics, etc.  It is about creating something by thinking across boundaries. It is related to an interdiscipline or an interdisciplinary field, which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. Large engineering teams are usually interdisciplinary, as a power station or mobile phone or other project requires the melding of several specialties. However, the term \"interdisciplinary\" is sometimes confined to academic settings.\nThe term interdisciplinary is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies\u2014along with their specific perspectives\u2014in the pursuit of a common task. The epidemiology of HIV/AIDS or global warming requires understanding of diverse disciplines to solve complex problems. Interdisciplinary may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields.\nThe adjective interdisciplinary is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines.  For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.\n\n"
    },
    {
      "id": "68833",
      "title": "Iteration",
      "url": "https://en.wikipedia.org/wiki/Iteration",
      "summary": "Iteration is the repetition of a process in order to generate a (possibly unbounded) sequence of outcomes. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration. \nIn mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms."
    },
    {
      "id": "48996671",
      "title": "Jeff Hammerbacher",
      "url": "https://en.wikipedia.org/wiki/Jeff_Hammerbacher",
      "summary": "Jeff Hammerbacher (born 1982 or 1983) is a data scientist. He was chief scientist and cofounder at Cloudera and later served on the faculty of the Icahn School of Medicine at Mount Sinai.\n\n"
    },
    {
      "id": "308367",
      "title": "Jim Gray (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist)",
      "summary": "James Nicholas Gray (1944 \u2013 declared dead in absentia 2012) was an American computer scientist who received the Turing Award in 1998 \"for seminal contributions to database and transaction processing research and technical leadership in system implementation\"."
    },
    {
      "id": "31074852",
      "title": "Journal of Computational and Graphical Statistics",
      "url": "https://en.wikipedia.org/wiki/Journal_of_Computational_and_Graphical_Statistics",
      "summary": "The Journal of Computational and Graphical Statistics is a quarterly peer-reviewed scientific journal published by Taylor & Francis on behalf of the American Statistical Association. Established in 1992, the journal covers the use of computational and graphical methods in statistics and data analysis, including numerical methods, graphical displays and methods, and perception. It is published jointly with the Institute of Mathematical Statistics and the Interface Foundation of North America. According to the Journal Citation Reports, the journal has a 2021 impact factor of 1.884."
    },
    {
      "id": "243391",
      "title": "Knowledge",
      "url": "https://en.wikipedia.org/wiki/Knowledge",
      "summary": "Knowledge is an awareness of facts, a familiarity with individuals and situations, or a practical skill. Knowledge of facts, also called propositional knowledge, is often characterized as true belief that is distinct from opinion or guesswork by virtue of justification. While there is wide agreement among philosophers that propositional knowledge is a form of true belief, many controversies focus on justification. This includes questions like how to understand justification, whether it is needed at all, and whether something else besides it is needed. These controversies intensified in the latter half of the 20th century due to a series of thought experiments that provoked alternative definitions.\nKnowledge can be produced in many ways. The main source of empirical knowledge is perception, which involves the usage of the senses to learn about the external world. Introspection allows people to learn about their internal mental states and processes. Other sources of knowledge include memory, rational intuition, inference, and testimony. According to foundationalism, some of these sources are basic in that they can justify beliefs, without depending on other mental states. Coherentists reject this claim and contend that a sufficient degree of coherence among all the mental states of the believer is necessary for knowledge. According to infinitism, an infinite chain of beliefs is needed.\nThe main discipline investigating knowledge is epistemology, which studies what people know, how they come to know it, and what it means to know something. It discusses the value of knowledge and the thesis of philosophical skepticism, which questions the possibility of knowledge. Knowledge is relevant to many fields like the sciences, which aim to acquire knowledge using the scientific method based on repeatable experimentation, observation, and measurement. Various religions hold that humans should seek knowledge and that God or the divine is the source of knowledge. The anthropology of knowledge studies how knowledge is acquired, stored, retrieved, and communicated in different cultures. The sociology of knowledge examines under what sociohistorical circumstances knowledge arises, and what sociological consequences it has. The history of knowledge investigates how knowledge in different fields has developed, and evolved, in the course of history.\n\n"
    },
    {
      "id": "31002435",
      "title": "Knowledge extraction",
      "url": "https://en.wikipedia.org/wiki/Knowledge_extraction",
      "summary": "Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\nThe RDB2RDF W3C group  is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).\n\n"
    },
    {
      "id": "233488",
      "title": "Machine learning",
      "url": "https://en.wikipedia.org/wiki/Machine_learning",
      "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to many fields including large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\n\n"
    },
    {
      "id": "3606435",
      "title": "Montpellier 2 University",
      "url": "https://en.wikipedia.org/wiki/Montpellier_2_University",
      "summary": "Montpellier 2 University (Universit\u00e9 Montpellier 2) was a French university in the acad\u00e9mie of Montpellier. It was one of the three universities formed in 1970 from the original University of Montpellier. Its main campus neighbors the Montpellier 3 University's main campus, and for this reason the nearest tramway station is named \"Universities of Sciences and Literature\" rather than \"University of Sciences\". In January 2015, Montpellier 1 University and Montpellier 2 University merged into the Montpellier University (Universit\u00e9 de Montpellier)."
    },
    {
      "id": "62500799",
      "title": "Nathan Yau",
      "url": "https://en.wikipedia.org/wiki/Nathan_Yau",
      "summary": "Nathan Yau is an American statistician and data visualization expert."
    },
    {
      "id": "3156699",
      "title": "National Science Board",
      "url": "https://en.wikipedia.org/wiki/National_Science_Board",
      "summary": "The National Science Board (NSB) of the United States establishes the policies of the National Science Foundation (NSF) within the framework of applicable national policies set forth by the president and the Congress. The NSB also serves as an independent policy advisory body to the president and Congress on science and engineering research and education issues. The board has a statutory obligation to \"...render to the President and to the Congress reports on specific, individual policy matters related to science and engineering and education in science engineering, as Congress or the President determines the need for such reports,\" (e.g. Science and Engineering Indicators; Report to Congress on Mid-scale Instrumentation at the National Science Foundation). All board members are presidential appointees. NSF's director serves as an ex officio 25th member and is appointed by the president and confirmed by the US Senate.\n\n"
    },
    {
      "id": "883885",
      "title": "OCLC",
      "url": "https://en.wikipedia.org/wiki/OCLC",
      "summary": "OCLC, Inc., doing business as OCLC, is an American nonprofit cooperative organization \"that provides shared technology services, original research, and community programs for its membership and the library community at large\". It was founded in 1967 as the Ohio College Library Center, then became the Online Computer Library Center as it expanded. In 2017, the name was formally changed to OCLC, Inc. OCLC and thousands of its member libraries cooperatively produce and maintain WorldCat, the largest online public access catalog in the world. OCLC is funded mainly by the fees that libraries pay (around $217.8 million annually in total as of 2021) for the many different services it offers. OCLC also maintains the Dewey Decimal Classification system."
    },
    {
      "id": "503009",
      "title": "PubMed",
      "url": "https://en.wikipedia.org/wiki/PubMed",
      "summary": "PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintain the database as part of the Entrez system of information retrieval.From 1971 to 1997, online access to the MEDLINE database had been primarily through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching. The PubMed system was offered free to the public starting in June 1997.\n\n"
    },
    {
      "id": "58731",
      "title": "Peter Naur",
      "url": "https://en.wikipedia.org/wiki/Peter_Naur",
      "summary": "Peter Naur (25 October 1928 \u2013 3 January 2016) was a Danish computer science pioneer and 2005 Turing award winner. He is best remembered as a contributor, with John Backus, to the Backus\u2013Naur form (BNF) notation used in describing the syntax for most programming languages. He also contributed to creating the language ALGOL 60.\n\n"
    },
    {
      "id": "60931",
      "title": "Phenomenon",
      "url": "https://en.wikipedia.org/wiki/Phenomenon",
      "summary": "A phenomenon  (pl.: phenomena), sometimes spelled phaenomenon, is an observable event. The term came into its modern philosophical usage through Immanuel Kant, who contrasted it with the noumenon, which cannot be directly observed. Kant was heavily influenced by Gottfried Wilhelm Leibniz in this part of his philosophy, in which phenomenon and noumenon serve as interrelated technical terms. Far predating this, the ancient Greek Pyrrhonist philosopher Sextus Empiricus also used phenomenon and noumenon as interrelated technical terms."
    },
    {
      "id": "1467948",
      "title": "Problem solving",
      "url": "https://en.wikipedia.org/wiki/Problem_solving",
      "summary": "Problem solving is the process of achieving a goal by overcoming obstacles, a frequent part of most activities. Problems in need of solutions range from simple personal tasks (e.g. how to turn on an appliance) to complex issues in business and technical fields. The former is an example of simple problem solving (SPS) addressing one issue, whereas the latter is complex problem solving (CPS) with multiple interrelated obstacles. Another classification of problem-solving tasks is into well-defined problems with specific obstacles and goals, and ill-defined problems in which the current situation is troublesome but it is not clear what kind of resolution to aim for. Similarly, one may distinguish formal or fact-based problems requiring psychometric intelligence, versus socio-emotional problems which depend on the changeable emotions of individuals or groups, such as tactful behavior, fashion, or gift choices.Solutions require sufficient resources and knowledge to attain the goal. Professionals such as lawyers, doctors, programmers, and consultants are largely problem solvers for issues that require technical skills and knowledge beyond general competence. Many businesses have found profitable markets by recognizing a problem and creating a solution: the more widespread and inconvenient the problem, the greater the opportunity to develop a scalable solution.\nThere are many specialized problem-solving techniques and methods in fields such as engineering, business, medicine, mathematics, computer science, philosophy, and social organization. The mental techniques to identify, analyze, and solve problems are studied in psychology and cognitive sciences. Also widely researched are the mental obstacles that prevent people from finding solutions; problem-solving impediments include confirmation bias, mental set, and functional fixedness."
    },
    {
      "id": "376707",
      "title": "R (programming language)",
      "url": "https://en.wikipedia.org/wiki/R_(programming_language)",
      "summary": "R is a programming language for statistical computing and data visualization. It has been adopted in the fields of data mining, bioinformatics, and data analysis.The core R language is augmented by a large number of extension packages, containing reusable code, documentation, and sample data.\nR software is open-source and free software. It is licensed by the GNU Project and available under the GNU General Public License. It is written primarily in C, Fortran, and R itself. Precompiled executables are provided for various operating systems.\nAs an interpreted language, R has a native command line interface. Moreover, multiple third-party graphical user interfaces are available, such as RStudio\u2014an integrated development environment\u2014and Jupyter\u2014a notebook interface."
    },
    {
      "id": "46951032",
      "title": "Scientific Data (journal)",
      "url": "https://en.wikipedia.org/wiki/Scientific_Data_(journal)",
      "summary": "Scientific Data is a peer-reviewed open access scientific journal published by Nature Research since 2014. It focuses on descriptions of data sets relevant to the natural sciences, medicine, engineering and social sciences, which are provided as machine-readable data, complemented with a human oriented narrative. The journal was not the first to publish data papers, but is one of a few journals whose content consists primarily of data papers. The journal is abstracted and indexed by Index Medicus/MEDLINE/PubMed.\n\n"
    },
    {
      "id": "26833",
      "title": "Scientific method",
      "url": "https://en.wikipedia.org/wiki/Scientific_method",
      "summary": "The scientific method is an empirical method for acquiring knowledge that has characterized the development of science since at least the 17th century. (For notable practitioners in previous centuries, see history of scientific method.) \nThe scientific method involves careful observation coupled with rigorous scepticism, because cognitive assumptions can distort the interpretation of the observation. Scientific inquiry includes creating a hypothesis through inductive reasoning, testing it through experiments and statistical analysis, and adjusting or discarding the hypothesis based on the results. \nThe above mentioned are principles of the scientific method, a definitive series of steps applicable to all scientific enterprises.Although procedures vary from one field of inquiry to another, the underlying process is frequently the same. The process in the scientific method involves making conjectures (hypothetical explanations), deriving predictions from the hypotheses as logical consequences, and then carrying out experiments or empirical observations based on those predictions. A hypothesis is a conjecture based on knowledge obtained while seeking answers to the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments or studies. A scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.The purpose of an experiment is to determine whether observations  agree or disagree with hypothesis.Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always in the same order."
    },
    {
      "id": "29006",
      "title": "Space telescope",
      "url": "https://en.wikipedia.org/wiki/Space_telescope",
      "summary": "A space telescope or space observatory is a telescope in outer space used to observe astronomical objects. Suggested by Lyman Spitzer in 1946, the first operational telescopes were the American Orbiting Astronomical Observatory, OAO-2 launched in 1968, and the Soviet Orion 1 ultraviolet telescope aboard space station Salyut 1 in 1971. Space telescopes avoid the filtering and distortion (scintillation) of electromagnetic radiation which they observe, and avoid light pollution which ground-based observatories encounter. They are divided into two types: Satellites which map the entire sky (astronomical survey), and satellites which focus on selected astronomical objects or parts of the sky and beyond. Space telescopes are distinct from Earth imaging satellites, which point toward Earth for satellite imaging, applied for weather analysis, espionage, and other types of information gathering.\n\n"
    },
    {
      "id": "8151410",
      "title": "Thomas H. Davenport",
      "url": "https://en.wikipedia.org/wiki/Thomas_H._Davenport",
      "summary": "Thomas Hayes \"Tom\" Davenport, Jr. (born October 17, 1954) is an American academic and author specializing in analytics, business process innovation, knowledge management, and artificial intelligence. He is currently the President\u2019s Distinguished Professor in Information Technology and Management at Babson College, a Fellow of the MIT Initiative on the Digital Economy, Co-founder of the International Institute for Analytics, and a Senior Advisor to Deloitte Analytics.\nDavenport has written, coauthored, or edited twenty books, including the first books on analytical competition, business process reengineering and achieving value from enterprise systems, and the best seller, Working Knowledge (with Larry Prusak), on knowledge management. He has written more than one hundred articles for such publications as Harvard Business Review, MIT Sloan Management Review, California Management Review, the Financial Times, and many other publications. Davenport has also been a columnist for The Wall Street Journal, CIO, InformationWeek, and Forbes magazines.\nIn 2003, Davenport was named one of the world\u2019s 'Top 25 Consultants' by Consulting magazine, and in 2005 was named one of the world\u2019s top three analysts of business and technology by readers of Optimize magazine. In 2012 he was named one of the world's \"Top 50 Business School Professors\" by Poets and Quants and Fortune Magazine.\nOne of his most popular books (coauthored with Jeanne Harris), Competing on Analytics: The New Science of Winning, provides guidelines for basing competitive strategies on the analysis of business data, and highlights several firms that do so.\nOne of his sons, Hayes Davenport, is a television comedy writer and podcaster living in Los Angeles.\nHis other son, Chase Davenport, makes surfboards and researches artificial intelligence in San Francisco."
    },
    {
      "id": "35263738",
      "title": "Vasant Dhar",
      "url": "https://en.wikipedia.org/wiki/Vasant_Dhar",
      "summary": "Vasant Dhar is a professor at the Stern School of Business and the Center for Data Science at New York University, former editor-in-chief of the journal Big Data and the founder of SCT Capital, one of the first machine-learning-based hedge funds in New York City in the 1990s. His research focuses on building scalable decision-making systems from large sources of data using techniques and principles from the disciplines of artificial intelligence and machine learning.\n\n"
    },
    {
      "id": "6390647",
      "title": "Wide-field Infrared Survey Explorer",
      "url": "https://en.wikipedia.org/wiki/Wide-field_Infrared_Survey_Explorer",
      "summary": "Wide-field Infrared Survey Explorer (WISE, observatory code C51, Explorer 92 and SMEX-6) is a NASA infrared astronomy space telescope in the Explorers Program launched in December 2009. WISE discovered thousands of minor planets and numerous star clusters. Its observations also supported the discovery of the first Y-type brown dwarf and Earth trojan asteroid.\nWISE performed an all-sky astronomical survey with images in 3.4, 4.6, 12 and 22 \u03bcm wavelength range bands, over ten months using a 40 cm (16 in) diameter infrared telescope in Earth orbit.After its solid hydrogen coolant depleted, it was placed in hibernation mode in February 2011.\nIn 2013, NASA reactivated the WISE telescope to search for near-Earth objects (NEO), such as comets and asteroids, that could collide with Earth. \nThe reactivation mission was called Near-Earth Object Wide-field Infrared Survey Explorer (NEOWISE). \nAs of August 2023, NEOWISE is 40% through the 20th coverage of the full sky.\nScience operations and data processing for WISE and NEOWISE take place at the Infrared Processing and Analysis Center at the California Institute of Technology in Pasadena, California.\nThe WISE All-Sky (WISEA) data, including processed images, source catalogs and raw data, was released to the public on 14 March 2012, and is available at the Infrared Science Archive.The NEOWISE mission is expected to end by early-2025 and the satellite to reentry some time after."
    },
    {
      "id": "46374359",
      "title": "William S. Cleveland",
      "url": "https://en.wikipedia.org/wiki/William_S._Cleveland",
      "summary": "William Swain Cleveland II (born 1943) is an American computer scientist and Professor of Statistics and Professor of Computer Science at Purdue University, known for his work on data visualization, particularly on nonparametric regression and local regression.\n\n"
    },
    {
      "id": "70358320",
      "title": "Women in Data",
      "url": "https://en.wikipedia.org/wiki/Women_in_Data",
      "summary": "Women in Data is an organisation and movement that aims to empower women and support them through the various stages of their careers in data. \nAlthough women comprise about 50% of the United Kingdom (UK) population, only 20% of professionals in artificial intelligence and data in the UK are women.Underrepresentation of women in data science can result in serious issues and in some cases, e.g., automobile crashes, actual harm.\nWith a membership of over 25,000, the organisation seeks to improve the representation of women and girls in data and technology, address some of the key issues, and connect data professionals with partner companies.\n\n"
    },
    {
      "id": "189373",
      "title": "Actuarial science",
      "url": "https://en.wikipedia.org/wiki/Actuarial_science",
      "summary": "Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in insurance, pension, finance, investment and other industries and professions. More generally, actuaries apply rigorous mathematics to model matters of uncertainty and life expectancy.\nActuaries are professionals trained in this discipline. In many countries, actuaries must demonstrate their competence by passing a series of rigorous professional examinations focused in fields such as probability and predictive analysis.\nActuarial science includes a number of interrelated subjects, including mathematics, probability theory, statistics, finance, economics, financial accounting and computer science. Historically, actuarial science used deterministic models in the construction of tables and premiums. The science has gone through revolutionary changes since the 1980s due to the proliferation of high speed computers and the union of stochastic actuarial models with modern financial theory.Many universities have undergraduate and graduate degree programs in actuarial science. In 2010, a study published by job search website CareerCast ranked actuary as the #1 job in the United States. The study used five key criteria to rank jobs: environment, income, employment outlook, physical demands, and stress. In 2023, U.S. News & World Report ranked actuary as the eighth best job in the business sector and the thirteenth best job in STEM."
    },
    {
      "id": "40744441",
      "title": "Herman J. Ad\u00e8r",
      "url": "https://en.wikipedia.org/wiki/Herman_J._Ad%C3%A8r",
      "summary": "Hermanus Johannes \"Herman J.\" Ad\u00e8r (born May 20, 1940) is a Dutch statistician/methodologist and consultant at the Vrije Universiteit, the VU University Medical Center and the University of Stavanger, known for work on Methodological Modelling and Social Research Methodology."
    },
    {
      "id": "11731170",
      "title": "Area chart",
      "url": "https://en.wikipedia.org/wiki/Area_chart",
      "summary": "An area chart or area graph displays graphically quantitative data. It is based on the line chart. The area between axis and line are commonly emphasized with colors, textures and hatchings. Commonly one compares two or more quantities with an area chart."
    },
    {
      "id": "393311",
      "title": "Bar chart",
      "url": "https://en.wikipedia.org/wiki/Bar_chart",
      "summary": "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally.  A vertical bar chart is sometimes called a column chart.\nA bar graph shows comparisons among discrete categories. One axis of the chart shows the specific categories being compared, and the other axis represents a measured value. Some bar graphs present bars clustered in groups of more than one, showing the values of more than one measured variable."
    },
    {
      "id": "1833304",
      "title": "Ben Shneiderman",
      "url": "https://en.wikipedia.org/wiki/Ben_Shneiderman",
      "summary": "Ben Shneiderman (born August 21, 1947) is an American computer scientist, a Distinguished University Professor in the University of Maryland Department of Computer Science, which is part of the University of Maryland College of Computer, Mathematical, and Natural Sciences at the University of Maryland, College Park, and the founding director (1983-2000) of the   University of Maryland Human-Computer Interaction Lab. He conducted fundamental research in the field of human\u2013computer interaction, developing new ideas, methods, and tools such as the direct manipulation interface, and his eight rules of design.\n\n"
    },
    {
      "id": "26161833",
      "title": "Berni Alder",
      "url": "https://en.wikipedia.org/wiki/Berni_Alder",
      "summary": "Berni Julian Alder (September 9, 1925 \u2013 September 7, 2020) was a German-born American physicist specialized in statistical mechanics, and a pioneer of computational modelling of matter."
    },
    {
      "id": "14092434",
      "title": "Bibcode",
      "url": "https://en.wikipedia.org/wiki/Bibcode",
      "summary": "The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.\n\n"
    },
    {
      "id": "2326042",
      "title": "Bifurcation theory",
      "url": "https://en.wikipedia.org/wiki/Bifurcation_theory",
      "summary": "Bifurcation theory is the mathematical study of changes in the qualitative or topological structure of a given family of curves, such as the integral curves of a family of vector fields, and the solutions of a family of differential equations. Most commonly applied to the mathematical study of dynamical systems, a bifurcation occurs when a small smooth change made to the parameter values (the bifurcation parameters) of a system causes a sudden 'qualitative' or topological change in its behavior. Bifurcations occur in both continuous systems (described by ordinary, delay or partial differential equations) and discrete systems (described by maps).\nThe name \"bifurcation\" was first introduced by Henri Poincar\u00e9 in 1885 in the first paper in mathematics showing such a behavior.\n\n"
    },
    {
      "id": "7838811",
      "title": "Bonferroni correction",
      "url": "https://en.wikipedia.org/wiki/Bonferroni_correction",
      "summary": "In statistics, the Bonferroni correction is a method to counteract the multiple comparisons problem.\n\n"
    },
    {
      "id": "6885770",
      "title": "Bootstrapping (statistics)",
      "url": "https://en.wikipedia.org/wiki/Bootstrapping_(statistics)",
      "summary": "Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.Bootstrapping estimates the properties of an estimand (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of  resamples with replacement, of the observed data set (and of equal size to the observed data set).\nIt may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors."
    },
    {
      "id": "2370583",
      "title": "Boris Galerkin",
      "url": "https://en.wikipedia.org/wiki/Boris_Galerkin",
      "summary": "Boris Grigoryevich Galerkin (Russian: \u0411\u043e\u0440\u0438\u0301\u0441 \u0413\u0440\u0438\u0433\u043e\u0301\u0440\u044c\u0435\u0432\u0438\u0447 \u0413\u0430\u043b\u0451\u0440\u043a\u0438\u043d, surname more accurately romanized as Galyorkin; 4 March [O.S. 20 February] 1871\u201312 July 1945) was a Soviet mathematician and an engineer.\n\n"
    },
    {
      "id": "1678795",
      "title": "Boundary element method",
      "url": "https://en.wikipedia.org/wiki/Boundary_element_method",
      "summary": "The boundary element method (BEM) is a numerical computational method of solving linear partial differential equations which have been formulated as integral equations (i.e. in boundary integral form), including fluid mechanics, acoustics, electromagnetics (where the technique is known as method of moments or abbreviated as MoM), fracture mechanics, and contact mechanics."
    },
    {
      "id": "160960",
      "title": "Box plot",
      "url": "https://en.wikipedia.org/wiki/Box_plot",
      "summary": "In descriptive statistics, a box plot or boxplot is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles. In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles, thus, the plot is also called the box-and-whisker plot and the box-and-whisker diagram. Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot.\nBox plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length). The spacings in each subsection of the box-plot indicate the degree of dispersion (spread) and skewness of the data, which are usually described using the five-number summary. In addition, the box-plot allows one to visually estimate various L-estimators, notably the interquartile range, midhinge, range, mid-range, and trimean. Box plots can be drawn either horizontally or vertically.\n\n"
    },
    {
      "id": "21707946",
      "title": "Bubble chart",
      "url": "https://en.wikipedia.org/wiki/Bubble_chart",
      "summary": "A bubble chart is a type of chart that displays three dimensions of data. Each entity with its triplet (v1, v2, v3) of associated data is plotted as a disk that expresses two of the vi values through the disk's xy location and the third through its size. Bubble charts can facilitate the understanding of social, economical, medical, and other scientific relationships.\nBubble charts can be considered a variation of the scatter plot, in which the data points are replaced with bubbles. As the documentation for Microsoft Office explains, \"You can use a bubble chart instead of a scatter chart if your data has three data series that each contain a set of values. The sizes of the bubbles are determined by the values in the third data series.\".\n\n"
    },
    {
      "id": "15393674",
      "title": "Bush tax cuts",
      "url": "https://en.wikipedia.org/wiki/Bush_tax_cuts",
      "summary": "The phrase Bush tax cuts refers to changes to the United States tax code passed originally during the presidency of George W. Bush and extended during the presidency of Barack Obama, through:\n\nEconomic Growth and Tax Relief Reconciliation Act of 2001 (EGTRRA)\nJobs and Growth Tax Relief Reconciliation Act of 2003 (JGTRRA)\nTax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010\nAmerican Taxpayer Relief Act of 2012 (partial extension)While each act has its own legislative history and effect on the tax code, the JGTRRA amplified and accelerated aspects of the EGTRRA.  Since 2003, the two acts have often been spoken of together, especially in terms of analyzing their effect on the U.S. economy and population and in discussing their political ramifications. Both laws were passed using controversial congressional reconciliation procedures.The Bush tax cuts had sunset provisions that made them expire at the end of 2010, since otherwise they would fall under the Byrd Rule. Whether to renew the lowered rates, and how, became the subject of extended political debate, which was resolved during the presidency of Barack Obama by a two-year extension that was part of a larger tax and economic package, the Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010. In 2012, during the fiscal cliff, Obama overcame the sunset provisions and made the tax cuts permanent for single people earning less than $400,000 per year and couples making less than $450,000 per year, but did not stop the sunset provisions from applying to higher incomes, under the American Taxpayer Relief Act of 2012.\nBefore the tax cuts, the highest marginal income tax rate was 39.6 percent.  After the cuts, the highest rate was 35 percent. Once the cuts were allowed to sunset for high income levels (single people making $400,000+ per year and couples making $450,000+ per year), the top income tax rate returned to 39.6 percent.\n\n"
    },
    {
      "id": "49749",
      "title": "Bern",
      "url": "https://en.wikipedia.org/wiki/Bern",
      "summary": "Bern (Swiss Standard German: [b\u025brn] , German: [b\u025b\u0281n]), or Berne (French: [b\u025b\u0281n] ), is the de facto capital of Switzerland, referred to as the \"federal city\". With a population of about 133,000 (as of 2022), Bern is the fifth-most populous city in Switzerland, behind Z\u00fcrich, Geneva, Basel and Lausanne. The Bern agglomeration, which includes 36 municipalities, had a population of 406,900 in 2014. The metropolitan area had a population of 660,000 in 2000.Bern is also the capital of the canton of Bern, the second-most populous of Switzerland's cantons. The official language is German, but the main spoken language is the local variant of the Alemannic Swiss German dialect, Bernese German. In 1983, the historic old town (in German: Altstadt) in the centre of Bern became a UNESCO World Heritage Site. It is notably surrounded by the Aare, a major river of the Swiss Plateau.\nAlthough fortified settlements were established since antiquity, the medieval city proper was founded by the Z\u00e4hringer ruling family, probably in 1191 by Berthold V, Duke of Z\u00e4hringen. Bern was made a free imperial city in 1218 and, in 1353, it joined the Swiss Confederacy, becoming one of its eight early cantons. Since then, Bern became a large city-state and a prominent actor of Swiss history by pursuing a policy of sovereign territorial expansion. Since the 15th century, the city was progressively rebuilt and acquired its current characteristics. Bern was made the Federal City in 1848. From about 5,000 inhabitants in the 15th century, the city passed the 100,000 mark in the 1920s."
    },
    {
      "id": "938069",
      "title": "Cartogram",
      "url": "https://en.wikipedia.org/wiki/Cartogram",
      "summary": "A cartogram (also called a value-area map or an anamorphic map, the latter common among German-speakers) is a thematic map of a set of features (countries, provinces, etc.), in which their geographic size is altered to be  directly proportional to a selected variable, such as travel time, population, or Gross National Product. Geographic space itself is thus warped, sometimes extremely, in order to visualize the distribution of the variable. It is one of the most abstract types of map; in fact, some forms may more properly be called diagrams. They are primarily used to display emphasis and for analysis as nomographs.Cartograms leverage the fact that size is the most intuitive visual variable for representing a total amount. In this, it is a strategy that is similar to proportional symbol maps, which scale point features, and many flow maps, which scale the weight of linear features. However, these two techniques only scale the map symbol, not space itself; a map that stretches the length of linear features is considered a linear cartogram (although additional flow map techniques may be added). Once constructed, cartograms are often used as a base for other thematic mapping techniques to visualize additional variables, such as choropleth mapping.\n\n"
    },
    {
      "id": "37196",
      "title": "Causality",
      "url": "https://en.wikipedia.org/wiki/Causality",
      "summary": "Causality is an influence by which one event, process, state, or object (a cause) contributes to the production of another event, process, state, or object (an effect) where the cause is partly responsible for the effect, and the effect is partly dependent on the cause. In general, a process has many causes, which are also said to be causal factors for it, and all lie in its past. An effect can in turn be a cause of, or causal factor for, many other effects, which all lie in its future. Some writers have held that causality is metaphysically prior to notions of time and space.Causality is an abstraction that indicates how the world progresses. As such a basic concept, it is more apt as an explanation of other concepts of progression than as something to be explained by others more basic. The concept is like those of agency and efficacy. For this reason, a leap of intuition may be needed to grasp it. Accordingly, causality is implicit in the logic and structure of ordinary language, as well as explicit in the language of scientific causal notation.\nIn English studies of Aristotelian philosophy, the word \"cause\" is used as a specialized technical term, the translation of Aristotle's term \u03b1\u1f30\u03c4\u03af\u03b1, by which Aristotle meant \"explanation\" or \"answer to a 'why' question\". Aristotle categorized the four types of answers as material, formal, efficient, and final \"causes\". In this case, the \"cause\" is the explanans for the explanandum, and failure to recognize that different kinds of \"cause\" are being considered can lead to futile debate. Of Aristotle's four explanatory modes, the one nearest to the concerns of the present article is the \"efficient\" one.\nDavid Hume, as part of his opposition to rationalism, argued that pure reason alone cannot prove the reality of efficient causality; instead, he appealed to custom and mental habit, observing that all human knowledge derives solely from experience.\nThe topic of causality remains a staple in contemporary philosophy."
    },
    {
      "id": "11548952",
      "title": "Censoring (statistics)",
      "url": "https://en.wikipedia.org/wiki/Censoring_(statistics)",
      "summary": "In statistics, censoring is a condition in which the value of a measurement or observation is only partially known.\nFor example, suppose a study is conducted to measure the impact of a drug on mortality rate.  In such a study, it may be known that an individual's age at death is at least 75 years (but may be more).  Such a situation could occur if the individual withdrew from the study at age 75, or if the individual is currently alive at the age of 75.\nCensoring also occurs when a value occurs outside the range of a measuring instrument.  For example, a bathroom scale might only measure up to 140 kg.  If a 160-kg individual is weighed using the scale, the observer would only know that the individual's weight is at least 140 kg.\nThe problem of censored data, in which the observed value of some variable is partially known, is related to the problem of missing data, where the observed value of some variable is unknown.\nCensoring should not be confused with the related idea truncation. With censoring, observations result either in knowing the exact value that applies, or in knowing that the value lies within an interval. With truncation, observations never result in values outside a given range: values in the population outside the range are never seen or never recorded if they are seen. Note that in statistics, truncation is not the same as rounding."
    },
    {
      "id": "743794",
      "title": "Chartjunk",
      "url": "https://en.wikipedia.org/wiki/Chartjunk",
      "summary": "Chartjunk consists of all visual elements in charts and graphs that are not necessary to comprehend the information represented on the graph, or that distract the viewer from this information.Markings and visual elements can be called chartjunk if they are not part of the minimum set of visuals necessary to communicate the information understandably.  Examples of unnecessary elements that might be called chartjunk include heavy or dark grid lines, unnecessary text, inappropriately complex or gimmicky font faces, ornamented chart axes, and display frames, pictures, backgrounds or icons within data graphs, ornamental shading and unnecessary dimensions.\nAnother kind of chartjunk skews the depiction and makes it difficult to understand the real data being displayed.  Examples of this type include items depicted out of scale to one another, noisy backgrounds making comparison between elements difficult in a chart or graph, and 3-D simulations in line and bar charts.\n\nThe term chartjunk was coined by Edward Tufte in his 1983 book The Visual Display of Quantitative Information. Tufte wrote:\n\nThe interior decoration of graphics generates a lot of ink that does not tell the viewer anything new. The purpose of decoration varies\u2014to make the graphic appear more scientific and precise, to enliven the display, to give the designer an opportunity to exercise artistic skills. Regardless of its cause, it is all non-data-ink or redundant data-ink, and it is often chartjunk.\nThe term is relatively recent and is often associated with Tufte in other references.The concept is analogous to Adolf Loos's idea that ornament is a crime.\n\n"
    },
    {
      "id": "47278",
      "title": "Cognitive bias",
      "url": "https://en.wikipedia.org/wiki/Cognitive_bias",
      "summary": "A cognitive bias is a systematic pattern of deviation from norm or rationality in judgment. Individuals create their own \"subjective reality\" from their perception of the input. An individual's construction of reality, not the objective input, may dictate their behavior in the world. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, and irrationality.While cognitive biases may initially appear to be negative, some are adaptive. They may lead to more effective actions in a given context. Furthermore, allowing cognitive biases enables faster decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics. Other cognitive biases are a \"by-product\" of human processing limitations, resulting from a lack of appropriate mental mechanisms (bounded rationality), the impact of an individual's constitution and biological state (see embodied cognition), or simply from a limited capacity for information processing.A continually evolving list of cognitive biases has been identified over the last six decades of research on human judgment and decision-making in cognitive science, social psychology, and behavioral economics. The study of cognitive biases has practical implications for areas including clinical judgment, entrepreneurship, finance, and management.\n\n"
    },
    {
      "id": "1519880",
      "title": "Collectively exhaustive events",
      "url": "https://en.wikipedia.org/wiki/Collectively_exhaustive_events",
      "summary": "In probability theory and logic, a set of events is jointly or collectively exhaustive if at least one of the events must occur.  For example, when rolling a six-sided die, the events 1, 2, 3, 4, 5, and 6  balls of a single outcome are collectively exhaustive, because they encompass the entire range of possible outcomes.\nAnother way to describe collectively exhaustive events is that their union must cover all the events within the entire sample space. For example, events A and B are said to be collectively exhaustive if\n\n  \n    \n      \n        A\n        \u222a\n        B\n        =\n        S\n      \n    \n    {\\displaystyle A\\cup B=S}\n  where S is the sample space.\nCompare this to the concept of a set of mutually exclusive events.  In such a set no more than one event can occur at a given time. (In some forms of mutual exclusion only one event can ever occur.) The set of all possible die rolls is both mutually exclusive and collectively exhaustive (i.e., \"MECE\"). The events 1 and 6 are mutually exclusive but not collectively exhaustive.  The events \"even\" (2,4 or 6) and \"not-6\" (1,2,3,4, or 5) are also collectively exhaustive but not mutually exclusive. In some forms of mutual exclusion only one event can ever occur, whether collectively exhaustive or not. For example, tossing a particular biscuit for a group of several dogs cannot be repeated, no matter which dog snaps it up.\nOne example of an event that is both collectively exhaustive and mutually exclusive is tossing a coin. The outcome must be either heads or tails, or p (heads or tails) = 1, so the outcomes are collectively exhaustive. When heads occurs, tails can't occur, or p (heads and tails) = 0, so the outcomes are also mutually exclusive.\nAnother example of events being collectively exhaustive and mutually exclusive at same time are, event \"even\" (2,4 or 6) and event \"odd\" (1,3 or 5) in a random experiment of rolling a six-sided die. These both events are mutually exclusive because even and odd outcome can never occur at same time. The union of both \"even\" and \"odd\" events give sample space of rolling the die, hence are collectively exhaustive.\n\n"
    },
    {
      "id": "34308675",
      "title": "Common-method variance",
      "url": "https://en.wikipedia.org/wiki/Common-method_variance",
      "summary": "In applied statistics, (e.g., applied to the social sciences and psychometrics), common-method variance  (CMV) is the spurious \"variance that is attributable to the measurement method rather than to the constructs the measures are assumed to represent\" or equivalently as \"systematic error variance shared among variables measured with and introduced as a function of the same method and/or source\". For example, an electronic survey method might influence results for those who might be unfamiliar with an electronic survey interface differently than for those who might be familiar. If measures are affected by CMV or common-method bias, the intercorrelations among them can be inflated or deflated depending upon several factors. Although it is sometimes assumed that CMV affects all variables, evidence suggests that whether or not the correlation between two variables is affected by CMV is a function of both the method and the particular constructs being measured."
    },
    {
      "id": "3849994",
      "title": "Computational electromagnetics",
      "url": "https://en.wikipedia.org/wiki/Computational_electromagnetics",
      "summary": "Computational electromagnetics (CEM), computational electrodynamics or electromagnetic modeling is the process of modeling the interaction of electromagnetic fields with physical objects and the environment using computers.\nIt typically involves using computer programs to compute approximate solutions to Maxwell's equations to calculate antenna performance, electromagnetic compatibility, radar cross section and electromagnetic wave propagation when not in free space.  A large subfield is antenna modeling computer programs, which calculate the radiation pattern and electrical properties of radio antennas, and are widely used to design antennas for specific applications.\n\n"
    },
    {
      "id": "305924",
      "title": "Computational fluid dynamics",
      "url": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics",
      "summary": "Computational fluid dynamics (CFD) is a branch of fluid mechanics that uses numerical analysis and data structures to analyze and solve problems that involve fluid flows.  Computers are used to perform the calculations required to simulate the free-stream flow of the fluid, and the interaction of the fluid (liquids and gases) with surfaces defined by boundary conditions. With high-speed supercomputers, better solutions can be achieved, and are often required to solve the largest and most complex problems. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as transonic or turbulent flows. Initial validation of such software is typically performed using experimental apparatus such as wind tunnels.  In addition, previously performed analytical or empirical analysis of a particular problem can be used for comparison.  A final validation is often performed using full-scale testing, such as flight tests.\nCFD is applied to a wide range of research and engineering problems in many fields of study and industries, including aerodynamics and aerospace analysis, hypersonics, weather simulation, natural science and environmental engineering, industrial system design and analysis, biological engineering, fluid flows and heat transfer, engine and combustion analysis, and visual effects for film and games."
    },
    {
      "id": "5205878",
      "title": "Computational mechanics",
      "url": "https://en.wikipedia.org/wiki/Computational_mechanics",
      "summary": "Computational mechanics is the discipline concerned with the use of computational methods to study phenomena governed by the principles of mechanics. Before the emergence of computational science (also called scientific computing) as a \"third way\" besides theoretical and experimental sciences, computational mechanics was widely considered to be a sub-discipline of applied mechanics. It is now considered to be a sub-discipline within computational science.\n\n"
    },
    {
      "id": "17156914",
      "title": "Computational particle physics",
      "url": "https://en.wikipedia.org/wiki/Computational_particle_physics",
      "summary": "Computational particle physics refers to the methods and computing tools developed in and used by  particle physics research. Like computational chemistry or computational biology, it is, for particle physics both a specific branch and an interdisciplinary field relying on computer science, theoretical and experimental particle physics and mathematics.\nThe main fields of computational particle physics are: lattice field theory (numerical computations), automatic calculation of particle interaction or decay (computer algebra) and event generators (stochastic methods).\n\n"
    },
    {
      "id": "106418",
      "title": "Computational physics",
      "url": "https://en.wikipedia.org/wiki/Computational_physics",
      "summary": "Computational physics is the study and implementation of numerical analysis to solve problems in physics. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics \u2014 an area of study which supplements both theory and experiment.\n\n"
    },
    {
      "id": "53713224",
      "title": "Computational thermodynamics",
      "url": "https://en.wikipedia.org/wiki/Computational_thermodynamics",
      "summary": "Computational thermodynamics is the use of computers to simulate thermodynamic problems specific to materials science, particularly used in the construction of phase diagrams.Several open and commercial programs exist to perform these operations. The concept of the technique is minimization of Gibbs free energy of the system; the success of this method is due not only to properly measuring thermodynamic properties, such as those in the list of thermodynamic properties, but also due to the extrapolation of the properties of metastable allotropes of the chemical elements.\n\n"
    },
    {
      "id": "375416",
      "title": "Computer simulation",
      "url": "https://en.wikipedia.org/wiki/Computer_simulation",
      "summary": "Computer simulation is the process of mathematical modelling, performed on a computer, which is designed to predict the behaviour of, or the outcome of, a real-world or physical system. The reliability of some mathematical models can be determined by comparing their results to the real-world outcomes they aim to predict. Computer simulations have become a useful tool for the mathematical modeling of many natural systems in physics (computational physics), astrophysics, climatology, chemistry, biology and manufacturing, as well as human systems in economics, psychology, social science, health care and engineering. Simulation of a system is represented as the running of the system's model. It can be used to explore and gain new insights into new technology and to estimate the performance of systems too complex for analytical solutions.Computer simulations are realized by running computer programs that can be either small, running almost instantly on small devices, or large-scale programs that run for hours or days on network-based groups of computers. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. In 1997, a desert-battle simulation of one force invading another involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around Kuwait, using multiple supercomputers in the DoD High Performance Computer Modernization Program.\nOther examples include a 1-billion-atom model of material deformation; a 2.64-million-atom model of the complex protein-producing organelle of all living organisms, the ribosome, in 2005;\na complete simulation of the life cycle of Mycoplasma genitalium in 2012; and the Blue Brain project at EPFL (Switzerland), begun in May 2005 to create the first computer simulation of the entire human brain, right down to the molecular level.Because of the computational cost of simulation, computer experiments are used to perform inference such as uncertainty quantification.\n\n"
    },
    {
      "id": "59160",
      "title": "Confirmation bias",
      "url": "https://en.wikipedia.org/wiki/Confirmation_bias",
      "summary": "Confirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's prior beliefs or values. People display this bias when they select information that supports their views, ignoring contrary information, or when they interpret ambiguous evidence as supporting their existing attitudes. The effect is strongest for desired outcomes, for emotionally charged issues, and for deeply entrenched beliefs. Confirmation bias is insuperable for most people, but they can manage it, for example, by education and training in critical thinking skills.\nBiased search for information, biased interpretation of this information, and biased memory recall, have been invoked to explain four specific effects:\n\nattitude polarization (when a disagreement becomes more extreme even though the different parties are exposed to the same evidence)\nbelief perseverance (when beliefs persist after the evidence for them is shown to be false)\nthe irrational primacy effect (a greater reliance on information encountered early in a series)\nillusory correlation (when people falsely perceive an association between two events or situations).A series of psychological experiments in the 1960s suggested that people are biased toward confirming their existing beliefs. Later work re-interpreted these results as a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. Explanations for the observed biases include wishful thinking and the limited human capacity to process information. Another proposal is that people show confirmation bias because they are pragmatically assessing the costs of being wrong, rather than investigating in a neutral, scientific way.\nFlawed decisions due to confirmation bias have been found in a wide range of political, organizational, financial and scientific contexts. These biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. For example, confirmation bias produces systematic errors in scientific research based on inductive reasoning (the gradual accumulation of supportive evidence). Similarly, a police detective may identify a suspect early in an investigation, but then may only seek confirming rather than disconfirming evidence. A medical practitioner may prematurely focus on a particular disorder early in a diagnostic session, and then seek only confirming evidence. In social media, confirmation bias is amplified by the use of filter bubbles, or \"algorithmic editing\", which display to individuals only information they are likely to agree with, while excluding opposing views.\n\n"
    },
    {
      "id": "1053718",
      "title": "Congressional Budget Office",
      "url": "https://en.wikipedia.org/wiki/Congressional_Budget_Office",
      "summary": "The Congressional Budget Office (CBO) is a federal agency within the legislative branch of the United States government that provides budget and economic information to Congress.\nInspired by California's Legislative Analyst's Office that manages the state budget in a strictly nonpartisan fashion, the CBO was created as a nonpartisan agency by the Congressional Budget and Impoundment Control Act of 1974.Whereas politicians on both sides of the aisle have criticized the CBO when its estimates have been politically inconvenient, economists and other academics overwhelmingly reject that the CBO is partisan or that it fails to produce credible forecasts. There is a consensus among economists that \"adjusting for legal restrictions on what the CBO can assume about future legislation and events, the CBO has historically issued credible forecasts of the effects of both Democratic and Republican legislative proposals.\""
    },
    {
      "id": "36108052",
      "title": "Contextualization (computer science)",
      "url": "https://en.wikipedia.org/wiki/Contextualization_(computer_science)",
      "summary": "In computer science, contextualization is the process of identifying the data relevant to an entity (e.g., a person or a city) based on the entity's contextual information."
    },
    {
      "id": "435754",
      "title": "Control chart",
      "url": "https://en.wikipedia.org/wiki/Control_chart",
      "summary": "Control charts are graphical plots used in production control to determine whether quality and manufacturing processes are being controlled under stable conditions. (ISO 7870-1) \nThe hourly status is arranged on the graph, and the occurrence of abnormalities is judged based on the presence of data that differs from the conventional trend or deviates from the control limit line.\nControl charts are classified into Shewhart individuals control chart (ISO 7870-2) and CUSUM(CUsUM)(or cumulative sum control chart)(ISO 7870-4).Control charts, also known as Shewhart charts (after Walter A. Shewhart) or process-behavior charts, are a statistical process control tool used to determine if a manufacturing or business process is in a state of control.  It is more appropriate to say that the control charts are the graphical device for Statistical Process Monitoring (SPM). Traditional control charts are mostly designed to monitor process parameters when the underlying form of the process distributions are known. However, more advanced techniques are available in the 21st century where incoming data streaming can-be monitored even without any knowledge of the underlying process distributions.  Distribution-free control charts are becoming increasingly popular."
    },
    {
      "id": "157057",
      "title": "Correlation",
      "url": "https://en.wikipedia.org/wiki/Correlation",
      "summary": "In statistics, correlation  or dependence  is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it usually refers to the degree to which a pair of variables are linearly related.  \nFamiliar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the so-called demand curve.\nCorrelations are useful because they can indicate a predictive relationship that can be exploited in practice.  For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation).\nFormally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence.  In informal parlance, correlation is synonymous with dependence. However, when used in a technical sense, correlation refers to any of several specific types of mathematical operations between the tested variables and their respective expected values. Essentially, correlation is the measure of how two or more variables are related to one another.  There are several correlation coefficients, often denoted \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   or \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n  , measuring the degree of correlation.  The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other).  Other correlation coefficients \u2013 such as Spearman's rank correlation \u2013 have been developed to be more robust than Pearson's, that is, more sensitive to nonlinear relationships. Mutual information can also be applied to measure dependence between two variables."
    },
    {
      "id": "379269",
      "title": "Cronbach's alpha",
      "url": "https://en.wikipedia.org/wiki/Cronbach%27s_alpha",
      "summary": "Cronbach's alpha (Cronbach's \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  ), also known as tau-equivalent reliability (\n  \n    \n      \n        \n          \u03c1\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\rho _{T}}\n  ) or coefficient alpha (coefficient \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  ), is a reliability coefficient and a measure of the internal consistency of tests and measures.Numerous studies warn against using Cronbach's alpha unconditionally. Statisticians regard reliability coefficients based on structural equation modeling (SEM) or generalizability theory as superior alternatives in many situations.\n\n"
    },
    {
      "id": "3144369",
      "title": "Cross-industry standard process for data mining",
      "url": "https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining",
      "summary": "The Cross-industry standard process for data mining, known as CRISP-DM, is an open standard process model that describes common approaches used by data mining experts. It is the most widely-used analytics model.In 2015, IBM released a new methodology called Analytics Solutions Unified Method for Data Mining/Predictive Analytics (also known as ASUM-DM), which refines and extends CRISP-DM."
    },
    {
      "id": "416612",
      "title": "Cross-validation (statistics)",
      "url": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)",
      "summary": "Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\nCross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. It is often used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. It can also be used to assess the quality of a fitted model and the stability of its parameters.\nIn a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\nOne round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\nIn summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance."
    },
    {
      "id": "201756",
      "title": "Daniel Patrick Moynihan",
      "url": "https://en.wikipedia.org/wiki/Daniel_Patrick_Moynihan",
      "summary": "Daniel Patrick Moynihan (March 16, 1927 \u2013 March 26, 2003) was an American politician and diplomat. A member of the Democratic Party, he represented New York in the United States Senate from 1977 until 2001 after serving as an adviser to President Richard Nixon, and as the United States' ambassador to India and to the United Nations. \nBorn in Tulsa, Oklahoma, Moynihan moved at a young age to New York City. Following a stint in the navy, he earned a Ph.D. in history from Tufts University. He worked on the staff of New York Governor W. Averell Harriman before joining President John F. Kennedy's administration in 1961. He served as an Assistant Secretary of Labor under Presidents Kennedy and President Lyndon B. Johnson, devoting much of his time to the War on Poverty. In 1965, he published the controversial Moynihan Report on black poverty. Moynihan left the Johnson administration in 1965 and became a professor at Harvard University.\nIn 1969, he accepted Nixon's offer to serve as an Assistant to the President for Domestic Policy, and he was elevated to the position of Counselor to the President later that year. He left the administration at the end of 1970, and accepted appointment as United States Ambassador to India in 1973. He accepted President Gerald Ford's appointment to the position of United States Ambassador to the United Nations in 1975, holding that position until early 1976; later that year he won election to the Senate.\nMoynihan served as Chairman of the Senate Environment Committee from 1992 to 1993 and as Chairman of the Senate Finance Committee from 1993 to 1995. He also led the Moynihan Secrecy Commission, which studied the regulation of classified information. He emerged as a strong critic of President Ronald Reagan's foreign policy and opposed President Bill Clinton's health care plan. He frequently broke with liberal positions, but opposed welfare reform in the 1990s. He also voted against the Defense of Marriage Act, the North American Free Trade Agreement, and the Congressional authorization for the Gulf War. He was tied with Jacob K. Javits as the longest-serving Senator from the state of New York until they were both surpassed by Chuck Schumer in 2023."
    },
    {
      "id": "155319",
      "title": "Data acquisition",
      "url": "https://en.wikipedia.org/wiki/Data_acquisition",
      "summary": "Data acquisition is the process of sampling signals that measure real-world physical conditions and converting the resulting samples into digital numeric values that can be manipulated by a computer. Data acquisition systems, abbreviated by the acronyms DAS, DAQ, or DAU, typically convert analog waveforms into digital values for processing. The components of data acquisition systems include:\n\nSensors, to convert physical parameters to electrical signals.\nSignal conditioning circuitry, to convert sensor signals into a form that can be converted to digital values.\nAnalog-to-digital converters, to convert conditioned sensor signals to digital values.Data acquisition applications are usually controlled by software programs developed using various general purpose programming languages such as Assembly, BASIC, C, C++, C#, Fortran, Java, LabVIEW, Lisp, Pascal, etc. Stand-alone data acquisition systems are often called data loggers.\nThere are also open-source software packages providing all the necessary tools to acquire data from different, typically specific, hardware equipment. These tools come from the scientific community where complex experiment requires fast, flexible, and adaptable software. Those packages are usually custom-fit but more general DAQ packages like the Maximum Integrated Data Acquisition System can be easily tailored and are used in several physics experiments.\n\n"
    },
    {
      "id": "54332547",
      "title": "Data blending",
      "url": "https://en.wikipedia.org/wiki/Data_blending",
      "summary": "Data blending is a process whereby big data from multiple sources are merged into a single data warehouse or data set.Data blending allows business analysts to cope with the expansion of data that they need to make critical business decisions based on good quality business intelligence. Data blending has been described as different from data integration due to the requirements of data analysts to merge sources very quickly, too quickly for any practical intervention by data scientists."
    },
    {
      "id": "28192799",
      "title": "Data custodian",
      "url": "https://en.wikipedia.org/wiki/Data_custodian",
      "summary": "In data governance groups, responsibilities for data management are increasingly divided between the business process owners and information technology (IT) departments.  Two functional titles commonly used for these roles are data steward and data custodian. \nData Stewards are commonly responsible for data content, context, and associated business rules. Data custodians are responsible for the safe custody, transport, storage of the data and implementation of business rules. Simply put, Data Stewards are responsible for what is stored in a data field, while data custodians are responsible for the technical environment and database structure. Common job titles for data custodians are database administrator (DBA), data modeler and ETL developer.\n\n"
    },
    {
      "id": "6222875",
      "title": "Data governance",
      "url": "https://en.wikipedia.org/wiki/Data_governance",
      "summary": "Data governance is a term used on both a macro and a micro level. The former is a political concept and forms part of international relations and Internet governance; the latter is a data management concept and forms part of corporate data governance.\n\n"
    },
    {
      "id": "24770596",
      "title": "Data system",
      "url": "https://en.wikipedia.org/wiki/Data_system",
      "summary": "Data system is a term used to refer to an organized collection of symbols and processes that may be used to operate on such symbols. Any organised collection of symbols and symbol-manipulating operations can be considered a data system. Hence, human-speech analysed at the level of phonemes can be considered a data system as can the Incan artefact of the khipu and an image stored as pixels. A data system is defined in terms of some data model and bears a resemblance to the idea of a physical symbol system.\nSymbols within some data systems may be persistent or not. Hence, the sounds of human speech are non-persistent symbols because they decay rapidly in air. In contrast, pixels stored on some peripheral storage device are persistent symbols.\n\n"
    },
    {
      "id": "10056274",
      "title": "Data transformation (statistics)",
      "url": "https://en.wikipedia.org/wiki/Data_transformation_(statistics)",
      "summary": "In statistics, data transformation is the application of a deterministic mathematical function to each point in a data set\u2014that is, each data point zi is replaced with the transformed value yi = f(zi), where f is a function. Transforms are usually applied so that the data appear to more closely meet the assumptions of a statistical inference procedure that is to be applied, or to improve the interpretability or appearance of graphs.\nNearly always, the function that is used to transform the data is invertible, and generally is continuous. The transformation is usually applied to a collection of comparable measurements. For example, if we are working with data on peoples' incomes in some currency unit, it would be common to transform each person's income value by the logarithm function."
    },
    {
      "id": "30666895",
      "title": "David Hand (statistician)",
      "url": "https://en.wikipedia.org/wiki/David_Hand_(statistician)",
      "summary": "David John Hand  (born 30 June 1950 in Peterborough) is a British statistician. His research interests include multivariate statistics, classification methods, pattern recognition, computational statistics and the foundations of statistics. He has written technical books on statistics, data mining, finance, classification methods, and measuring wellbeing, as well as science popularisation books including The Improbability Principle: Why Coincidences, Miracles, and Rare Events Happen Every Day; Dark Data: Why What You Don't Know Matters; and Statistics: A Very Short Introduction. In 1991 he launched the journal Statistics and Computing."
    },
    {
      "id": "35276459",
      "title": "David McCandless",
      "url": "https://en.wikipedia.org/wiki/David_McCandless",
      "summary": "David McCandless (born 1971) is a British data-journalist, writer and information designer."
    },
    {
      "id": "8187",
      "title": "Descriptive statistics",
      "url": "https://en.wikipedia.org/wiki/Descriptive_statistics",
      "summary": "A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features from a collection of information, while descriptive statistics (in the mass noun sense) is the process of using and analysing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics) by its aim to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, is not developed on the basis of probability theory, and are frequently nonparametric statistics. Even when a data analysis draws its main conclusions using inferential statistics, descriptive statistics are generally also presented. For example, in papers reporting on human subjects, typically a table is included giving the overall sample size, sample sizes in important subgroups (e.g., for each treatment or exposure group), and demographic or clinical characteristics such as the average age, the proportion of subjects of each sex, the proportion of subjects with related co-morbidities, etc.\nSome measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness."
    },
    {
      "id": "23321884",
      "title": "DevInfo",
      "url": "https://en.wikipedia.org/wiki/DevInfo",
      "summary": "DevInfo was a database system developed under the auspices of the United Nations and endorsed by the United Nations Development Group for monitoring human development with the specific purpose of monitoring the Millennium Development Goals (MDGs), which is a set of Human Development Indicators. DevInfo was a tool for organizing, storing and presenting data in a uniform way to facilitate data sharing at the country level across government departments, UN agencies and development partners. It was distributed royalty-free to all UN member states. It was a further development of the earlier UNICEF database system ChildInfo.\nThe Global DevInfo Initiative, led by UNICEF on behalf of the UN system, is dedicated to furthering human development by offering information technology-based solutions aimed at addressing development-related challenges. This is achieved by integrating management information systems, geographic information systems, software training, technical support services, data dissemination solutions and technical publications. The DevInfo Initiative takes a strategic approach towards strengthening the monitoring and evaluation capacity of governments and agencies by developing innovative technological solutions to better track human development progress."
    },
    {
      "id": "416589",
      "title": "Exploratory data analysis",
      "url": "https://en.wikipedia.org/wiki/Exploratory_data_analysis",
      "summary": "In statistics, exploratory data analysis (EDA) is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.\n\n"
    },
    {
      "id": "8525",
      "title": "Digital signal processing",
      "url": "https://en.wikipedia.org/wiki/Digital_signal_processing",
      "summary": "Digital signal processing (DSP) is the use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations.  The digital signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency. In digital electronics, a digital signal is represented as a pulse train, which is typically generated by the switching of a transistor.Digital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech processing, sonar, radar and other sensor array processing, spectral density estimation, statistical signal processing, digital image processing, data compression, video coding, audio coding, image compression, signal processing for telecommunications, control systems, biomedical engineering, and seismology, among others.\nDSP can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains.\nThe application of digital computation to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression. Digital signal processing is also fundamental to digital technology, such as digital telecommunication and wireless communications. DSP is applicable to both streaming data and static (stored) data.\n\n"
    },
    {
      "id": "579867",
      "title": "Dimensionality reduction",
      "url": "https://en.wikipedia.org/wiki/Dimensionality_reduction",
      "summary": "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.\n\n"
    },
    {
      "id": "12578506",
      "title": "Dissipative particle dynamics",
      "url": "https://en.wikipedia.org/wiki/Dissipative_particle_dynamics",
      "summary": "Dissipative particle dynamics (DPD) is an off-lattice mesoscopic simulation technique which involves a set of particles moving in continuous space and discrete time. Particles represent whole molecules or fluid regions, rather than single atoms, and atomistic details are not considered relevant to the processes addressed. The particles' internal degrees of freedom are integrated out and replaced by simplified pairwise dissipative and random forces, so as to conserve momentum locally and ensure correct hydrodynamic behaviour. The main advantage of this method is that it gives access to longer time and length scales than are possible using conventional MD simulations. Simulations of polymeric fluids in volumes up to 100 nm in linear dimension for tens of microseconds are now common.\nDPD was initially devised by Hoogerbrugge and Koelman to avoid the lattice artifacts of the so-called lattice gas automata and to tackle hydrodynamic time and space scales beyond those available with molecular dynamics (MD). It was subsequently reformulated and slightly modified by P. Espa\u00f1ol to ensure the proper thermal equilibrium state. A series of new DPD algorithms with reduced computational complexity and better control of transport properties are presented. The algorithms presented in this article choose randomly a pair particle for applying DPD thermostating thus reducing the computational complexity.\n\n"
    },
    {
      "id": "10672320",
      "title": "Dropout (communications)",
      "url": "https://en.wikipedia.org/wiki/Dropout_(communications)",
      "summary": "A dropout is a momentary loss of signal in a communications system, usually caused by noise, propagation anomalies, or system malfunctions.  For analog signals, a dropout is frequently gradual and partial, depending on the cause.  For digital signals, dropouts are more pronounced, usually being sudden and complete, due to the cliff effect.  In mobile telephony, a dropout of more than a few seconds will result in a dropped call."
    },
    {
      "id": "1324565",
      "title": "DuPont analysis",
      "url": "https://en.wikipedia.org/wiki/DuPont_analysis",
      "summary": "DuPont analysis (also known as the DuPont identity, DuPont equation, DuPont framework, DuPont model, DuPont method or DuPont system) is a tool used in financial analysis, where return on equity (ROE) is separated into its component parts.\nUseful in several contexts, this \"decomposition\" of ROE allows financial managers to focus on the key metrics of financial performance individually, and thereby to identify strengths and weaknesses within the company that should be addressed. Similarly, it allows investors to compare the operational efficiency of two comparable firms.The name derives from the DuPont company, which began using this formula in the 1920s. A DuPont explosives salesman, Donaldson Brown, submitted an internal efficiency report to his superiors in 1912 that contained the formula.\n\n"
    },
    {
      "id": "20789898",
      "title": "Early case assessment",
      "url": "https://en.wikipedia.org/wiki/Early_case_assessment",
      "summary": "Early case assessment refers to estimating risk (cost of time and money) to prosecute or defend a legal case.  Global organizations deal with legal discovery and disclosure requests for electronically stored information \"ESI\" and paper documents on a regular basis.\nOver 90% of all cases settle prior to trial.  Often an organization will spend significant time and money on a case only to find they want to settle for whatever reason.  Legal discovery costs are usually the most burdensome financially to both plaintiff and defendant.  Often, and during cases in the United States, an opposing party will strategize on how to make it as difficult as possible for you to comply with the discovery process, including time and cost to respond to discovery requests.  Because of this, organizations have a continued need to conduct early case assessment to determine their risks and  benefits of taking a case to trial without painful settlement discussions.\nMany service organizations, law firms, and corporations refer to early case assessment differently.  Consultants hired by the corporation or law firm on a case manage cases on a risk basis. There also exist a number of software tools that assist in and help facilitate the process of early case assessment. Effective early case assessment might require the combination of professional expertise and software. This pairing, depending on the professional and tools used can provide various degrees of early case assessment review.  Early case assessment, as a managed process, often requires customization to each case and the client involved.\nThe early case assessment lifecycle will typically include all of the following:\n\nPerform a risk-benefit analysis.\nPlace and manage a legal hold on potentially responsive documents (paper and ESI) in appropriate countries.\nPreserve information abroad.\nGather relevant information for attorney and expert document review.\nProcess potentially relevant information for purposes of filtering, search term, or data analytics.\nInformation hosting for attorney and expert document review, commenting, redaction.\nProduce documents to parties in the case.\nReuse information in future cases.Early case assessment software is typically used by attorneys, corporate legal departments, risk managers, forensics teams, IT professionals and independent consultants to help them analyze unstructured electronically stored information.\nThe software approach to early case assessment typically includes the following:\n\nDetermine the source files to analyze.\nPoint the analysis tool to the files to be analyzed.\nSet parameters for the assessment.\nAllow the program to automatically scan and assess the data, which may be located on local hard drives, removable media, file servers, whole networks, etc.)\nReview reports generated by the software.\n\n"
    },
    {
      "id": "61612443",
      "title": "Ed Hawkins (climatologist)",
      "url": "https://en.wikipedia.org/wiki/Ed_Hawkins_(climatologist)",
      "summary": "Edward Hawkins  (born 1977) is a British climate scientist who is Professor of climate science at the University of Reading, principal research scientist at the National Centre for Atmospheric Science (NCAS), editor of Climate Lab Book blog and lead scientist for the Weather Rescue citizen science project. He is known for his data visualizations of climate change for the general public such as warming stripes and climate spirals."
    },
    {
      "id": "9252",
      "title": "Education",
      "url": "https://en.wikipedia.org/wiki/Education",
      "summary": "Education is the transmission of knowledge, skills, and character traits and comes in many forms. Formal education happens in a complex institutional framework, like public schools. Non-formal education is also structured but takes place outside the formal schooling system, while informal education is unstructured learning through daily experiences. Formal and non-formal education are divided into levels that include early childhood education, primary education, secondary education, and tertiary education. Other classifications focus on the teaching method, like teacher-centered and student-centered education, and on the subject, like science education, language education, and physical education. The term \"education\" can also refer to the mental states and qualities of educated people and the academic field studying educational phenomena.\nThe precise definition of education is disputed, and there are disagreements about what the aims of education are and to what extent education is different from indoctrination by fostering critical thinking. These disagreements affect how to identify, measure, and improve forms of education. Fundamentally, education socializes children into society by teaching cultural values and norms. It equips them with the skills needed to become productive members of society. This way, it stimulates economic growth and raises awareness of local and global problems. Organized institutions affect many aspects of education. For example, governments set education policies to determine when school classes happen, what is taught, and who can or must attend. International organizations, like UNESCO, have been influential in promoting primary education for all children.\nMany factors influence whether education is successful. Psychological factors include  motivation, intelligence, and personality. Social factors, like socioeconomic status, ethnicity, and gender, are often linked to discrimination. Further factors include access to educational technology, teacher quality, and parent involvement.\nThe main academic field investigating education is called education studies. It examines what education is, what aims and effects it has, and how to improve it. Education studies has many subfields, like philosophy, psychology, sociology, and economics of education. It also discusses comparative education, pedagogy, and the history of education.\nIn prehistory, education happened informally through oral communication and imitation. With the rise of ancient civilizations, writing was invented, and the amount of knowledge grew. This caused a shift from informal to formal education. Initially, formal education was mainly available to elites and religious groups. The invention of the printing press in the 15th century made books more widely available. This increased general literacy. Beginning in the 18th and 19th centuries, public education became more important. This development led to the worldwide process of making primary education available to all, free of charge, and compulsory up to a certain age. Today, over 90% of all primary-school-age children worldwide attend primary school."
    },
    {
      "id": "1706783",
      "title": "Edward Norton Lorenz",
      "url": "https://en.wikipedia.org/wiki/Edward_Norton_Lorenz",
      "summary": "Edward Norton Lorenz (May 23, 1917 \u2013 April 16, 2008) was an American mathematician and meteorologist who established the theoretical basis of weather and climate predictability, as well as the basis for computer-aided atmospheric physics and meteorology. He is best known as the founder of modern chaos theory, a branch of mathematics focusing on the behavior of dynamical systems that are highly sensitive to initial conditions.His discovery of deterministic chaos \"profoundly influenced a wide range of basic sciences and brought about one of the most dramatic changes in mankind's view of nature since Sir Isaac Newton,\" according to the committee that awarded him the 1991 Kyoto Prize for basic sciences in the field of earth and planetary sciences."
    },
    {
      "id": "263977",
      "title": "Edward Tufte",
      "url": "https://en.wikipedia.org/wiki/Edward_Tufte",
      "summary": "Edward Rolf Tufte (; born March 14, 1942), sometimes known as \"ET\", is an American statistician and professor emeritus of political science, statistics, and computer science at Yale University. He is noted for his writings on information design and as a pioneer in the field of data visualization."
    },
    {
      "id": "1339400",
      "title": "Federal Highway Administration",
      "url": "https://en.wikipedia.org/wiki/Federal_Highway_Administration",
      "summary": "The Federal Highway Administration (FHWA) is a division of the United States Department of Transportation that specializes in highway transportation.  The agency's major activities are grouped into two programs, the Federal-aid Highway Program and the Federal Lands Highway Program. Its role had previously been performed by the Office of Road Inquiry, Office of Public Roads and the Bureau of Public Roads.\n\n"
    },
    {
      "id": "758833",
      "title": "Past",
      "url": "https://en.wikipedia.org/wiki/Past",
      "summary": "The past is the set of all events that occurred before a given point in time. The past is contrasted with and defined by the present and the future. The concept of the past is derived from the linear fashion in which human observers experience time, and is accessed through memory and recollection. In addition, human beings have recorded the past since the advent of written language. The first known use of the word \"past\" was in the fourteenth century; it developed as the past participle of the Middle English verb passen meaning \"to pass.\""
    },
    {
      "id": "6184541",
      "title": "Financial statement analysis",
      "url": "https://en.wikipedia.org/wiki/Financial_statement_analysis",
      "summary": "Financial statement analysis (or just financial analysis) is the process of reviewing and analyzing a company's financial statements to make better economic decisions to earn income in future. These statements include the income statement, balance sheet, statement of cash flows, notes to accounts and a statement of changes in equity (if applicable). Financial statement analysis is a method or process involving specific techniques for evaluating risks, performance, valuation, financial health, and future prospects of an organization.It is used by a variety of stakeholders, such as credit and equity investors, the government, the public, and decision-makers within the organization. These stakeholders have different interests and apply a variety of different techniques to meet their needs. For example, equity investors are interested in the long-term earnings power of the organization and perhaps the sustainability and growth of dividend payments. Creditors want to ensure the interest and principal is paid on the organizations debt securities (e.g., bonds) when due.\nCommon methods of financial statement analysis include horizontal and vertical analysis and the use of financial ratios.  Historical information combined with a series of assumptions and adjustments to the financial information may be used to project future performance. The Chartered Financial Analyst designation is available for professional financial analysts.\n\n"
    },
    {
      "id": "6054681",
      "title": "Finite difference method",
      "url": "https://en.wikipedia.org/wiki/Finite_difference_method",
      "summary": "In numerical analysis, finite-difference methods (FDM) are a class of numerical techniques for solving differential equations by approximating derivatives with finite differences. Both the spatial domain and time domain (if applicable) are discretized, or broken into a finite number of intervals, and the values of the solution at the end points of the intervals are approximated by solving algebraic equations containing finite differences and values from nearby points.\nFinite difference methods convert ordinary differential equations (ODE) or partial differential equations (PDE), which may be nonlinear, into a system of linear equations that can be solved by matrix algebra techniques. Modern computers can perform these linear algebra computations efficiently which, along with their relative ease of implementation, has led to the widespread use of FDM in modern numerical analysis.\nToday, FDM are one of the most common approaches to the numerical solution of PDE, along with finite element methods.\n\n"
    },
    {
      "id": "18233581",
      "title": "Finite element method",
      "url": "https://en.wikipedia.org/wiki/Finite_element_method",
      "summary": "The finite element method (FEM) is an extremely popular method for numerically solving differential equations arising in engineering and mathematical modeling. Typical problem areas of interest include the traditional fields of structural analysis, heat transfer, fluid flow, mass transport, and electromagnetic potential.\nThe FEM is a general numerical method for solving partial differential equations in two or three space variables (i.e., some boundary value problems). To solve a problem, the FEM subdivides a large system into smaller, simpler parts called finite elements.  This is achieved by a particular space discretization in the space dimensions, which is implemented by the construction of a mesh of the object: the numerical domain for the solution, which has a finite number of points. \nThe finite element method formulation of a boundary value problem finally results in a system of algebraic equations. The method approximates the unknown function over the domain.\nThe simple equations that model these finite elements are then assembled into a larger system of equations that models the entire problem. The FEM then approximates a solution by minimizing an associated error function via the calculus of variations.\nStudying or analyzing a phenomenon with FEM is often referred to as finite element analysis (FEA)."
    },
    {
      "id": "345017",
      "title": "Finite volume method",
      "url": "https://en.wikipedia.org/wiki/Finite_volume_method",
      "summary": "The finite volume method (FVM) is a method for representing and evaluating partial differential equations in the form of algebraic equations.\nIn the finite volume method, volume integrals in a partial differential equation that contain a divergence term are converted to surface integrals, using the divergence theorem. \nThese terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods are conservative. Another advantage of the finite volume method is that it is easily formulated to allow for unstructured meshes. The method is used in many computational fluid dynamics packages.\n\"Finite volume\" refers to the small volume surrounding each node point on a mesh.Finite volume methods can be compared and contrasted with the finite difference methods, which approximate derivatives using nodal values, or finite element methods, which create local approximations of a solution using local data, and construct a global approximation by stitching them together. In contrast a finite volume method evaluates exact expressions for the average value of the solution over some volume, and uses this data to construct approximations of the solution within cells.\n\n"
    },
    {
      "id": "509709",
      "title": "Gibbs sampling",
      "url": "https://en.wikipedia.org/wiki/Gibbs_sampling",
      "summary": "In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for sampling from a specified multivariate probability distribution when direct sampling from the joint distribution is difficult, but sampling from the conditional distribution is more practical.  This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables).  Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.\nGibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference.  It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM).\nAs with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired. Generally, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded."
    },
    {
      "id": "39414862",
      "title": "Gideon J. Mellenbergh",
      "url": "https://en.wikipedia.org/wiki/Gideon_J._Mellenbergh",
      "summary": "Gideon Jan (Don) Mellenbergh (9 August 1938 \u2013 27 March 2021) was a Dutch psychologist, who was Professor of Psychological methods at the University of Amsterdam, known for his contribution in the field of psychometrics, and Social Research Methodology."
    },
    {
      "id": "41916270",
      "title": "Hadley Wickham",
      "url": "https://en.wikipedia.org/wiki/Hadley_Wickham",
      "summary": "Hadley Alexander Wickham (born 14 October 1979) is a New Zealand statistician known for his work on open-source software for the R statistical programming environment. He is the chief scientist at Posit, PBC and an adjunct professor of statistics at the University of Auckland, Stanford University, and Rice University. His work includes the data visualisation system ggplot2 and the tidyverse, a collection of R packages for data science based on the concept of tidy data."
    },
    {
      "id": "6116152",
      "title": "Hans Rosling",
      "url": "https://en.wikipedia.org/wiki/Hans_Rosling",
      "summary": "Hans Rosling (Swedish pronunciation: [\u02c8h\u0251\u02d0ns \u02c8r\u00fb\u02d0sl\u026a\u014b]; 27 July 1948 \u2013 7 February 2017) was a Swedish physician, academic and public speaker. He was a professor of international health at Karolinska Institute and was the co-founder and chairman of the Gapminder Foundation, which developed the Trendalyzer software system. He held presentations around the world, including several TED Talks in which he promoted the use of data (and data visualization) to explore development issues. His posthumously published book Factfulness, coauthored with his daughter-in-law Anna Rosling R\u00f6nnlund and son Ola Rosling, became an international bestseller."
    },
    {
      "id": "14349",
      "title": "Harmonica",
      "url": "https://en.wikipedia.org/wiki/Harmonica",
      "summary": "The harmonica, also known as a French harp, mouth harp or mouth organ, is a free reed wind instrument used worldwide in many musical genres, notably in blues, American folk music, classical music, jazz, country, and rock. The many types of harmonica include diatonic, chromatic, tremolo, octave, orchestral, and bass versions. A harmonica is played by using the mouth (lips and tongue) to direct air into or out of one (or more) holes along a mouthpiece. Behind each hole is a chamber containing at least one reed. The most common is the \ndiatonic Richter-tuned with ten air passages and twenty reeds, often called the blues harp. A harmonica reed is a flat, elongated spring typically made of brass, stainless steel, or bronze, which is secured at one end over a slot that serves as an airway. When the free end is made to vibrate by the player's air, it alternately blocks and unblocks the airway to produce sound.\nReeds are tuned to individual pitches. Tuning may involve changing a reed's length, the weight near its free end, or the stiffness near its fixed end. Longer, heavier, and springier reeds produce deeper, lower sounds; shorter, lighter, and stiffer reeds make higher-pitched sounds. If, as on most modern harmonicas, a reed is affixed above or below its slot rather than in the plane of the slot, it responds more easily to air flowing in the direction that initially would push it into the slot, i.e., as a closing reed. This difference in response to air direction makes it possible to include both a blow reed and a draw reed in the same air chamber and to play them separately without relying on flaps of plastic or leather (valves, wind-savers) to block the nonplaying reed.\nAn important technique in performance is bending, causing a drop in pitch by making embouchure adjustments. Bending isolated reeds is possible, as on chromatic and other harmonica models with wind-savers, but also to both lower, and raise (overbend, overblow, overdraw) the pitch produced by pairs of reeds in the same chamber, as on a diatonic or other unvalved harmonica. Such two-reed pitch changes actually involve sound production by the normally silent reed, the opening reed (for instance, the blow reed while the player is drawing)."
    },
    {
      "id": "22048289",
      "title": "Handle System",
      "url": "https://en.wikipedia.org/wiki/Handle_System",
      "summary": "The Handle System is the Corporation for National Research Initiatives's proprietary registry assigning persistent identifiers, or handles, to information resources, and for resolving \"those handles into the information necessary to locate, access, and otherwise make use of the resources\".As with handles used elsewhere in computing, Handle System handles are opaque, and encode no information about the underlying resource, being bound only to metadata regarding the resource. Consequently, the handles are not rendered invalid by changes to the metadata.\nThe system was developed by Bob Kahn at the Corporation for National Research Initiatives (CNRI). The original work was funded by the Defense Advanced Research Projects Agency (DARPA) between 1992 and 1996, as part of a wider framework for distributed digital object services, and was thus contemporaneous with the early deployment of the World Wide Web, with similar goals.\nThe Handle System was first implemented in autumn 1994, and was administered and operated by CNRI until December 2015, when a new \"multi-primary administrator\" (MPA) mode of operation was introduced. The DONA Foundation now administers the system's Global Handle Registry and accredits MPAs, including CNRI and the International DOI Foundation.\nThe system currently provides the underlying infrastructure for such handle-based systems as Digital Object Identifiers and DSpace, which are mainly used to provide access to scholarly, professional and government documents and other information resources.\nCNRI provides specifications and the source code for reference implementations for the servers and protocols used in the system under a royalty-free \"Public License\", similar to an open source license.Thousands of handle services are currently running. Over 1000 of these are at universities and libraries, but they are also in operation at national laboratories, research groups, government agencies, and commercial enterprises, receiving over 200 million resolution requests per month.\n\n"
    },
    {
      "id": "13266",
      "title": "Histogram",
      "url": "https://en.wikipedia.org/wiki/Histogram",
      "summary": "A histogram is a visual representation of the distribution of numeric data. The term was first introduced by Karl Pearson.  To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014 divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent and are often (but not required to be) of equal size.Bins are typically of equal width, but unequal bin sizes are sometimes used.\nHistograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.\nHistograms are sometimes confused with bar charts. A histogram is used for quantitative data, where the bins represent ranges of values, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.A bar graph and a histogram are two common types of graphical representations of data. While they may look similar, there are some key differences between the two that are important to understand.\nA bar graph is a chart that uses bars to represent the frequency or quantity of different categories of data. The bars can be either vertical or horizontal, and they are typically arranged either horizontally or vertically to make it easy to compare the different categories. Bar graphs are useful for displaying data that can be divided into discrete categories, such as the number of students in different grade levels at a school.\nA histogram, on the other hand, is a graph that shows the distribution of numerical data. It is a type of bar chart that shows the frequency or number of observations within different numerical ranges, called bins. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The histogram provides a visual representation of the distribution of the data, showing the number of observations that fall within each bin. This can be useful for identifying patterns and trends in the data, and for making comparisons between different datasets."
    },
    {
      "id": "21073209",
      "title": "Hypothesis",
      "url": "https://en.wikipedia.org/wiki/Hypothesis",
      "summary": "A hypothesis (pl.: hypotheses) is a proposed explanation for a phenomenon. For a hypothesis to be a scientific hypothesis, the scientific method requires that one can test it. Scientists generally base scientific hypotheses on previous observations that cannot satisfactorily be explained with the available scientific theories. Even though the words \"hypothesis\" and \"theory\" are often used interchangeably, a scientific hypothesis is not the same as a scientific theory. A working hypothesis is a provisionally accepted hypothesis proposed for further research in a process beginning with an educated guess or thought.A different meaning of the term hypothesis is used in formal logic, to denote the antecedent of a proposition; thus in the proposition \"If P, then Q\", P denotes the hypothesis (or antecedent); Q can be called a consequent. P is the assumption in a (possibly counterfactual) What If question. The adjective hypothetical, meaning \"having the nature of a hypothesis\", or \"being assumed to exist as an immediate consequence of a hypothesis\", can refer to any of these meanings of the term \"hypothesis\"."
    },
    {
      "id": "30284",
      "title": "Statistical hypothesis testing",
      "url": "https://en.wikipedia.org/wiki/Statistical_hypothesis_testing",
      "summary": "A statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis.\nMore generally, hypothesis testing allows us to make probabilistic statements about population parameters. Thus, it is one way of making decisions under uncertainty. Typically, hypothesis testing procedures involve a user selected tradeoff between false positives and false negatives."
    },
    {
      "id": "1309220",
      "title": "Imputation (statistics)",
      "url": "https://en.wikipedia.org/wiki/Imputation_(statistics)",
      "summary": "In statistics, imputation is the process of replacing missing data with substituted values. When substituting for a data point, it is known as \"unit imputation\"; when substituting for a component of a data point, it is known as \"item imputation\". There are three main problems that missing data causes: missing data can introduce a substantial amount of bias, make the handling and analysis of the data more arduous, and create reductions in efficiency. Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. That is to say, when one or more values are missing for a case, most statistical packages default to discarding any case that has a missing value, which may introduce bias or affect the representativeness of the results. Imputation preserves all cases by replacing missing data with an estimated value based on other available information. Once all missing values have been imputed, the data set can then be analysed using standard techniques for complete data. There have been many theories embraced by scientists to account for missing data but the majority of them introduce bias.  A few of the well known attempts to deal with missing data include: hot deck and cold deck imputation; listwise and pairwise deletion; mean imputation; non-negative matrix factorization; regression imputation; last observation carried forward; stochastic imputation; and multiple imputation.\n\n"
    },
    {
      "id": "27577",
      "title": "Statistical inference",
      "url": "https://en.wikipedia.org/wiki/Statistical_inference",
      "summary": "Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates. It is assumed that the observed data set is sampled from a larger population.\nInferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. In machine learning, the term inference is sometimes used instead to mean \"make a prediction, by evaluating an already trained model\"; in this context inferring properties of the model is referred to as training or learning (rather than inference), and using a model for prediction is referred to as inference (instead of prediction); see also predictive inference.\n\n"
    },
    {
      "id": "18985062",
      "title": "Information",
      "url": "https://en.wikipedia.org/wiki/Information",
      "summary": "Information is an abstract concept that refers to that which has the power to inform.  At the most fundamental level, information pertains to the interpretation (perhaps formally) of that which may be sensed, or their abstractions.  Any natural process that is not completely random and any observable pattern in any medium can be said to convey some amount of information.  Whereas digital signals and other data use discrete signs to convey information, other phenomena and artifacts such as analogue signals, poems, pictures, music or other sounds, and currents convey information in a more continuous form.  Information is not knowledge itself, but the meaning that may be derived from a representation through interpretation.The concept of information is relevant or connected to various concepts, including constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, proposition, representation, and entropy.\nInformation is often processed iteratively: Data available at one step are processed into information to be interpreted and processed at the next step.  For example, in written text each symbol or letter conveys information relevant to the word it is part of, each word conveys information relevant to the phrase it is part of, each phrase conveys information relevant to the sentence it is part of, and so on until at the final step information is interpreted and becomes knowledge in a given domain.  In a digital signal, bits may be interpreted into the symbols, letters, numbers, or structures that convey the information available at the next level up.  The key characteristic of information is that it is subject to interpretation and processing.\nThe derivation of information from a signal or message may be thought of as the resolution of ambiguity or uncertainty that arises during the interpretation of patterns within the signal or message.Information may be structured as data. Redundant data can be compressed up to an optimal size, which is the theoretical limit of compression.\nThe information available through a collection of data may be derived by analysis.  For example, a restaurant collects data from every customer order.  That information may be analyzed to produce knowledge that is put to use when the business subsequently wants to identify the most popular or least popular dish.Information can be transmitted in time, via data storage, and space, via communication and telecommunication. Information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, all information is always conveyed as the content of a message.\nInformation can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication.\nThe uncertainty of an event is measured by its probability of occurrence. Uncertainty is inversely proportional to the probability of occurrence. Information theory takes advantage of this by concluding that more uncertain events require more information to resolve their uncertainty. The bit is a typical unit of information. It is 'that which reduces uncertainty by half'. Other units such as the nat may be used. For example, the information encoded in one \"fair\" coin flip is log2(2/1) = 1 bit, and in two fair coin flips is log2(4/1) = 2 bits. A 2011 Science article estimates that 97% of technologically stored information was already in digital bits in 2007 and that the year 2002 was the beginning of the digital age for information storage (with digital storage capacity bypassing analogue for the first time)."
    },
    {
      "id": "429063",
      "title": "Information art",
      "url": "https://en.wikipedia.org/wiki/Information_art",
      "summary": "Information art, which is also known as informatism or data art, is an art form that is inspired by and principally incorporates data, computer science, information technology, artificial intelligence, and related data-driven fields. The information revolution has resulted in over-abundant data that are critical in a wide range of areas, from the Internet to healthcare systems. Related to conceptual art, electronic art and new media art, informatism considers this new technological, economical, and cultural paradigm shift, such that artworks may provide social commentaries, synthesize multiple disciplines, and develop new aesthetics. Realization of information art often take, although not necessarily, interdisciplinary and multidisciplinary approaches incorporating visual, audio, data analysis, performance, and others. Furthermore, physical and virtual installations involving informatism often provide human-computer interaction that generate artistic contents based on the processing of large amounts of data.\n\n"
    },
    {
      "id": "234824",
      "title": "Information design",
      "url": "https://en.wikipedia.org/wiki/Information_design",
      "summary": "Information design is the practice of presenting information in a way that fosters an efficient and effective understanding of the information. The term has come to be used for a specific area of graphic design related to displaying information effectively, rather than just attractively or for artistic expression. Information design is closely related to the field of data visualization and is often taught as part of graphic design courses. The broad applications of information design along with its close connections to other fields of design and communication practices have created some overlap in the definitions of communication design, data visualization, and information architecture.  \nAccording to Per Mollerup, information design is explanation design. It explains facts of the universe and leads to knowledge and informed action.\n\n"
    },
    {
      "id": "450703",
      "title": "Display device",
      "url": "https://en.wikipedia.org/wiki/Display_device",
      "summary": "A display device is an output device for presentation of information in visual or tactile form (the latter used for example in tactile electronic displays for blind people). When the input information that is supplied has an electrical signal the display is called an electronic display.\nCommon applications for electronic visual displays are television sets or computer monitors."
    },
    {
      "id": "2008426",
      "title": "Information systems technician",
      "url": "https://en.wikipedia.org/wiki/Information_systems_technician",
      "summary": "An information systems technician is a technician whose responsibility is maintaining communications and computer systems."
    },
    {
      "id": "2336317",
      "title": "Intelligence cycle",
      "url": "https://en.wikipedia.org/wiki/Intelligence_cycle",
      "summary": "The intelligence cycle is an idealized model of how intelligence is  processed in civilian and military intelligence agencies, and law enforcement organizations.  It is a closed path consisting of repeating nodes, which (if followed) will result in finished intelligence. The stages of the intelligence cycle include the issuance of requirements by decision makers, collection, processing, analysis, and publication (i.e., dissemination) of intelligence. The circuit is completed when decision makers provide feedback and revised requirements. The intelligence cycle is also called intelligence process by the U.S. Department of Defense (DoD) and the uniformed services.\n\n"
    },
    {
      "id": "714366",
      "title": "Internal consistency",
      "url": "https://en.wikipedia.org/wiki/Internal_consistency",
      "summary": "In statistics and research, internal consistency is typically a measure based on the correlations between different items on the same test (or the same subscale on a larger test). It measures whether several items that propose to measure the same general construct produce similar scores. For example, if a respondent expressed agreement with the statements \"I like to ride bicycles\" and \"I've enjoyed riding bicycles in the past\", and disagreement with the statement \"I hate bicycles\", this would be indicative of good internal consistency of the test.\n\n"
    },
    {
      "id": "216262",
      "title": "JSTOR",
      "url": "https://en.wikipedia.org/wiki/JSTOR",
      "summary": "JSTOR ( JAY-stor; short for Journal Storage) is a digital library of academic journals, books, and primary sources founded in 1994. Originally containing digitized back issues of academic journals, it now encompasses books and other primary sources as well as current issues of journals in the humanities and social sciences. It provides full-text searches of almost 2,000 journals. Most access is by subscription but some of the site is public domain, and open access content is available free of charge."
    },
    {
      "id": "50146071",
      "title": "Jeffrey Heer",
      "url": "https://en.wikipedia.org/wiki/Jeffrey_Heer",
      "summary": "Jeffrey Michael Heer (born June 15, 1979) is an American computer scientist best known for his work on information visualization and interactive data analysis. He is a professor of computer science & engineering at the University of Washington, where he directs the UW Interactive Data Lab. He co-founded Trifacta with Joe Hellerstein and Sean Kandel in 2012.\n\n"
    },
    {
      "id": "15942",
      "title": "John von Neumann",
      "url": "https://en.wikipedia.org/wiki/John_von_Neumann",
      "summary": "John von Neumann ( von NOY-m\u0259n; Hungarian: Neumann J\u00e1nos Lajos [\u02c8n\u0252jm\u0252n \u02c8ja\u02d0no\u0283 \u02c8l\u0252jo\u0283]; December 28, 1903 \u2013 February 8, 1957) was a Hungarian-American mathematician, physicist, computer scientist, engineer and polymath. He had perhaps the widest coverage of any mathematician of his time,  integrating pure and applied sciences and making major contributions to many fields, including mathematics, physics, economics, computing, and statistics. He was a pioneer in building the mathematical framework of quantum physics, in the development of functional analysis, and in game theory, introducing or codifying concepts including cellular automata, the universal constructor and the digital computer. His analysis of the structure of self-replication preceded the discovery of the structure of DNA.\nDuring World War II, von Neumann worked on the Manhattan Project. He developed the mathematical models behind the explosive lenses used in the implosion-type nuclear weapon. Before and after the war, he consulted for many organizations including the Office of Scientific Research and Development, the Army's Ballistic Research Laboratory, the Armed Forces Special Weapons Project and the Oak Ridge National Laboratory. At the peak of his influence in the 1950s, he chaired a number of Defense Department committees including the Strategic Missile Evaluation Committee and the ICBM Scientific Advisory Committee. He was also a member of the influential Atomic Energy Commission in charge of all atomic energy development in the country. He played a key role alongside Bernard Schriever and Trevor Gardner in the design and development of the United States' first ICBM programs. At that time he was considered the nation's foremost expert on nuclear weaponry and the leading defense scientist at the Pentagon.\nVon Neumann's contributions and intellectual ability drew praise from colleagues in physics, mathematics, and beyond. Accolades he received range from the Medal of Freedom to a crater on the Moon named in his honor."
    },
    {
      "id": "34498283",
      "title": "Jonathan Koomey",
      "url": "https://en.wikipedia.org/wiki/Jonathan_Koomey",
      "summary": "Jonathan Koomey is a researcher who identified a long-term trend in energy-efficiency of computing that has come to be known as Koomey's law.  From 1984 to 2003, Dr. Koomey was at Lawrence Berkeley National Laboratory, where he founded and led the End-Use Forecasting group, and has been a visiting professor at Stanford University, Yale University, and the University of California, Berkeley. He has also been a lecturer and a consulting professor at Stanford and a lecturer at UC Berkeley.  He is a graduate of Harvard University (A.B) and University of California at Berkeley (M.S. and Ph.D).  His research focuses on the economics of greenhouse gas emissions and the effects of information technology on resource use.  He has also published extensively on critical thinking skills and business analytics."
    },
    {
      "id": "38455554",
      "title": "Julia (programming language)",
      "url": "https://en.wikipedia.org/wiki/Julia_(programming_language)",
      "summary": "Julia is a high-level, general-purpose dynamic programming language, most commonly used for numerical analysis and computational science. Distinctive aspects of Julia's design include a type system with parametric polymorphism and the use of multiple dispatch as a core programming paradigm, efficient garbage collection, and a just-in-time (JIT) compiler (with support for ahead-of-time compilation).\nJulia can be run similar to (interpreted) scripting languages (i.e. Julia has a REPL), and does by default using its runtime (when preinstalled), but Julia programs/source code can also optionally be sent to users in one ready-to-install/run file, which can be made quickly, not needing anything preinstalled. Julia programs can also be (separately) compiled to binary executables, even allowing no-source-code distribution. Such compilation is not needed for speed, since Julia is also compiled when running interactively, but it can help with hiding source code. Features of the language can be separately compiled, so Julia can be used, for example, with its runtime or without it (which allows for smaller executables and libraries but is limited in capabilities). Julia can be called from other languages, e.g. Python and R, and several Julia packages have been made easily available from those languages, in the form of libraries for them.\nJulia's Visual Studio Code extension provides a fully-featured integrated development environment with support for debugging, linting, and profiling."
    },
    {
      "id": "73044681",
      "title": "Julie Linsey",
      "url": "https://en.wikipedia.org/wiki/Julie_Linsey",
      "summary": "Julie Stahmer Linsey (born 1979) is an American mechanical engineer whose research concerns creativity in the early phases of engineering design. She is a professor in the George W. Woodruff School of Mechanical Engineering at Georgia Tech.\n\n"
    },
    {
      "id": "24960689",
      "title": "KNIME",
      "url": "https://en.wikipedia.org/wiki/KNIME",
      "summary": "KNIME (), the Konstanz Information Miner, is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining \"Building Blocks of Analytics\" concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming.\nSince 2006, KNIME has been used in pharmaceutical research, it also used in other areas such as CRM customer data analysis, business intelligence, text mining and financial data analysis. Recently attempts were made to use KNIME as robotic process automation (RPA) tool.KNIME's headquarters are based in Zurich, with additional offices in Konstanz, Berlin, and Austin (USA).\n\n"
    },
    {
      "id": "396743",
      "title": "Kenneth G. Wilson",
      "url": "https://en.wikipedia.org/wiki/Kenneth_G._Wilson",
      "summary": "Kenneth Geddes \"Ken\" Wilson (June 8, 1936 \u2013 June 15, 2013) was an American theoretical physicist and a pioneer in leveraging computers for studying particle physics. He was awarded the 1982 Nobel Prize in Physics for his work on phase transitions\u2014illuminating the subtle essence of phenomena like melting ice and emerging magnetism.  It was embodied in his fundamental work on the renormalization group."
    },
    {
      "id": "75971902",
      "title": "Datasaurus dozen",
      "url": "https://en.wikipedia.org/wiki/Datasaurus_dozen",
      "summary": "The Datasaurus dozen comprises thirteen data sets that have nearly identical simple descriptive statistics to two decimal places, yet have very different distributions and appear very different when graphed. It was inspired by the smaller Anscombe's quartet that was created in 1973."
    },
    {
      "id": "55704468",
      "title": "LTPP Data Analysis Contest",
      "url": "https://en.wikipedia.org/wiki/LTPP_Data_Analysis_Contest",
      "summary": "The LTPP International Data Analysis Contest or the LTPP Data Analysis Contest is an annual international data analysis contest held by the American Society of Civil Engineers and Federal Highway Administration. As the name suggests, the participants are supposed to use the LTPP data in their analysis. The winners of this data analysis contest are announced in the early January during the Transportation Research Board annual meeting."
    },
    {
      "id": "5074413",
      "title": "Lattice Boltzmann methods",
      "url": "https://en.wikipedia.org/wiki/Lattice_Boltzmann_methods",
      "summary": "The lattice Boltzmann methods (LBM), originated from the lattice gas automata (LGA) method (Hardy-Pomeau-Pazzis and Frisch-Hasslacher-Pomeau models), is a class of computational fluid dynamics (CFD) methods for fluid simulation. Instead of solving the Navier\u2013Stokes equations directly, a fluid density on a lattice is simulated with streaming and collision (relaxation) processes. The method is versatile as the model fluid can straightforwardly be made to mimic common fluid behaviour like vapour/liquid coexistence, and so fluid systems such as liquid droplets can be simulated. Also, fluids in complex environments such as porous media can be straightforwardly simulated, whereas with complex boundaries other CFD methods can be hard to work with. \n\n"
    },
    {
      "id": "21850768",
      "title": "Leland Wilkinson",
      "url": "https://en.wikipedia.org/wiki/Leland_Wilkinson",
      "summary": "Leland Wilkinson (November 5, 1944 \u2013 December 10, 2021) was an American statistician and computer scientist at H2O.ai and Adjunct Professor of Computer Science at University of Illinois at Chicago. Wilkinson developed the SYSTAT statistical package in the early 1980s, sold it to SPSS in 1995, and worked at SPSS for 10 years recruiting and managing the visualization team. He left SPSS in 2008 and became Executive VP of SYSTAT Software Inc. in Chicago. He then served as the VP of Data Visualization at Skytree, Inc and VP of Statistics at Tableau Software before joining H2O.ai. His research focused on scientific visualization and statistical graphics. In these communities he was well known for his book The Grammar of Graphics, which was the foundation for the R package ggplot2.\n\n"
    },
    {
      "id": "227686",
      "title": "Lennard-Jones potential",
      "url": "https://en.wikipedia.org/wiki/Lennard-Jones_potential",
      "summary": "In computational chemistry, molecular physics, and physical chemistry the Lennard-Jones potential (also termed the LJ potential or 12-6 potential; named for John Lennard-Jones) is an intermolecular pair potential. Out of all the intermolecular potentials, the Lennard-Jones potential is probably the one that has been the most extensively studied. It is considered an archetype model for simple yet realistic intermolecular interactions.\nThe Lennard-Jones potential models soft repulsive and attractive (van der Waals) interactions. Hence, the Lennard-Jones potential describes electronically neutral atoms or molecules. The commonly used expression for the Lennard-Jones potential is\n\nwhere r is the distance between two interacting particles, \u03b5 is the depth of the potential well (usually referred to as 'dispersion energy'), and \u03c3 is the distance at which the particle-particle potential energy V is zero (often referred to as 'size of the particle'). The Lennard-Jones potential has its minimum at a distance of \n  \n    \n      \n        r\n        =\n        \n          r\n          \n            \n              m\n              i\n              n\n            \n          \n        \n        =\n        \n          2\n          \n            1\n            \n              /\n            \n            6\n          \n        \n        \u03c3\n        ,\n      \n    \n    {\\displaystyle r=r_{\\rm {min}}=2^{1/6}\\sigma ,}\n   where the potential energy has the value \n  \n    \n      \n        V\n        =\n        \u2212\n        \u03b5\n        .\n      \n    \n    {\\displaystyle V=-\\varepsilon .}\n  \nThe Lennard-Jones potential is a simplified model that yet describes the essential features of interactions between simple atoms and molecules: Two interacting particles repel each other at very close distance, attract each other at moderate distance, and do not interact at infinite distance, as shown in Figure 1. The Lennard-Jones potential is a pair potential, i.e. no three- or multi-body interactions are covered by the potential.\nStatistical mechanics and computer simulations can be used to study the Lennard-Jones potential and to obtain thermophysical properties of the 'Lennard-Jones substance'. The Lennard-Jones substance is often referred to as 'Lennard-Jonesium,' suggesting that it is viewed as a (fictive) chemical element.  Moreover, its energy and length parameters can be adjusted to fit many different real substances. Both the Lennard-Jones potential and, accordingly, the Lennard-Jones substance are simplified yet realistic models, such as they accurately capture essential physical principles like the presence of a critical and a triple point, condensation and freezing. Due in part to its mathematical simplicity, the Lennard-Jones potential has been extensively used in studies on matter since the early days of computer simulation. The Lennard-Jones potential is probably still the most frequently studied model potential.The Lennard-Jones potential is usually the standard choice for the development of theories for matter (especially soft-matter) as well as for the development and testing of computational methods and algorithms. Upon adjusting the model parameters \u03b5 and \u03c3 to real substance properties, the Lennard-Jones potential can be used to describe simple substance (like noble gases) with good accuracy. Furthermore, the Lennard-Jones potential is often used as a building block in molecular models (a.k.a. force fields) for more complex substances."
    },
    {
      "id": "1558201",
      "title": "Lev Manovich",
      "url": "https://en.wikipedia.org/wiki/Lev_Manovich",
      "summary": "Lev Manovich ( MAN-\u0259-vitch) is an artist, an author and a theorist of digital culture. He is a Distinguished Professor at the Graduate Center of the City University of New York. Manovich played a key role in creating four new research fields: new media studies (1991-), software studies (2001-), cultural analytics (2007-) and AI aesthetics (2018-). Manovich's current research focuses on generative media, AI culture, digital art, and media theory.Manovich is the founder and director of the Cultural Analytics Lab (called Software Studies Initiative 2007-2016), which pioneered use of data science and data visualization for the analysis of massive collections of images and video (cultural analytics). The lab was commissioned to create visualizations of cultural datasets for Google, New York Public Library, and New York's Museum of Modern Art (MoMA).He is the author and editor of 15 books including The Language of New Media that has been translated into fourteen languages. Manovich's latest academic book Cultural Analytics was published in 2020 by the MIT Press.\n\n"
    },
    {
      "id": "51354460",
      "title": "List of big data companies",
      "url": "https://en.wikipedia.org/wiki/List_of_big_data_companies",
      "summary": "This is an alphabetical list of notable IT companies using the marketing term big data:\n\nAlpine Data Labs, an analytics interface working with Apache Hadoop and big data\nAzure Data Lake is a highly scalable data storage and analytics service. The service is hosted in Azure, Microsoft's public cloud\nBig Data Partnership, a professional services company based in London\nBig Data Scoring, a cloud-based service that lets consumer lenders improve loan quality and acceptance rates through the use of big data\nBigPanda, a technology company headquartered in Mountain View, California\nBright Computing,  developer of software for deploying and managing high-performance (HPC) clusters, big data clusters, and OpenStack in data centers and in the cloud\nClarivate Analytics, a global company that owns and operates a collection of subscription-based services focused largely on analytics\nCloudera, an American-based software company that provides Apache Hadoop-based software, support and services, and training to business customers\nCompuverde, an IT company with a focus on big data storage\nCVidya, a provider of big data analytics products for communications and digital service providers\nCybatar Cloud, a cloud-based system for managing, assigning, tracking and monitoring on-demand goods and service delivery tasks and agents.\nDatabricks, a company founded by the creators of Apache Spark\nDataiku, a French computer software company\nDataStax\nDomo\nFluentd\nGreenplum\nGroundhog Technologies\nHack/reduce\nHazelcast\nHortonworks\nHPCC Systems\nIBM\nImply Corporation\nMapR\nMarkLogic\nMedio\nMedopad\nNetApp\nOracle Cloud Platform\nPalantir Technologies\nPentaho, a data integration and business analytics company with an enterprise-class, open source-based platform for big data deployments\nPitney Bowes\nPlatfora\nQumulo\nRocket Fuel Inc.\nSAP SE, offers the SAP Data Hub to connect data bases and other products through acquisition of Altiscale\nSalesforceIQ\nSense Networks\nShanghai Data Exchange\nSK Telecom, developer of big data analytics platform Metatron Discovery\nSojern\nSplunk\nSumo Logic\nTeradata\nThetaRay\nTubeMogul\nVoloMetrix\nZaloni, deployment and vendor agnostic data lake management platform\nZoomdata\nInData Labs\nDataToBiz\nDataforest\nTalentica\nASCENDING\nThirdEye Data\nGreenM\naltexsoft\nGroup BWT\n\n"
    },
    {
      "id": "49082762",
      "title": "List of datasets for machine-learning research",
      "url": "https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research",
      "summary": "These datasets are used in machine learning (ML) research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.Many organizations including  governments publish and share their datasets . The datasets are classified, based on the licenses, as Open data and Non-Open data.\nThe datasets from  various governmental-bodies are presented in List of open government data sites. The datasets are ported on open data portals. They are made available for searching, depositing and accessing through interfaces like Open API.  The datasets are made available as various sorted types and subtypes."
    },
    {
      "id": "1444165",
      "title": "MECE principle",
      "url": "https://en.wikipedia.org/wiki/MECE_principle",
      "summary": "The MECE principle, (mutually exclusive and collectively exhaustive) is a grouping principle for separating a set of items into subsets that are mutually exclusive (ME) and collectively exhaustive (CE). It was developed in the late 1960s by Barbara Minto at McKinsey & Company and underlies her Minto Pyramid Principle, and while she takes credit for MECE, according to her interview with McKinsey, she says the idea for MECE goes back as far as to Aristotle.The MECE principle has been used in the business mapping process wherein the optimum arrangement of information is exhaustive and does not double count at any level of the hierarchy. Examples of MECE arrangements include categorizing people by year of birth (assuming all years are known), apartments by their building number, letters by postmark, and dice rolls. A non-MECE example would be categorization by nationality, because nationalities are neither mutually exclusive (some people have dual nationality) nor collectively exhaustive (some people have none)."
    },
    {
      "id": "23244381",
      "title": "Manipulation check",
      "url": "https://en.wikipedia.org/wiki/Manipulation_check",
      "summary": "Manipulation check is a term in experimental research in the social sciences which refers to certain kinds of secondary evaluations of an experiment.\n\n"
    },
    {
      "id": "642328",
      "title": "McKinsey & Company",
      "url": "https://en.wikipedia.org/wiki/McKinsey_%26_Company",
      "summary": "McKinsey & Company is an American multinational strategy and management consulting firm that offers professional services to corporations, governments, and other organizations. Founded in 1926 by James O. McKinsey, McKinsey is the oldest and largest of the \"Big Three\" management consultancies (MBB). The firm mainly focuses on the finances and operations of their clients.\nUnder the direction of Marvin Bower, McKinsey expanded into Europe during the 1940s and 1950s. In the 1960s, McKinsey's Fred Gluck\u2014along with Boston Consulting Group's Bruce Henderson, Bill Bain at Bain & Company, and Harvard Business School's Michael Porter\u2014initiated a program designed to transform corporate culture. A 1975 publication by McKinsey's John L. Neuman introduced the business practice of \"overhead value analysis\" that contributed to a downsizing trend that eliminated many jobs in middle management.McKinsey has a notoriously competitive hiring process and is widely seen as one of the most selective employers in the world. McKinsey recruits primarily from top business schools and was one of the first management consultancies to recruit a limited number of candidates with advanced academic degrees (e.g. PhD, MD) and deep field expertise, and who have demonstrated business acumen and analytical skills. McKinsey publishes a business magazine, the McKinsey Quarterly.\nMcKinsey has been the subject of significant controversy related to its business practices. The company has been criticized for its role promoting OxyContin use during the opioid crisis in North America, its work with Enron, and its work for authoritarian regimes like Saudi Arabia and Russia.\n\n"
    },
    {
      "id": "19022",
      "title": "Measurement",
      "url": "https://en.wikipedia.org/wiki/Measurement",
      "summary": "Measurement is the quantification of attributes of an object or event, which can be used to compare with other objects or events.\nIn other words, measurement is a process of determining how large or small a physical quantity is as compared to a basic reference quantity of the same kind.\nThe scope and application of measurement are dependent on the context and discipline. In natural sciences and engineering, measurements do not apply to nominal properties of objects or events, which is consistent with the guidelines of the International vocabulary of metrology published by the International Bureau of Weights and Measures. However, in other fields such as statistics as well as the social and behavioural sciences, measurements can have multiple levels, which would include nominal, ordinal, interval and ratio scales.Measurement is a cornerstone of trade, science, technology and quantitative research in many disciplines. Historically, many measurement systems existed for the varied fields of human existence to facilitate comparisons in these fields. Often these were achieved by local agreements between trading partners or collaborators. Since the 18th century, developments progressed towards unifying, widely accepted standards that resulted in the modern International System of Units (SI). This system reduces all physical measurements to a mathematical combination of seven base units. The science of measurement is pursued in the field of metrology.\nMeasurement is defined as the process of comparison of an unknown quantity with a known or standard quantity.\n\n"
    },
    {
      "id": "47403",
      "title": "Instrumentation",
      "url": "https://en.wikipedia.org/wiki/Instrumentation",
      "summary": "Instrumentation is a collective term for measuring instruments, used for indicating, measuring, and recording physical quantities. It is also a field of study about the art and science about making measurement instruments, involving the related areas of metrology, automation, and control theory.\nThe term has its origins in the art and science of scientific instrument-making.\nInstrumentation can refer to devices as simple as direct-reading thermometers, or as complex as multi-sensor components of industrial control systems. Instruments can be found in laboratories, refineries, factories and vehicles, as well as in everyday household use (e.g., smoke detectors and thermostats)\n\n"
    },
    {
      "id": "56107",
      "title": "Metropolis\u2013Hastings algorithm",
      "url": "https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm",
      "summary": "In statistics and statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value). Metropolis\u2013Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high.  For single-dimensional distributions, there are usually other methods (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and these are free from the problem of autocorrelated samples that is inherent in MCMC methods.\n\n"
    },
    {
      "id": "47939883",
      "title": "Mike Bostock",
      "url": "https://en.wikipedia.org/wiki/Mike_Bostock",
      "summary": "Michael Bostock is an American computer scientist and data visualization specialist. He is one of the co-creators of Observable and a key developer of D3.js, a JavaScript library used to produce dynamic, interactive  data visualizations for web browsers. He also contributed to the preceding Protovis framework.\n\n"
    },
    {
      "id": "36197584",
      "title": "Misleading graph",
      "url": "https://en.wikipedia.org/wiki/Misleading_graph",
      "summary": "In statistics, a misleading graph, also known as a distorted graph, is a graph that misrepresents data, constituting a misuse of statistics and with the result that an incorrect conclusion may be derived from it.\nGraphs may be misleading by being excessively complex or poorly constructed. Even when constructed to display the characteristics of their data accurately, graphs can be subject to different interpretations, or unintended kinds of data can seemingly and ultimately erroneously be derived.Misleading graphs may be created intentionally to hinder the proper interpretation of data or accidentally due to unfamiliarity with graphing software, misinterpretation of data, or because data cannot be accurately conveyed. Misleading graphs are often used in false advertising. One of the first authors to write about misleading graphs was Darrell Huff, publisher of the 1954 book How to Lie with Statistics.\nThe field of data visualization describes ways to present information that avoids creating misleading graphs."
    },
    {
      "id": "7859676",
      "title": "Missing data",
      "url": "https://en.wikipedia.org/wiki/Missing_data",
      "summary": "In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation.  Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.\nMissing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (\"subject\"). Some items are more likely to generate a nonresponse than others: for example items about private subjects such as income. Attrition is a type of missingness that can occur in longitudinal studies\u2014for instance studying development where a measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing.\nData often are missing in research in economics, sociology, and political science because governments or private entities choose not to, or fail to, report critical statistics, or because the information is not available. Sometimes missing values are caused by the researcher\u2014for example, when data collection is done improperly or mistakes are made in data entry.These forms of missingness take different types, with different impacts on the validity of conclusions from research: Missing completely at random, missing at random, and missing not at random.  Missing data can be handled similarly as censored data.\n\n"
    },
    {
      "id": "198608",
      "title": "Molecular dynamics",
      "url": "https://en.wikipedia.org/wiki/Molecular_dynamics",
      "summary": "Molecular dynamics (MD) is a computer simulation method for analyzing the physical movements of atoms and molecules. The atoms and molecules are allowed to interact for a fixed period of time, giving a view of the dynamic \"evolution\" of the system. In the most common version, the trajectories of atoms and molecules are determined by numerically solving Newton's equations of motion for a system of interacting particles, where forces between the particles and their potential energies are often calculated using interatomic potentials or molecular mechanical force fields. The method is applied mostly in chemical physics, materials science, and biophysics.\nBecause molecular systems typically consist of a vast number of particles, it is impossible to determine the properties of such complex systems analytically; MD simulation circumvents this problem by using numerical methods. However, long MD simulations are mathematically ill-conditioned, generating cumulative errors in numerical integration that can be minimized with proper selection of algorithms and parameters, but not eliminated.\nFor systems that obey the ergodic hypothesis, the evolution of one molecular dynamics simulation may be used to determine the macroscopic thermodynamic properties of the system: the time averages of an ergodic system correspond to microcanonical ensemble averages. MD has also been termed \"statistical mechanics by numbers\" and \"Laplace's vision of Newtonian mechanics\" of predicting the future by animating nature's forces and allowing insight into molecular motion on an atomic scale."
    },
    {
      "id": "1112960",
      "title": "Monte Carlo integration",
      "url": "https://en.wikipedia.org/wiki/Monte_Carlo_integration",
      "summary": "In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly chooses points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.There are different methods to perform a Monte Carlo integration, such as uniform sampling, stratified sampling, importance sampling, sequential Monte Carlo (also known as a particle filter), and mean-field particle methods."
    },
    {
      "id": "56098",
      "title": "Monte Carlo method",
      "url": "https://en.wikipedia.org/wiki/Monte_Carlo_method",
      "summary": "Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. The name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, physicist Stanislaw Ulam, was inspired by his uncle's gambling habits.\nMonte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution. They can also be used to model phenomena with significant uncertainty in inputs, such as calculating the risk of a nuclear power plant failure. Monte Carlo methods are often implemented using computer simulations, and they can provide approximate solutions to problems that are otherwise intractable or too complex to analyze mathematically.\nMonte Carlo methods are widely used in various fields of science, engineering, and mathematics, such as physics, chemistry, biology, statistics, artificial intelligence, finance, and cryptography. They have also been applied to social sciences, such as sociology, psychology, and political science. Monte Carlo methods have been recognized as one of the most important and influential ideas of the 20th century, and they have enabled many scientific and technological breakthroughs.\nMonte Carlo methods also have some limitations and challenges, such as the trade-off between accuracy and computational cost, the curse of dimensionality, the reliability of random number generators, and the verification and validation of the results."
    },
    {
      "id": "47933469",
      "title": "Moritz Stefaner",
      "url": "https://en.wikipedia.org/wiki/Moritz_Stefaner",
      "summary": "Moritz Stefaner is a German data visualization specialist. He is notable for his work for organisations like the OECD, the World Economic Forum, Skype, dpa, and Max Planck Research Society. Stefaner is a multiple winner of the Kantar Information is Beautiful awards. His data visualisation work has been exhibited at Venice Biennale of Architecture and Ars Electronica. He has contributed to Beautiful Visualisation published by Springer and was interviewed for the books New Challenges for Data Design published by Springer and Alberto Cairo's The Functional Art.One of Stefaner's most widely known works is the visualisation of the OECD Better Life Index. Among other notable projects is his interactive installation On Broadway, his work for the FIFA, and his design of the new OECD data portal.With Enrico Bertini, he produces Data Stories, a podcast on data visualization. Stefaner studied Cognitive Science (B.Sc., University of Osnabr\u00fcck) and Interface Design (M.A., University of Applied Sciences Potsdam). He lives in Lilienthal, Germany.\n\n"
    },
    {
      "id": "41240510",
      "title": "Morse/Long-range potential",
      "url": "https://en.wikipedia.org/wiki/Morse/Long-range_potential",
      "summary": "The Morse/Long-range potential (MLR potential) is an interatomic interaction model for the potential energy of a diatomic molecule. Due to the simplicity of the regular Morse potential (it only has three adjustable parameters), it is very limited in its applicability in modern spectroscopy. The MLR potential is a modern version of the Morse potential which has the correct theoretical long-range form of the potential naturally built into it.  It has been an important tool for spectroscopists to represent experimental data, verify measurements, and make predictions. It is useful for its extrapolation capability when data for certain regions of the potential are missing, its ability to predict energies with accuracy often better than the most sophisticated ab initio techniques, and its ability to determine precise empirical values for physical parameters such as the dissociation energy, equilibrium bond length, and long-range constants. Cases of particular note include:\n\nthe c-state of dilithium (Li2): where the MLR potential was successfully able to bridge a gap of more than 5000 cm\u22121 in experimental data. Two years later it was found that the MLR potential was able to successfully predict the energies in the middle of this gap, correctly within about 1 cm\u22121. The accuracy of these predictions was much better than the most sophisticated ab initio techniques at the time.\nthe A-state of Li2: where Le Roy et al. constructed an MLR potential which determined the C3 value for atomic lithium to a higher-precision than any previously measured atomic oscillator strength, by an order of magnitude. This lithium oscillator strength is related to the radiative lifetime of atomic lithium and is used as a benchmark for atomic clocks and measurements of fundamental constants.\nthe a-state of KLi: where the MLR was used to build an analytic global potential successfully despite there only being a small amount of levels observed near the top of the potential."
    },
    {
      "id": "762970",
      "title": "Morse potential",
      "url": "https://en.wikipedia.org/wiki/Morse_potential",
      "summary": "The Morse potential, named after physicist Philip M. Morse, is a convenient \ninteratomic interaction model for the potential energy of a diatomic molecule. It is a better approximation for the vibrational structure of the molecule than the quantum harmonic oscillator because it explicitly includes the effects of bond breaking, such as the existence of unbound states. It also accounts for the anharmonicity of real bonds and the non-zero transition probability for overtone and combination bands. The Morse potential can also be used to model other interactions such as the interaction between an atom and a surface. Due to its simplicity (only three fitting parameters), it is not used in modern spectroscopy. However, its mathematical form inspired the MLR (Morse/Long-range) potential, which is the most popular potential energy function used for fitting spectroscopic data."
    },
    {
      "id": "42637526",
      "title": "Mosaic plot",
      "url": "https://en.wikipedia.org/wiki/Mosaic_plot",
      "summary": "A mosaic plot, Marimekko chart, Mekko chart, or sometimes percent stacked bar plot is a graphical visualization of data from two or more qualitative variables. It is the multidimensional extension of spineplots, which graphically display the same information for only one variable. It gives an overview of the data and makes it possible to recognize relationships between different variables. For example,  independence is shown when the boxes across categories all have the same areas. Mosaic plots were introduced by Hartigan and Kleiner in 1981 and expanded on by Friendly in 1994.\nMosaic plots are also called Marimekko or Mekko charts because they resemble some Marimekko prints. However, in statistical applications, mosaic plots can be colored and shaded according to deviations from independence, whereas\nMarimekko charts are colored according to the category levels, as in the image.\nAs with bar charts and spineplots, the area of the tiles, also known as the bin size, is proportional to the number of observations within that category."
    },
    {
      "id": "30928751",
      "title": "Multilinear principal component analysis",
      "url": "https://en.wikipedia.org/wiki/Multilinear_principal_component_analysis",
      "summary": "Multilinear principal component analysis (MPCA)  is a multilinear extension of principal component analysis (PCA). MPCA is employed in the analysis of M-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a \"data tensor\".  M-way arrays may be modeled by \n\nlinear tensor models such as CANDECOMP/Parafac, or\nmultilinear tensor models, such as multilinear principal component analysis (MPCA), or multilinear independent component analysis (MICA), etc.The origin of MPCA can be traced back to the Tucker decomposition and Peter Kroonenberg's \"3-mode PCA\" work. In 2000, De Lathauwer et al. restated Tucker and Kroonenberg's work in clear and concise numerical computational terms in their SIAM paper entitled \"Multilinear Singular Value Decomposition\", (HOSVD) and in their paper \"On the Best Rank-1 and Rank-(R1, R2, ..., RN ) Approximation of Higher-order Tensors\".Circa 2001, Vasilescu and Terzopoulos reframed the data analysis, recognition and synthesis problems as multilinear tensor problems. Tensor factor analysis is the compositional consequence of several causal factors of data formation, and are well suited for multi-modal data tensor analysis.  The power of the tensor framework was showcased by analyzing human motion joint angles, facial images or textures in terms of their causal factors of data formation in the following works: Human Motion Signatures\n(CVPR 2001, ICPR 2002), face recognition \u2013 TensorFaces,\n(ECCV 2002, CVPR 2003, etc.) and computer graphics \u2013 TensorTextures (Siggraph 2004).\nHistorically, MPCA has been referred to as \"M-mode PCA\", a terminology which was coined by Peter Kroonenberg in 1980. In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA terminology as a way to better differentiate between linear and multilinear tensor decomposition, as well as, to better differentiate between the work that computed 2nd order statistics associated with each data tensor mode(axis), and subsequent work on Multilinear Independent Component Analysis that computed higher order statistics associated with each tensor mode/axis.\nMultilinear PCA may be applied to compute the causal factors of data formation, or as signal processing tool on data tensors whose individual observation have either been vectorized, or whose observations are treated as a collection of column/row observations, \"data matrix\" and concatenated into a data tensor.  The main disadvantage of this approach is that rather than computing all possible combinations\nMPCA computes a set of orthonormal matrices associated with each mode of the data tensor which are analogous to the orthonormal row and column space of a matrix computed by the matrix SVD.  This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data associated with each data tensor mode(axis)."
    },
    {
      "id": "30909817",
      "title": "Multilinear subspace learning",
      "url": "https://en.wikipedia.org/wiki/Multilinear_subspace_learning",
      "summary": "Multilinear subspace learning is an approach for disentangling the causal factor of data formation and performing  dimensionality reduction.   \nThe Dimensionality reduction can be performed on a data tensor that contains a collection of observations have been vectorized, or observations that are treated as matrices and concatenated into a data tensor.  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).\nThe mapping from a high-dimensional vector space to a set of lower dimensional vector spaces is a multilinear projection. When observations are retained in the same organizational structure as matrices or higher order tensors, their representations are computed by performing linear projections into the column space, row space and fiber space.Multilinear subspace learning algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA)."
    },
    {
      "id": "2789271",
      "title": "Multiphysics simulation",
      "url": "https://en.wikipedia.org/wiki/Multiphysics_simulation",
      "summary": "In computational modelling, multiphysics simulation (often shortened to simply \"multiphysics\") is defined as the simultaneous simulation of different aspects of a physical system or systems and the interactions among them. For example, simultaneous simulation of the physical stress on an object, the temperature distribution of the object and the thermal expansion which leads to the variation of the stress and temperature distributions would be considered a multiphysics simulation. Multiphysics simulation is related to multiscale simulation, which is the simultaneous simulation of a single process on either multiple time or distance scales.As an interdisciplinary field, multiphysics simulation can span many science and engineering disciplines. Simulation methods frequently include numerical analysis,  partial differential equations and tensor analysis.\n\n"
    },
    {
      "id": "41221419",
      "title": "Multiway data analysis",
      "url": "https://en.wikipedia.org/wiki/Multiway_data_analysis",
      "summary": "Multiway data analysis is a method of analyzing large data sets by representing a collection of observations as a multiway array, \n  \n    \n      \n        \n          \n            A\n          \n        \n        \u2208\n        \n          \n            \n              C\n            \n          \n          \n            \n              I\n              \n                0\n              \n            \n            \u00d7\n            \n              I\n              \n                1\n              \n            \n            \u00d7\n            \u2026\n            \n              I\n              \n                c\n              \n            \n            \u00d7\n            \u2026\n            \n              I\n              \n                C\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}\\in {\\mathbb {C} }^{I_{0}\\times I_{1}\\times \\dots I_{c}\\times \\dots I_{C}}}\n  . The proper choice of data organization into (C+1)-way array, and analysis techniques can reveal patterns in the underlying data undetected by other methods."
    },
    {
      "id": "312648",
      "title": "Mutual exclusivity",
      "url": "https://en.wikipedia.org/wiki/Mutual_exclusivity",
      "summary": "In logic and probability theory, two events (or propositions) are mutually exclusive or disjoint if they cannot both occur at the same time. A clear example is the set of outcomes of a single coin toss, which can result in either heads or tails, but not both.\nIn the coin-tossing example, both outcomes are, in theory, collectively exhaustive, which means that at least one of the outcomes must happen, so these two possibilities together exhaust all the possibilities. However, not all mutually exclusive events are collectively exhaustive. For example, the outcomes 1 and 4 of a single roll of a six-sided die are mutually exclusive (both cannot happen at the same time) but not collectively exhaustive (there are other possible outcomes; 2,3,5,6).\n\n"
    },
    {
      "id": "4917686",
      "title": "N-body simulation",
      "url": "https://en.wikipedia.org/wiki/N-body_simulation",
      "summary": "In physics and astronomy, an N-body simulation is a simulation of a dynamical system of particles, usually under the influence of physical forces, such as gravity (see n-body problem for other applications). N-body simulations are widely used tools in astrophysics, from investigating the dynamics of few-body systems like the Earth-Moon-Sun system to understanding the evolution of the large-scale structure of the universe. In physical cosmology, N-body simulations are used to study processes of non-linear structure formation such as galaxy filaments and galaxy halos from the influence of dark matter. Direct N-body simulations are used to study the dynamical evolution of star clusters."
    },
    {
      "id": "63145468",
      "title": "Nadieh Bremer",
      "url": "https://en.wikipedia.org/wiki/Nadieh_Bremer",
      "summary": "Nadieh Bremer is a data scientist and data visualization designer. She is based out of a small town outside of Amsterdam."
    },
    {
      "id": "7309022",
      "title": "Nearest neighbor search",
      "url": "https://en.wikipedia.org/wiki/Nearest_neighbor_search",
      "summary": "Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. \nFormally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q \u2208 M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.\nMost commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.\n\n"
    },
    {
      "id": "71193445",
      "title": "Necessary condition analysis",
      "url": "https://en.wikipedia.org/wiki/Necessary_condition_analysis",
      "summary": "Necessary condition analysis (NCA) is a research approach and tool employed to discern \"necessary conditions\" within datasets. These indispensable conditions stand as pivotal determinants of particular outcomes, wherein the absence of such conditions ensures the absence of the intended result. Illustratively, the admission of a student into a Ph.D. program necessitates an adequate GMAT score; the progression of AIDS mandates the presence of HIV; and the realization of organizational change will not occur without the commitment of management. Singular in nature, these conditions possess the potential to function as bottlenecks for the desired outcome. Their absence unequivocally guarantees the failure of the intended objective, a deficiency that cannot be offset by the influence of other contributing factors. It is noteworthy, however, that the mere presence of the necessary condition does not ensure the assured attainment of success. In such instances, the condition demonstrates its necessity but lacks sufficiency. To obviate the risk of failure, the simultaneous satisfaction of each distinct necessary condition is imperative. NCA serves as a systematic mechanism, furnishing the rationale and methodological apparatus requisite for the identification and assessment of necessary conditions within extant or novel datasets. It is a powerful method for investigating causal relationships and determining the minimum requirements that must be present for an outcome to be achieved.\n\n"
    },
    {
      "id": "146103",
      "title": "Nonlinear system",
      "url": "https://en.wikipedia.org/wiki/Nonlinear_system",
      "summary": "In mathematics and science, a nonlinear system (or a non-linear system) is a system in which the change of the output is not proportional to the change of the input. Nonlinear problems are of interest to engineers, biologists, physicists, mathematicians, and many other scientists since most systems are inherently nonlinear in nature. Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems.\nTypically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one.\nIn other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations. In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it.\nAs nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos, and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.\nSome authors use the term nonlinear science for the study of nonlinear systems. This term is disputed by others:\n\nUsing a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals.\n\n"
    },
    {
      "id": "40158142",
      "title": "Nonlinear system identification",
      "url": "https://en.wikipedia.org/wiki/Nonlinear_system_identification",
      "summary": "System identification is a method of identifying or measuring the mathematical model of a system from measurements of the system inputs and outputs. The applications of system identification include any system where the inputs and outputs can be measured and include industrial processes, control systems, economic data, biology and the life sciences, medicine, social systems and many more.\nA nonlinear system is defined as any system that is not linear, that is any system that does not satisfy the superposition principle. This negative definition tends to obscure that there are very many different types of nonlinear systems. Historically, system identification for nonlinear systems has developed by focusing on specific classes of system and can be broadly categorized into five basic approaches, each defined by a model class:\n\nVolterra series models,\nBlock-structured models,\nNeural network models,\nNARMAX models, and\nState-space models.There are four steps to be followed for system identification: data gathering, model postulate, parameter identification, and model validation. Data gathering is considered as the first and essential part in identification terminology, used as the input for the model which is prepared later. It consists of selecting an appropriate data set, pre-processing and processing. It involves the implementation of the known algorithms together with the transcription of flight tapes, data storage and data management, calibration, processing, analysis, and presentation. Moreover, model validation is necessary to gain confidence in, or reject, a particular model. In particular, the parameter estimation and the model validation are integral parts of the system identification. Validation refers to the process of confirming the conceptual model and demonstrating an adequate correspondence between the computational results of the model and the actual data.\n\n"
    },
    {
      "id": "21462",
      "title": "Normal distribution",
      "url": "https://en.wikipedia.org/wiki/Normal_distribution",
      "summary": "In statistics, a normal distribution or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              \u03c3\n              \n                \n                  2\n                  \u03c0\n                \n              \n            \n          \n        \n        \n          e\n          \n            \u2212\n            \n              \n                1\n                2\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                      \u2212\n                      \u03bc\n                    \n                    \u03c3\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\n  The parameter \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   is the mean or expectation of the distribution (and also its median and mode), while the parameter \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is its standard deviation. The variance of the distribution is \n  \n    \n      \n        \n          \u03c3\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  . A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate.\nNormal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable\u2014whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of independent normal deviates is a normal deviate. Many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed.\nA normal distribution is sometimes informally called a bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions). For other names, see Naming.\nThe univariate probability distribution is generalized for vectors in the multivariate normal distribution and for matrices in the matrix normal distribution."
    },
    {
      "id": "397245",
      "title": "Numeracy",
      "url": "https://en.wikipedia.org/wiki/Numeracy",
      "summary": "Numeracy is the ability to understand, reason with, and to apply simple numerical concepts. The charity National Numeracy states: \"Numeracy means understanding how mathematics is used in the real world and being able to apply it to make the best possible decisions...It's as much about thinking and reasoning as about 'doing sums'\". Basic numeracy skills consist of comprehending fundamental arithmetical operations like addition, subtraction, multiplication, and division. For example, if one can understand simple mathematical equations such as 2 + 2 = 4, then one would be considered to possess at least basic numeric knowledge. Substantial aspects of numeracy also include number sense, operation sense, computation, measurement, geometry, probability and statistics. A numerically literate person can manage and respond to the mathematical demands of life.By contrast, innumeracy (the lack of numeracy) can have a negative impact. Numeracy has an influence on healthy behaviors, financial literacy, and career decisions. Therefore, innumeracy may negatively affect economic choices, financial outcomes, health outcomes, and life satisfaction. It also may distort risk perception in health decisions. Greater numeracy has been associated with reduced susceptibility to framing effects, less influence of nonnumerical information such as mood states, and greater sensitivity to different levels of numerical risk. Ellen Peters and her colleagues argue that  achieving the benefits of numeric literacy, however, may depend on one's numeric self-efficacy or confidence in one's skills.\n\n"
    },
    {
      "id": "216827",
      "title": "O'Reilly Media",
      "url": "https://en.wikipedia.org/wiki/O%27Reilly_Media",
      "summary": "O'Reilly Media (formerly O'Reilly & Associates) is an American learning company established by Tim O'Reilly that publishes books, produces tech conferences, and provides an online learning platform. Its distinctive brand features a woodcut of an animal on many of its book covers.\n\n"
    },
    {
      "id": "2539154",
      "title": "Orange (software)",
      "url": "https://en.wikipedia.org/wiki/Orange_(software)",
      "summary": "Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative qualitative data analysis and interactive data visualization."
    },
    {
      "id": "40935351",
      "title": "Over-the-counter data",
      "url": "https://en.wikipedia.org/wiki/Over-the-counter_data",
      "summary": "Over-the-counter data (OTCD) is a design approach used in data systems, particularly educational technology data systems, in order to increase the accuracy of users' data analyses by better reporting data. The approach involves adhering to standards that are organized by five components: Label, Supplemental Documentation, Help System, Package/Display, and Content.OTCD was inspired by the varied ways over-the-counter medication supports those using its contents. Just as it would be negligent for over-the-counter medication to contain no labeling, documentation, or other supports helping people to use its contents safely, it is deemed negligent for data systems to display data for educators without providing them with the necessary supports to best ensure it is used correctly when educators use the data to treat students\u2019 needs."
    },
    {
      "id": "38833779",
      "title": "Pandas (software)",
      "url": "https://en.wikipedia.org/wiki/Pandas_(software)",
      "summary": "Pandas (stylized as pandas) is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. The name is derived from the term \"panel data\", an econometrics term for data sets that include observations over multiple time periods for the same individuals, as well as a play on the phrase \"Python data analysis\".:\u200a5\u200a Wes McKinney started building what would become Pandas at AQR Capital while he was a researcher there from 2007 to 2010.The development of Pandas introduced into Python many comparable features of working with DataFrames that were established in the R programming language. The library is built upon another library, NumPy."
    },
    {
      "id": "2499813",
      "title": "Panel data",
      "url": "https://en.wikipedia.org/wiki/Panel_data",
      "summary": "In statistics and econometrics, panel data and longitudinal data are both multi-dimensional data involving measurements over time. Panel data is a subset of longitudinal data where observations are for the same subjects each time.\nTime series and cross-sectional data can be thought of as special cases of panel data that are in one dimension only (one panel member or individual for the former, one time point for the latter). A literature search often involves time series, cross-sectional, or panel data. Cross-panel data (CPD) is an innovative yet underappreciated source of information in the mathematical and statistical sciences. CPD stands out from other research methods because it vividly illustrates how independent and dependent variables may shift between countries. This panel data collection allows researchers to examine the connection between variables across several cross-sections and time periods and analyze the results of policy actions in other nations.A study that uses panel data is called a longitudinal study or panel study.\n\n"
    },
    {
      "id": "4060171",
      "title": "Pareto chart",
      "url": "https://en.wikipedia.org/wiki/Pareto_chart",
      "summary": "A Pareto chart is a type of chart that contains both bars and a line graph, where individual values are represented in descending order by bars, and the cumulative total is represented by the line.  The chart is named for the Pareto principle, which, in turn, derives its name from Vilfredo Pareto, a noted Italian economist."
    },
    {
      "id": "596816",
      "title": "Particle-in-cell",
      "url": "https://en.wikipedia.org/wiki/Particle-in-cell",
      "summary": "In plasma physics, the particle-in-cell (PIC) method refers to a technique used to solve a certain class of partial differential equations.  In this method, individual particles (or fluid elements) in a Lagrangian frame are tracked in continuous phase space, whereas moments of the distribution such as densities and currents are computed simultaneously on Eulerian (stationary) mesh points.\nPIC methods were already in use as early as 1955,\neven before the first Fortran compilers were available. The method gained popularity for plasma simulation in the late 1950s and early 1960s by Buneman, Dawson, Hockney, Birdsall, Morse and others. In plasma physics applications, the method amounts to following the trajectories of charged particles in self-consistent electromagnetic (or electrostatic) fields computed on a fixed mesh."
    },
    {
      "id": "153390",
      "title": "Phillips curve",
      "url": "https://en.wikipedia.org/wiki/Phillips_curve",
      "summary": "The Phillips curve is an economic model, named after Bill Phillips, that correlates reduced unemployment with increasing wages in an economy.  While Phillips did not directly link employment and inflation, this was a trivial deduction from his statistical findings. Paul Samuelson and Robert Solow made the connection explicit and subsequently Milton Friedman and Edmund Phelps put the theoretical structure in place.\nWhile there is a short-run tradeoff between unemployment and inflation, it has not been observed in the long run. In 1967 and 1968, Friedman and Phelps asserted that the Phillips curve was only applicable in the short run and that, in the long run, inflationary policies would not decrease unemployment. Friedman correctly predicted the Stagflation of the 1970's.In the 2010s the slope of the Phillips curve appears to have declined and there has been controversy over the usefulness of the Phillips curve in predicting inflation. A 2022 study found that the slope of the Phillips curve is small and was small even during the early 1980s. Nonetheless, the Phillips curve is still used by central banks in understanding and forecasting inflation."
    },
    {
      "id": "394780",
      "title": "Physics Analysis Workstation",
      "url": "https://en.wikipedia.org/wiki/Physics_Analysis_Workstation",
      "summary": "The Physics Analysis Workstation (PAW) is an interactive, scriptable computer software tool for data analysis and graphical presentation in High Energy Physics (HEP).\nThe development of this software tool started at CERN in 1986, it was optimized for the processing of very large amounts of data. It was based on and intended for inter-operation with components of CERNLIB, an extensive collection of Fortran libraries.\nPAW had been a standard tool in high energy physics for decades, yet was essentially unmaintained. Despite continuing popularity as of 2008, it has been losing ground to the C++-based ROOT package. Conversion tutorials exist. In 2014, development and support were stopped."
    },
    {
      "id": "6879051",
      "title": "Line chart",
      "url": "https://en.wikipedia.org/wiki/Line_chart",
      "summary": "A line chart or line graph, also known as curve chart, is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments.   It is a basic type of chart common in many fields. It is similar to a scatter plot except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments.  A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. In these cases they are known as run charts.\n\n"
    },
    {
      "id": "19774918",
      "title": "Plot (graphics)",
      "url": "https://en.wikipedia.org/wiki/Plot_(graphics)",
      "summary": "A plot is a graphical technique for representing a data set, usually as a graph showing the relationship between two or more variables. The plot can be drawn by hand or by a computer. In the past, sometimes mechanical or electronic plotters were used. Graphs are a visual representation of the relationship between variables, which are very useful for humans who can then quickly derive an understanding which may not have come from lists of values. Given a scale or ruler, graphs can also be used to read off the value of an unknown variable plotted as a function of a known one, but this can also be done with data presented in tabular form. Graphs of functions are used in mathematics, sciences, engineering, technology, finance, and other areas.\n\n"
    },
    {
      "id": "76340",
      "title": "Principal component analysis",
      "url": "https://en.wikipedia.org/wiki/Principal_component_analysis",
      "summary": "Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.\nThe data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified.\n\nThe principal components of a collection of points in a real coordinate space are a sequence of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   unit vectors, where the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th vector is the direction of a line that best fits the data while being orthogonal to the first \n  \n    \n      \n        i\n        \u2212\n        1\n      \n    \n    {\\displaystyle i-1}\n   vectors. Here, a best-fitting line is defined as one that minimizes the average squared perpendicular distance from the points to the line. These directions (i.e., principal components) constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Many studies use the first two principal components in order to plot the data in two dimensions and to visually identify clusters of closely related data points.Principal component analysis has applications in many fields such as population genetics, microbiome studies, and atmospheric science."
    },
    {
      "id": "23543",
      "title": "Probability distribution",
      "url": "https://en.wikipedia.org/wiki/Probability_distribution",
      "summary": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).For instance, if X is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). More commonly, probability distributions are used to compare the relative occurrence of many different random values.\nProbability distributions can be defined in different ways and for discrete or for continuous variables. Distributions with special properties or for especially important applications are given specific names."
    },
    {
      "id": "484764",
      "title": "Process theory",
      "url": "https://en.wikipedia.org/wiki/Process_theory",
      "summary": "A process theory is a system of ideas that explains how an entity changes and develops. Process theories are often contrasted with variance theories, that is, systems of ideas that explain the variance in a dependent variable based on one or more independent variables. While process theories focus on how something happens, variance theories focus on why something happens. Examples of process theories include evolution by natural selection, continental drift and the nitrogen cycle."
    },
    {
      "id": "16722927",
      "title": "Propensity score matching",
      "url": "https://en.wikipedia.org/wiki/Propensity_score_matching",
      "summary": "In the statistical analysis of observational data, propensity score matching (PSM) is a statistical matching technique that attempts to estimate the effect of a treatment, policy, or other intervention by accounting for the covariates that predict receiving the treatment. PSM attempts to reduce the bias due to confounding variables that could be found in an estimate of the treatment effect obtained from simply comparing outcomes among units that received the treatment versus those that did not. Paul R. Rosenbaum and Donald Rubin introduced the technique in 1983.The possibility of bias arises because a difference in the treatment outcome (such as the average treatment effect) between treated and untreated groups may be caused by a factor that predicts treatment rather than the treatment itself. In randomized experiments, the randomization enables unbiased estimation of treatment effects; for each covariate, randomization implies that treatment-groups will be balanced on average, by the law of large numbers. Unfortunately, for observational studies, the assignment of treatments to research subjects is typically not random. Matching attempts to reduce the treatment assignment bias, and mimic randomization, by creating a sample of units that received the treatment that is comparable on all observed covariates to a sample of units that did not receive the treatment. \nThe \"propensity\" describes how likely a unit is to have been treated, given its covariate values. The stronger the confounding of treatment and covariates, and hence the stronger the bias in the analysis of the naive treatment effect, the better the covariates predict whether a unit is treated or not. By having units with similar propensity scores in both treatment and control, such confounding is reduced.\nFor example, one may be interested to know the consequences of smoking. An observational study is required since it is unethical to randomly assign people to the treatment 'smoking.' The treatment effect estimated by simply comparing those who smoked to those who did not smoke would be biased by any factors that predict smoking (e.g.: gender and age). PSM attempts to control for these biases by making the groups receiving treatment and not-treatment comparable with respect to the control variables.\n\n"
    },
    {
      "id": "389564",
      "title": "Quantitative research",
      "url": "https://en.wikipedia.org/wiki/Quantitative_research",
      "summary": "Quantitative research is a research strategy that focuses on quantifying the collection and analysis of data. It is formed from a deductive approach where emphasis is placed on the testing of theory, shaped by empiricist and positivist philosophies.Associated with the natural, applied, formal, and social sciences this research strategy promotes the objective empirical investigation of observable phenomena to test and understand relationships. This is done through a range of quantifying methods and techniques, reflecting on its broad utilization as a research strategy across differing academic disciplines.There are several situations where quantitative research may not be the most appropriate or effective method to use:\n1. When exploring in-depth or complex topics.\n2. When studying subjective experiences and personal opinions.\n3. When conducting exploratory research.\n4. When studying sensitive or controversial topics\nThe objective of quantitative research is to develop and employ mathematical models, theories, and hypotheses pertaining to phenomena. The process of measurement is central to quantitative research because it provides the fundamental connection between empirical observation and mathematical expression of quantitative relationships.\nQuantitative data is any data that is in numerical form such as statistics, percentages, etc. The researcher analyses the data with the help of statistics and hopes the numbers will yield an unbiased result that can be generalized to some larger population. Qualitative research, on the other hand, inquires deeply into specific experiences, with the intention of describing and exploring meaning through text, narrative, or visual-based data, by developing themes exclusive to that set of participants.Quantitative research is widely used in psychology, economics, demography, sociology, marketing, community health, health & human development, gender studies, and political science; and less frequently in anthropology and history. Research in mathematical sciences, such as physics, is also \"quantitative\" by definition, though this use of the term differs in context. In the social sciences, the term relates to empirical methods originating in both philosophical positivism and the history of statistics, in contrast with qualitative research methods.\nQualitative research produces information only on the particular cases studied, and any more general conclusions are only hypotheses. Quantitative methods can be used to verify which of such hypotheses are true. A comprehensive analysis of 1274 articles published in the top two American sociology journals between 1935 and 2005 found that roughly two-thirds of these articles used quantitative method.\n\n"
    },
    {
      "id": "65462",
      "title": "Randomization",
      "url": "https://en.wikipedia.org/wiki/Randomization",
      "summary": "Randomization is a statistical process in which a random mechanism is employed to select a sample from a population or assign subjects to different groups. The process is crucial in ensuring the random allocation of experimental units or treatment protocols, thereby minimizing selection bias and enhancing the statistical validity. It facilitates the objective comparison of treatment effects in experimental design, as it equates groups statistically by balancing both known and unknown factors at the outset of the study. In statistical terms, it underpins the principle of probabilistic equivalence among groups, allowing for the unbiased estimation of treatment effects and the generalizability of conclusions drawn from sample data to the broader population.Randomization is not haphazard; instead, a random process is a sequence of random variables describing a process whose outcomes do not follow a deterministic pattern but follow an evolution described by probability distributions. For example, a random sample of individuals from a population refers to a sample where every individual has a known probability of being sampled. This would be contrasted with nonprobability sampling, where arbitrary individuals are selected. A runs test can be used to determine whether the occurrence of a set of measured values is random. Randomization is widely applied in various fields, especially in scientific research, statistical analysis, and resource allocation, to ensure fairness and validity in the outcomes.In various contexts, randomization may involve\n\nGenerating Random Permutations: This is essential in various situations, such as shuffling cards. By randomly rearranging the sequence, it ensures fairness and unpredictability in games and experiments.\nSelecting Random Samples from Populations: In statistical sampling, this method is vital for obtaining representative samples. By randomly choosing a subset of individuals, biases are minimized, ensuring that the sample accurately reflects the larger population.\nRandom Allocation in Experimental Design: Random assignment of experimental units to treatment or control conditions is fundamental in scientific studies. This approach ensures that each unit has an equal chance of receiving any treatment, thereby reducing systematic bias and improving the reliability of experimental results.\nGenerating Random Numbers: The process of random number generation is central to simulations, cryptographic applications, and statistical analysis. These numbers form the basis for simulations, model testing, and secure data encryption.\nData Stream Transformation: In telecommunications, randomization is used to transform data streams. Techniques like scramblers randomize the data to prevent predictable patterns, which is crucial for securing communication channels and enhancing transmission reliability.\"Randomization has many uses in gambling, political use, statistical analysis, art, cryptography, gaming and other fields."
    },
    {
      "id": "6895400",
      "title": "Raw data",
      "url": "https://en.wikipedia.org/wiki/Raw_data",
      "summary": "Raw data, also known as primary data, are data (e.g., numbers, instrument readings, figures, etc.) collected from a source. In the context of examinations, the raw data might be described as a raw score (after test scores).\nIf a scientist sets up a computerized thermometer which records the temperature of a chemical mixture in a test tube every minute, the list of temperature readings for every minute, as printed out on a spreadsheet or viewed on a computer screen are \"raw data\".  Raw data have not been subjected to processing, \"cleaning\" by researchers to remove outliers, obvious instrument reading errors or data entry errors, or any analysis (e.g., determining central tendency aspects such as the average or median result). As well, raw data have not been subject to any other manipulation by a software program or a human researcher, analyst or technician. They are also referred to as primary data. Raw data is a relative term (see data), because even once raw data have been \"cleaned\" and processed by one team of researchers, another team may consider these processed data to be \"raw data\" for another stage of research.  Raw data can be inputted to a computer program or used in manual procedures such as analyzing statistics from a survey. The term \"raw data\" can refer to the binary data on electronic storage devices, such as hard disk drives (also referred to as \"low-level data\").\n\n"
    },
    {
      "id": "826997",
      "title": "Regression analysis",
      "url": "https://en.wikipedia.org/wiki/Regression_analysis",
      "summary": "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\nRegression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data."
    },
    {
      "id": "239138",
      "title": "Reliability (statistics)",
      "url": "https://en.wikipedia.org/wiki/Reliability_(statistics)",
      "summary": "In statistics and psychometrics, reliability is the overall consistency of a measure. A measure is said to have a high reliability if it produces similar results under consistent conditions:\"It is the characteristic of a set of test scores that relates to the amount of random error from the measurement process that might be embedded in the scores. Scores that are highly reliable are precise, reproducible, and consistent from one testing occasion to another. That is, if the testing process were repeated with a group of test takers, essentially the same results would be obtained. Various kinds of reliability coefficients, with values ranging between 0.00 (much error) and 1.00 (no error), are usually used to indicate the amount of error in the scores.\" For example, measurements of people's height and weight are often extremely reliable.\n\n"
    },
    {
      "id": "29915274",
      "title": "Residual bit error rate",
      "url": "https://en.wikipedia.org/wiki/Residual_bit_error_rate",
      "summary": "The residual bit error rate (RBER) is a receive quality metric in digital transmission, one of several used to quantify the accuracy of the received data."
    },
    {
      "id": "3095637",
      "title": "Response rate (survey)",
      "url": "https://en.wikipedia.org/wiki/Response_rate_(survey)",
      "summary": "In survey research, response rate, also known as completion rate or return rate, is the number of people who answered the survey divided by the number of people in the sample. It is usually expressed in the form of a percentage.  The term is also used in direct marketing to refer to the number of people who responded to an offer.\nThe general consensus in academic surveys is to choose one of the six definitions summarized by the American Association for Public Opinion Research (AAPOR). These definitions are endorsed by the National Research Council and the Journal of the American Medical Association, among other well recognized institutions. They are:\n\nResponse Rate 1 (RR1) \u2013 or the minimum response rate, is the number of complete interviews divided by the number of interviews (complete plus partial) plus the number of non-interviews (refusal and break-off plus non-contacts plus others) plus all cases of unknown eligibility (unknown if housing unit, plus unknown, other).\nResponse Rate 2 (RR2) \u2013 RR1 + counting partial interviews as respondents.\nResponse Rate 3 (RR3) \u2013 estimates what proportion of cases of unknown eligibility is actually eligible. Those respondents estimated to be ineligible are excluded from the denominator. The method of estimation *must* be explicitly stated with RR3.\nResponse Rate 4 (RR4) \u2013 allocates cases of unknown eligibility as in RR3, but also includes partial interviews as respondents as in RR2.\nResponse Rate 5 (RR5) \u2013 is either a special case of RR3 in that it assumes that there are no eligible cases among the cases of unknown eligibility or the rare case in which there are no cases of unknown eligibility. RR5 is only appropriate when it is valid to assume that none of the unknown cases are eligible ones, or when there are no unknown cases.\nResponse Rate 6 (RR6) \u2013 makes that same assumption as RR5 and also includes partial interviews as respondents. RR6 represents the maximum response rate.The six AAPOR definitions vary with respect to whether or not the surveys are partially or entirely completed and how researchers deal with unknown nonrespondents. Definition #1, for example, does NOT include partially completed surveys in the numerator, while definition #2 does. Definitions 3\u20136 deal with the unknown eligibility of potential respondents who could not be contacted. For example, there is no answer at the doors of 10 houses you attempted to survey. Maybe 5 of those you already know house people who qualify for your survey based on neighbors telling you whom lived there, but the other 5 are completely unknown. Maybe the dwellers fit your target population, maybe they don't. This may or may not be considered in your response rate, depending on which definition you use.\nExample: if 1,000 surveys were sent by mail, and 257 were successfully completed (entirely) and returned, then the response rate would be 25.7%."
    },
    {
      "id": "20733547",
      "title": "Richard Veryard",
      "url": "https://en.wikipedia.org/wiki/Richard_Veryard",
      "summary": "Richard Veryard FRSA (born 1955) is a British computer scientist, author and business consultant, known for his work on service-oriented architecture and the service-based business.\n\n"
    },
    {
      "id": "21511800",
      "title": "Richards Heuer",
      "url": "https://en.wikipedia.org/wiki/Richards_Heuer",
      "summary": "Richards \"Dick\" J. Heuer, Jr. (July 15, 1927 \u2013 August 21, 2018) was a CIA veteran of 45 years and most known for his work on analysis of competing hypotheses and his book, Psychology of Intelligence Analysis. The former provides a methodology for overcoming intelligence biases while the latter outlines how mental models and natural biases impede clear thinking and analysis. Throughout his career, he worked in collection operations, counterintelligence, intelligence analysis and personnel security. In 2010 he co-authored a book with Randolph (Randy) H. Pherson titled Structured Analytic Techniques for Intelligence Analysis."
    },
    {
      "id": "9461390",
      "title": "Riemann solver",
      "url": "https://en.wikipedia.org/wiki/Riemann_solver",
      "summary": "A Riemann solver is a numerical method used to solve a Riemann problem. They are heavily used in computational fluid dynamics and computational magnetohydrodynamics.\n\n"
    },
    {
      "id": "23814012",
      "title": "Robert D. Richtmyer",
      "url": "https://en.wikipedia.org/wiki/Robert_D._Richtmyer",
      "summary": "Robert Davis Richtmyer (October 10, 1910 \u2013 September 24, 2003) was an American physicist, mathematician, educator, author, and musician."
    },
    {
      "id": "6392749",
      "title": "Run chart",
      "url": "https://en.wikipedia.org/wiki/Run_chart",
      "summary": "A run chart, also known as a run-sequence plot is a graph that displays observed data in a time sequence.  Often, the data displayed represent some aspect of the output or performance of a manufacturing or other business process. It is therefore a form of line chart.\n\n"
    },
    {
      "id": "620083",
      "title": "Sensitivity analysis",
      "url": "https://en.wikipedia.org/wiki/Sensitivity_analysis",
      "summary": "Sensitivity analysis is the study of how the uncertainty in the output of a mathematical model or system (numerical or otherwise) can be divided and allocated to different sources of uncertainty in its inputs. A related practice is uncertainty analysis, which has a greater focus on uncertainty quantification and propagation of uncertainty; ideally, uncertainty and sensitivity analysis should be run in tandem.\nThe process of recalculating outcomes under alternative assumptions to determine the impact of a variable under sensitivity analysis can be useful for a range of purposes, including:\n\nTesting the robustness of the results of a model or system in the presence of uncertainty.\nIncreased understanding of the relationships between input and output variables in a system or model.\nUncertainty reduction, through the identification of model input that cause significant uncertainty in the output and should therefore be the focus of attention in order to increase robustness (perhaps by further research).\nSearching for errors in the model (by encountering unexpected relationships between inputs and outputs).\nModel simplification \u2013 fixing model input that has no effect on the output, or identifying and removing redundant parts of the model structure.\nEnhancing communication from modelers to decision makers (e.g. by making recommendations more credible, understandable, compelling or persuasive).\nFinding regions in the space of input factors for which the model output is either maximum or minimum or meets some optimum criterion (see optimization and Monte Carlo filtering).\nIn case of calibrating models with large number of parameters, a primary sensitivity test can ease the calibration stage by focusing on the sensitive parameters. Not knowing the sensitivity of parameters can result in time being uselessly spent on non-sensitive ones.\nTo seek to identify important connections between observations, model inputs, and predictions or forecasts, leading to the development of better models.\n\n"
    },
    {
      "id": "5665800",
      "title": "Sergei Godunov",
      "url": "https://en.wikipedia.org/wiki/Sergei_Godunov",
      "summary": "Sergei Konstantinovich Godunov (Russian: \u0421\u0435\u0440\u0433\u0435\u0301\u0439 \u041a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u0438\u0301\u043d\u043e\u0432\u0438\u0447 \u0413\u043e\u0434\u0443\u043d\u043e\u0301\u0432; 17 July 1929 \u2013 15 July 2023) was a Soviet and Russian professor at the Sobolev Institute of Mathematics of the Russian Academy of Sciences in Novosibirsk, Russia.\n\n"
    },
    {
      "id": "67353201",
      "title": "Shirley Wu",
      "url": "https://en.wikipedia.org/wiki/Shirley_Wu",
      "summary": "Shirley Wu is a data scientist specialized in data art and data visualizations. She is a freelancer based out of San Francisco, California. With Nadieh Bremer, Wu is the author of Data Sketches.\n\n"
    },
    {
      "id": "531362",
      "title": "Small multiple",
      "url": "https://en.wikipedia.org/wiki/Small_multiple",
      "summary": "A small multiple (sometimes called trellis chart, lattice chart, grid chart, or panel chart) is a series of similar graphs or charts using the same scale and axes, allowing them to be easily compared. It uses multiple views to show different partitions of a dataset. The term was popularized by Edward Tufte.\nAccording to Tufte,\n\nAt the heart of quantitative reasoning is a single question: Compared to what? Small multiple designs, multivariate and data bountiful, answer directly by visually enforcing comparisons of changes, of the differences among objects, of the scope of alternatives. For a wide range of problems in data presentation, small multiples are the best design solution."
    },
    {
      "id": "1266110",
      "title": "Smoothed-particle hydrodynamics",
      "url": "https://en.wikipedia.org/wiki/Smoothed-particle_hydrodynamics",
      "summary": "Smoothed-particle hydrodynamics (SPH) is a computational method used for simulating the mechanics of continuum media, such as solid mechanics and fluid flows. It was developed by Gingold and Monaghan and Lucy in 1977, initially for astrophysical problems.  It has been used in many fields of research, including astrophysics, ballistics, volcanology, and oceanography. It is a meshfree Lagrangian method (where the co-ordinates move with the fluid), and the resolution of the method can easily be adjusted with respect to variables such as density."
    },
    {
      "id": "1743077",
      "title": "Sparkline",
      "url": "https://en.wikipedia.org/wiki/Sparkline",
      "summary": "A sparkline is a very small line chart, typically drawn without axes or coordinates. It presents the general shape of a variation (typically over time) in some measurement, such as temperature or stock market price, in a simple and highly condensed way. Whereas a typical chart is designed to professionally show as much data as possible, and is set off from the flow of text, sparklines are intended to be succinct, memorable, and located where they are discussed. Sparklines are small enough to be embedded in text, or several sparklines may be grouped together as elements of a small multiple.  \n\n"
    },
    {
      "id": "15934463",
      "title": "Statistical graphics",
      "url": "https://en.wikipedia.org/wiki/Statistical_graphics",
      "summary": "Statistical graphics, also known as statistical graphical techniques, are graphics used in the field of statistics for data visualization.\n\n"
    },
    {
      "id": "27576",
      "title": "Statistical model",
      "url": "https://en.wikipedia.org/wiki/Statistical_model",
      "summary": "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process. When referring specifically to probabilities, the corresponding term is probabilistic model.\nA statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables.  As such, a statistical model is \"a formal representation of a theory\" (Herman Ad\u00e8r quoting Kenneth Bollen).All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.\n\n"
    },
    {
      "id": "26502065",
      "title": "Statistical model validation",
      "url": "https://en.wikipedia.org/wiki/Statistical_model_validation",
      "summary": "In statistics, model validation is the task of evaluating whether a chosen statistical model is appropriate or not. Oftentimes in statistical inference, inferences from models that appear to fit their data may be flukes, resulting in a misunderstanding by researchers of the actual relevance of their model. To combat this, model validation is used to test whether a statistical model can hold up to permutations in the data. This topic is not to be confused with the closely related task of model selection, the process of discriminating between multiple candidate models: model validation does not concern so much the conceptual design of models as it tests only the consistency between a chosen model and its stated outputs.\nThere are many ways to validate a model. Residual plots plot the difference between the actual data and the model's predictions: correlations in the residual plots may indicate a flaw in the model. Cross validation is a method of model validation that iteratively refits the model, each time leaving out just a small sample and comparing whether the samples left out are predicted by the model: there are many kinds of cross validation. Predictive simulation is used to compare simulated data to actual data. External validation involves fitting the model to new data. Akaike information criterion estimates the quality of a model.\n\n"
    },
    {
      "id": "27580",
      "title": "Statistical unit",
      "url": "https://en.wikipedia.org/wiki/Statistical_unit",
      "summary": "In statistics, a unit is one member of a set of entities being studied. It is the main source for the mathematical abstraction of a \"random variable\". Common examples of a unit would be a single person, animal, plant, manufactured item, or country that belongs to a larger collection of such entities being studied.\n\n"
    },
    {
      "id": "977649",
      "title": "Stem-and-leaf display",
      "url": "https://en.wikipedia.org/wiki/Stem-and-leaf_display",
      "summary": "A stem-and-leaf display or stem-and-leaf plot is a device for  presenting quantitative data in a graphical format, similar to a histogram, to assist in visualizing the shape of a distribution. They evolved from Arthur Bowley's work in the early 1900s, and are useful tools in exploratory data analysis. Stemplots became more commonly used in the 1980s after the publication of John Tukey's book on exploratory data analysis in 1977. The popularity during those years is attributable to their use of monospaced (typewriter) typestyles that allowed computer technology of the time to easily produce the graphics. Modern computers' superior graphic capabilities have meant these techniques are less often used.\nThis plot has been implemented in Octave and R.A stem-and-leaf plot is also called a stemplot, but the latter term often refers to another chart type.  A simple stem plot may refer to plotting a matrix of y values onto a common x axis, and identifying the common x value with a vertical line, and the individual y values with symbols on the line.Unlike histograms, stem-and-leaf displays retain the original data to at least two significant digits, and put the data in order, thereby easing the move to order-based inference and non-parametric statistics.\n\n"
    },
    {
      "id": "17813659",
      "title": "Structured data analysis (statistics)",
      "url": "https://en.wikipedia.org/wiki/Structured_data_analysis_(statistics)",
      "summary": "Structured data analysis is the statistical data analysis of structured data. This can arise either in the form of an a priori structure such as multiple-choice questionnaires or in situations with the need to search for structure that fits the given data, either exactly or approximately. This structure can then be used for making comparisons, predictions, manipulations etc."
    },
    {
      "id": "6816305",
      "title": "Undertone series",
      "url": "https://en.wikipedia.org/wiki/Undertone_series",
      "summary": "In music, the undertone series or subharmonic series is a sequence of notes that results from inverting the intervals of the overtone series. While overtones naturally occur with the physical production of music on instruments, undertones must be produced in unusual ways. While the overtone series is based upon arithmetic multiplication of frequencies, resulting in a harmonic series, the undertone series is based on arithmetic division.\n\n"
    },
    {
      "id": "552466",
      "title": "System identification",
      "url": "https://en.wikipedia.org/wiki/System_identification",
      "summary": "The field of system identification uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models as well as model reduction. A common approach is to start from measurements of the behavior of the system and the external influences (inputs to the system) and try to determine a mathematical relation between them without going into many details of what is actually happening inside the system; this approach is called black box system identification.\n\n"
    },
    {
      "id": "337862",
      "title": "Table (information)",
      "url": "https://en.wikipedia.org/wiki/Table_(information)",
      "summary": "A table is an arrangement of information or data, typically in rows and columns, or possibly in a more complex structure. Tables are widely used in communication, research, and data analysis. Tables appear in print media, handwritten notes, computer software, architectural ornamentation, traffic signs, and many other places. The precise conventions and terminology for describing tables vary depending on the context. Further, tables differ significantly in variety, structure, flexibility, notation, representation and use. Information or data conveyed in table form is said to be in tabular format (adjective). In books and technical articles, tables are typically presented apart from the main text in numbered and captioned floating blocks.\n\n"
    },
    {
      "id": "47389113",
      "title": "Tamara Munzner",
      "url": "https://en.wikipedia.org/wiki/Tamara_Munzner",
      "summary": "Tamara Macushla Munzner (born 1969) is an American-Canadian scientist. She is an expert in information visualization who works as a professor of computer science at the University of British Columbia (UBC).\n\n"
    },
    {
      "id": "10559845",
      "title": "Test method",
      "url": "https://en.wikipedia.org/wiki/Test_method",
      "summary": "A test method is a method for a test in science or engineering, such as a physical test, chemical test, or statistical test. It is a definitive procedure that produces a test result. In order to ensure accurate and relevant test results, a test method should be \"explicit, unambiguous, and experimentally feasible.\", as well as effective and reproducible.A test can be considered an observation or experiment that determines one or more characteristics of a given sample, product, process, or service. The purpose of testing involves a prior determination of expected observation and a comparison of that expectation to what one actually observes. The results of testing can be qualitative (yes/no), quantitative (a measured value), or categorical and can be derived from personal observation or the output of a precision measuring instrument.\nUsually the test result is the dependent variable, the measured response based on the particular conditions of the test or the level of the independent variable. Some tests, however, may involve changing the independent variable to determine the level at which a certain response occurs: in this case, the test result is the independent variable.\n\n"
    },
    {
      "id": "473317",
      "title": "Content analysis",
      "url": "https://en.wikipedia.org/wiki/Content_analysis",
      "summary": "Content analysis is the study of documents and communication artifacts, which might be texts of various formats, pictures, audio or video. Social scientists use content analysis to examine patterns in communication in a replicable and systematic manner. One of the key advantages of using content analysis to analyse social phenomena is their non-invasive nature, in contrast to simulating social experiences or collecting survey answers.\nPractices and philosophies of content analysis vary between academic disciplines. They all involve systematic reading or observation of texts or artifacts which are assigned labels (sometimes called codes) to indicate the presence of interesting, meaningful pieces of content. By systematically labeling the content of a set of texts, researchers can analyse patterns of content quantitatively using statistical methods, or use qualitative methods to analyse meanings of content within texts.\nComputers are increasingly used in content analysis to automate the labeling (or coding) of documents. Simple computational techniques can provide descriptive data such as word frequencies and document lengths. Machine learning classifiers can greatly increase the number of texts that can be labeled, but the scientific utility of doing so is a matter of debate. Further, numerous computer-aided text analysis (CATA) computer programs are available that analyze text for pre-determined linguistic, semantic, and psychological characteristics.\n\n"
    },
    {
      "id": "318439",
      "title": "Text mining",
      "url": "https://en.wikipedia.org/wiki/Text_mining",
      "summary": "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\nText analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via the application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.\nA typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted. The document is the basic element when starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.\n\n"
    },
    {
      "id": "1592887",
      "title": "Treemapping",
      "url": "https://en.wikipedia.org/wiki/Treemapping",
      "summary": "In information visualization and computing, treemapping is a method for displaying hierarchical data using nested figures, usually rectangles.\nTreemaps display hierarchical (tree-structured) data as a set of nested rectangles. Each branch of the tree is given a rectangle, which is then tiled with smaller rectangles representing sub-branches. A leaf node's rectangle has an area proportional to a specified dimension of the data. Often the leaf nodes are colored to show a separate dimension of the data.\nWhen the color and size dimensions are correlated in some way with the tree structure, one can often easily see patterns that would be difficult to spot in other ways, such as whether a certain color is particularly relevant. A second advantage of treemaps is that, by construction, they make efficient use of space. As a result, they can legibly display thousands of items on the screen simultaneously.\n\n"
    },
    {
      "id": "3087410",
      "title": "Turbulence modeling",
      "url": "https://en.wikipedia.org/wiki/Turbulence_modeling",
      "summary": "In fluid dynamics, turbulence modeling is the construction and use of a mathematical model to predict the effects of turbulence. Turbulent flows are commonplace in most real-life scenarios. In spite of decades of research, there is no analytical theory to predict the evolution of these turbulent flows. The equations governing turbulent flows can only be solved directly for simple cases of flow. For most real-life turbulent flows, CFD simulations use turbulent models to predict the evolution of turbulence. These turbulence models are simplified constitutive equations that predict the statistical evolution of turbulent flows."
    },
    {
      "id": "5657877",
      "title": "Type I and type II errors",
      "url": "https://en.wikipedia.org/wiki/Type_I_and_type_II_errors",
      "summary": "In statistical hypothesis testing, a type I error is the mistaken rejection of a null hypothesis that is actually true.  A type I error is also known as a \"false positive\" finding or conclusion; example: \"an innocent person is convicted\".  A type II error is the failure to reject a null hypothesis that is actually false.  A type II error is also known as a \"false negative\" finding or conclusion; example: \"a guilty person is not convicted\". Much of statistical theory revolves around the minimization of one or both of these errors, though the complete elimination of either is a statistical impossibility if the outcome is not determined by a known, observable causal process.\nBy selecting a low threshold (cut-off) value and modifying the alpha (\u03b1) level, the quality of the hypothesis test can be increased. The knowledge of type I errors and type II errors is widely used in medical science, biometrics and computer science.Intuitively, type I errors can be thought of as errors of commission (i.e., the researcher unluckily concludes that something is the fact). For instance, consider a study where researchers compare a drug with a placebo. If the patients who are given the drug get better than the patients given the placebo by chance, it may appear that the drug is effective, but in fact the conclusion is incorrect.\nIn reverse, type II errors are errors of omission.  In the example above, if the patients who got the drug did not get better at a higher rate than the ones who got the placebo, but this was a random fluke, that would be a type II error. The consequence of a type II error depends on the size and direction of the missed determination and the circumstances. An expensive cure for one in a million patients may be inconsequential even if it truly is a cure.\n\n"
    },
    {
      "id": "11526222",
      "title": "United Nations Sustainable Development Group",
      "url": "https://en.wikipedia.org/wiki/United_Nations_Sustainable_Development_Group",
      "summary": "The United Nations Sustainable Development Group (UNSDG), previously the United Nations Development Group (UNDG), is a consortium of 36 United Nations funds, programmes, specialized agencies, departments and offices that play a role in development. It was created by the Secretary-General of the United Nations in order to improve the effectiveness of United Nations development activities at the country level.\nIts strategic priorities are to respond to the Triennial comprehensive policy review (TCPR) \u2013 which became in 2008 the Quadrennial comprehensive policy review (QCPR) \u2013 and global development priorities, as well as to ensure the UN development system becomes more internally focused and coherent. The UNSDG strategic priorities give direction to UNSDG members' efforts at the global, regional and country level to facilitate a step change in the quality and impact of UN support at the country level. The UNSDG (at the time the UNDG) was one of the main UN actors involved in the development of the Post-2015 Development Agenda which lead to the creation of the Sustainable Development Goals. The UNDG was renamed as the UNSDG around January 2018.\n\n"
    },
    {
      "id": "21280496",
      "title": "Visual perception",
      "url": "https://en.wikipedia.org/wiki/Visual_perception",
      "summary": "Visual perception is the ability to interpret the surrounding environment through photopic vision (daytime vision), color vision, scotopic vision (night vision), and mesopic vision (twilight vision), using light in the visible spectrum reflected by objects in the environment. This is different from visual acuity, which refers to how clearly a person sees (for example \"20/20 vision\"). A person can have problems with visual perceptual processing even if they have 20/20 vision.\nThe resulting perception is also known as vision, sight, or eyesight (adjectives visual, optical, and ocular, respectively). The various physiological components involved in vision are referred to collectively as the visual system, and are the focus of much research in linguistics, psychology, cognitive science, neuroscience, and molecular biology, collectively referred to as vision science."
    },
    {
      "id": "61159333",
      "title": "Warming stripes",
      "url": "https://en.wikipedia.org/wiki/Warming_stripes",
      "summary": "Warming stripes (sometimes referred to as climate stripes, climate timelines or stripe graphics) are data visualization graphics that use a series of coloured stripes chronologically ordered to visually portray long-term temperature trends. Warming stripes reflect a \"minimalist\" style, conceived to use colour alone to avoid technical distractions to intuitively convey global warming trends to non-scientists.The initial concept of visualizing historical temperature data has been extended to involve animation, to visualize sea level rise and predictive climate data, and to visually juxtapose temperature trends with other data such as atmospheric CO2 concentration, global glacier retreat, precipitation, progression of ocean depths, aviation emission's percentage contribution to global warming, and biodiversity loss. In less technical contexts, the graphics have been embraced by climate activists, used as cover images of books and magazines, used in fashion design, projected onto natural landmarks, and used on athletic team uniforms, music festival stages, and public infrastructure.\n\n"
    },
    {
      "id": "50903",
      "title": "Wavelet",
      "url": "https://en.wikipedia.org/wiki/Wavelet",
      "summary": "A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a \"brief oscillation\". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.\n\nFor example, a wavelet could be created to have a frequency of Middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the Middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.\nAs a mathematical tool, wavelets can be used to extract information from many kinds of data, including audio signals and images. Sets of wavelets are needed to analyze data fully. \"Complementary\" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet-based compression/decompression algorithms, where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square-integrable functions. This is accomplished through coherent states.\nIn classical physics, the diffraction phenomenon is described by the Huygens\u2013Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity."
    },
    {
      "id": "502806",
      "title": "Yukawa potential",
      "url": "https://en.wikipedia.org/wiki/Yukawa_potential",
      "summary": "In particle, atomic and condensed matter physics, a Yukawa potential (also called a screened Coulomb potential) is a potential named after the Japanese physicist Hideki Yukawa. The potential is of the form:\n\n  \n    \n      \n        \n          V\n          \n            Yukawa\n          \n        \n        (\n        r\n        )\n        =\n        \u2212\n        \n          g\n          \n            2\n          \n        \n        \n          \n            \n              e\n              \n                \u2212\n                \u03b1\n                m\n                r\n              \n            \n            r\n          \n        \n        ,\n      \n    \n    {\\displaystyle V_{\\text{Yukawa}}(r)=-g^{2}{\\frac {e^{-\\alpha mr}}{r}},}\n  where \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   is a magnitude scaling constant, i.e. is the amplitude of potential,  m is the mass of the particle, r is the radial distance to the particle, and \u03b1 is another scaling constant, so that \n  \n    \n      \n        r\n        \u2248\n        \n          \n            \n              1\n              \n                \u03b1\n                m\n              \n            \n          \n        \n      \n    \n    {\\displaystyle r\\approx {\\tfrac {1}{\\alpha m}}}\n   is the approximate range. The potential is monotonically increasing in r and it is negative, implying the force is attractive. In the SI system, the unit of the Yukawa potential is (1/meters).\nThe Coulomb potential of electromagnetism is an example of a Yukawa potential with the \n  \n    \n      \n        \n          e\n          \n            \u2212\n            \u03b1\n            m\n            r\n          \n        \n      \n    \n    {\\displaystyle e^{-\\alpha mr}}\n   factor equal to 1, everywhere. This can be interpreted as saying that the photon mass m is equal to 0. The photon is the force-carrier between interacting, charged particles.\nIn interactions between a meson field and a fermion field, the constant \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   is equal to the gauge coupling constant between those fields.  In the case of the nuclear force, the fermions would be a proton and another proton or a neutron.\n\n"
    },
    {
      "id": "454746",
      "title": "Application software",
      "url": "https://en.wikipedia.org/wiki/Application_software",
      "summary": "An application program (software application, or application, or app for short) is a computer program designed to carry out a specific task other than one relating to the operation of the computer itself, typically to be used by end-users. Word processors, media players, and accounting software are examples. The collective noun \"application software\" refers to all applications collectively. The other principal classifications of software are system software, relating to the operation of the computer, and utility software (\"utilities\").\nApplications may be bundled with the computer and its system software or published separately and may be coded as proprietary, open-source, or projects. The term \"app\" usually refers to applications for mobile devices such as phones.\n\n"
    },
    {
      "id": "3425793",
      "title": "Armstrong's axioms",
      "url": "https://en.wikipedia.org/wiki/Armstrong%27s_axioms",
      "summary": "Armstrong's axioms are a set of references (or, more precisely, inference rules) used to infer all the functional dependencies on a relational database. They were developed by William W. Armstrong in his 1974 paper. The axioms are sound in generating only functional dependencies in the closure of a set of functional dependencies (denoted as  \n  \n    \n      \n        \n          F\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle F^{+}}\n  ) when applied to that set (denoted as \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  ). They are also complete in that repeated application of these rules will generate all functional dependencies in the closure \n  \n    \n      \n        \n          F\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle F^{+}}\n  . \nMore formally, let \n  \n    \n      \n        \u27e8\n        R\n        (\n        U\n        )\n        ,\n        F\n        \u27e9\n      \n    \n    {\\displaystyle \\langle R(U),F\\rangle }\n   denote a relational scheme over the set of attributes \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n   with a set of functional dependencies \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  . We say that a functional dependency \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is logically implied by \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  , and denote it with \n  \n    \n      \n        F\n        \u22a8\n        f\n      \n    \n    {\\displaystyle F\\models f}\n   if and only if for every instance \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   of \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n   that satisfies the functional dependencies in \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  , \n  \n    \n      \n        r\n      \n    \n    {\\displaystyle r}\n   also satisfies \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  . We denote by \n  \n    \n      \n        \n          F\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle F^{+}}\n   the set of all functional dependencies that are logically implied by \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  .\nFurthermore, with respect to a set of inference rules \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , we say that a functional dependency \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is derivable from the functional dependencies in \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   by the set of inference rules \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  , and we denote it by \n  \n    \n      \n        F\n        \n          \u22a2\n          \n            A\n          \n        \n        f\n      \n    \n    {\\displaystyle F\\vdash _{A}f}\n   if and only if \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   is obtainable by means of repeatedly applying the inference rules in \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   to functional dependencies in \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  . We denote by \n  \n    \n      \n        \n          F\n          \n            A\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle F_{A}^{*}}\n   the set of all functional dependencies that are derivable from \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n   by inference rules in \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  .\nThen, a set of inference rules \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is sound if and only if the following holds:\n\n  \n    \n      \n        \n          F\n          \n            A\n          \n          \n            \u2217\n          \n        \n        \u2286\n        \n          F\n          \n            +\n          \n        \n      \n    \n    {\\displaystyle F_{A}^{*}\\subseteq F^{+}}\n  \nthat is to say, we cannot derive by means of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   functional dependencies that are not logically implied by \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  .\nThe set of inference rules \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is said to be complete if the following holds:\n\n  \n    \n      \n        \n          F\n          \n            +\n          \n        \n        \u2286\n        \n          F\n          \n            A\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle F^{+}\\subseteq F_{A}^{*}}\n  \nmore simply put, we are able to derive by \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   all the functional dependencies that are logically implied by \n  \n    \n      \n        F\n      \n    \n    {\\displaystyle F}\n  ."
    },
    {
      "id": "63325563",
      "title": "Blockchain-based database",
      "url": "https://en.wikipedia.org/wiki/Blockchain-based_database",
      "summary": "The blockchain-based database is a combination of traditional database and distributed database where data is transacted and recorded via Database Interface (also known as Compute Interface) supported by multiple-layers of blockchains. The database itself is shared in the form of an encrypted/immutable ledger which makes the information open for everyone."
    },
    {
      "id": "336975",
      "title": "Candidate key",
      "url": "https://en.wikipedia.org/wiki/Candidate_key",
      "summary": "A candidate key, or simply a key, of a relational database is any set of columns that have a unique combination of values in each row, with the additional constraint that removing any column could produce duplicate combinations of values. \nA candidate key is a minimal superkey, \ni.e., a superkey that doesn't contain a smaller one. Therefore, a relation can have multiple candidate keys, each with a different number of attributes.Specific candidate keys are sometimes called primary keys, secondary keys or alternate keys.\nThe columns in a candidate key are called prime attributes, and a column that does not occur in any candidate key is called a non-prime attribute.\nEvery relation without NULL values will have at least one candidate key: Since there cannot be duplicate rows, the set of all columns is a superkey, and if that isn't minimal, some subset of that will be minimal.\nThere is a functional dependency from the candidate key to all the attributes in the relation.\nThe superkeys of a relation are all the possible ways we can identify a row. The candidate keys are the minimal subsets of each superkey and as such, they are an important concept for the design of database schema.\n\n"
    },
    {
      "id": "3074936",
      "title": "Capacity planning",
      "url": "https://en.wikipedia.org/wiki/Capacity_planning",
      "summary": "Capacity planning is the process of determining the production capacity needed by an organization to meet changing demands for its products. In the context of capacity planning, design capacity is the maximum amount of work that an organization or individual is capable of completing in a given period. Effective capacity is the maximum amount of work that an organization or individual is capable of completing in a given period due to constraints such as quality problems, delays, material handling, etc. \nThe phrase is also used in business computing and information technology as a synonym for capacity management. IT capacity planning involves estimating the storage, computer hardware, software and connection infrastructure resources required over some future period of time. A common concern of enterprises is whether the required resources are in place to handle an increase in users or number of interactions. Capacity management is concerned about  adding central processing units (CPUs), memory and storage to a physical or virtual server. This has been the traditional and vertical way of scaling up web applications, however IT capacity planning has been developed with the goal of forecasting the requirements for this vertical scaling approach.A discrepancy between the capacity of an organization and the demands of its customers results in inefficiency, either in under-utilized resources or unfulfilled customer demand. The goal of capacity planning is to minimize this discrepancy. Demand for an organization's capacity varies based on changes in production output, such as increasing or decreasing the production quantity of an existing product, or producing new products. Better utilization of existing capacity can be accomplished through improvements in overall equipment effectiveness (OEE). Capacity can be increased through introducing new techniques, equipment and materials, increasing the number of workers or machines, increasing the number of shifts, or acquiring additional production facilities.\nCapacity is calculated as (number of machines or workers) \u00d7 (number of shifts) \u00d7 (utilization) \u00d7 (efficiency).\n\n"
    },
    {
      "id": "8237163",
      "title": "Cardinality (data modeling)",
      "url": "https://en.wikipedia.org/wiki/Cardinality_(data_modeling)",
      "summary": "Within data modelling, cardinality is the numerical relationship between rows of one table and rows in another. Common cardinalities include one-to-one, one-to-many, and many-to-many. Cardinality can be used to define data models as well as analyze entities within datasets."
    },
    {
      "id": "33354721",
      "title": "Cloud database",
      "url": "https://en.wikipedia.org/wiki/Cloud_database",
      "summary": "A cloud database is a database that typically runs on a cloud computing platform and access to the database is provided as-a-service. There are two common deployment models: users can run databases on the cloud independently, using a virtual machine image, or they can purchase access to a database service, maintained by a cloud database provider. Of the databases available on the cloud, some are SQL-based and some use a NoSQL data model.\nDatabase services take care of scalability and high availability of the database. Database services make the underlying software-stack transparent to the user.\n\n"
    },
    {
      "id": "488211",
      "title": "Codd's 12 rules",
      "url": "https://en.wikipedia.org/wiki/Codd%27s_12_rules",
      "summary": "Codd's twelve rules are a set of thirteen rules (numbered zero to twelve) proposed by Edgar F. Codd, a pioneer of the relational model for databases, designed to define what is required from a database management system in order for it to be considered relational, i.e., a relational database management system (RDBMS). They are sometimes referred to as \"Codd's Twelve Commandments\"."
    },
    {
      "id": "6870132",
      "title": "Column-oriented DBMS",
      "url": "https://en.wikipedia.org/wiki/Column-oriented_DBMS",
      "summary": "A column-oriented DBMS or columnar DBMS is a database management system (DBMS) that stores data tables by column rather than by row. Benefits include more efficient access to data when only querying a subset of columns (by eliminating the need to read columns that are not relevant), and more options for data compression. However, they are typically less efficient for inserting new data.\nPractical use of a column store versus a row store differs little in the relational DBMS world. Both columnar and row databases can use traditional database query languages like SQL to load data and perform queries. Both row and columnar databases can become the backbone in a system to serve data for common extract, transform, load (ETL) and  tools.\n\n"
    },
    {
      "id": "1041167",
      "title": "Column (database)",
      "url": "https://en.wikipedia.org/wiki/Column_(database)",
      "summary": "In a relational database, a column is a set of data values of a particular type, one value for each row of the database. A column may contain text values, numbers, or even pointers to files in the operating system. Columns typically contain simple types, though some relational database systems allow columns to contain more complex data types, such as whole documents, images, or even video clips. A column can also be called an attribute.\nEach row would provide a data value for each column and would then be understood as a single structured data value. For example, a database that represents company contact information might have the following columns: ID, Company Name, Address Line 1, Address Line 2, City, and Postal Code. More formally, a row is a tuple containing a specific value for each column, for example: (1234, 'Big Company Inc.', '123 East Example Street', '456 West Example Drive', 'Big City', 98765).\n\n"
    },
    {
      "id": "14410703",
      "title": "Comparison of database administration tools",
      "url": "https://en.wikipedia.org/wiki/Comparison_of_database_administration_tools",
      "summary": "The following tables compare general and technical information for a number of available database administration tools. Please see individual product articles for further information. This article is neither all-inclusive nor necessarily up to date.\nSystems listed on a light purple background are no longer in active development.\n\n"
    },
    {
      "id": "49259327",
      "title": "Comparison of multi-model databases",
      "url": "https://en.wikipedia.org/wiki/Comparison_of_multi-model_databases",
      "summary": "Comparison of multi-model databases (database management systems)."
    },
    {
      "id": "12912603",
      "title": "Comparison of object database management systems",
      "url": "https://en.wikipedia.org/wiki/Comparison_of_object_database_management_systems",
      "summary": "This is a comparison of notable object database management systems, showing what fundamental object database features are implemented natively."
    },
    {
      "id": "1569036",
      "title": "Comparison of relational database management systems",
      "url": "https://en.wikipedia.org/wiki/Comparison_of_relational_database_management_systems",
      "summary": "The following tables compare general and technical information for a number of relational database management systems. Please see the individual products' articles for further information. Unless otherwise specified in footnotes, comparisons are based on the stable versions without any add-ons, extensions or external programs."
    },
    {
      "id": "217356",
      "title": "Concurrency control",
      "url": "https://en.wikipedia.org/wiki/Concurrency_control",
      "summary": "In information technology and computer science,  especially in the fields of computer programming, operating systems, multiprocessors, and databases, concurrency control ensures that correct results for concurrent operations are generated, while getting those results as quickly as possible.\nComputer systems, both software and hardware, consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in memory or storage), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and theories to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm.\nFor example, a failure in concurrency control can result in data corruption from torn read or write operations.\n\n"
    },
    {
      "id": "287180",
      "title": "Create, read, update and delete",
      "url": "https://en.wikipedia.org/wiki/Create,_read,_update_and_delete",
      "summary": "In computer programming, create, read, update, and delete (CRUD) are the four basic operations of persistent storage. CRUD is also sometimes used to describe user interface conventions that facilitate viewing, searching, and changing information using computer-based forms and reports.\n\n"
    },
    {
      "id": "2104836",
      "title": "Cursor (databases)",
      "url": "https://en.wikipedia.org/wiki/Cursor_(databases)",
      "summary": "In computer science, a database cursor is a mechanism that enables traversal over the records in a database. Cursors facilitate subsequent processing in conjunction with the traversal, such as retrieval, addition and removal of database records. The database cursor characteristic of traversal makes cursors akin to the programming language concept of iterator.\nCursors are used by database programmers to process individual rows returned by database system queries. Cursors enable manipulation of whole result sets at once. In this scenario, a cursor enables the sequential processing of rows in a result set.\nIn SQL procedures, a cursor makes it possible to define a result set (a set of data rows) and perform complex logic on a row by row basis. By using the same mechanics, a SQL procedure can also define a result set and return it directly to the caller of the SQL procedure or to a client application.\nA cursor can be viewed as a pointer to one row in a set of rows. The cursor can only reference one row at a time, but can move to other rows of the result set as needed.\n\n"
    },
    {
      "id": "6968",
      "title": "Customer relationship management",
      "url": "https://en.wikipedia.org/wiki/Customer_relationship_management",
      "summary": "Customer relationship management (CRM) is a process in which a business or other organization administers its interactions with customers, typically using data analysis to study large amounts of information.CRM systems compile data from a range of different communication channels, including a company's website, telephone (which many software come with a softphone), email, live chat, marketing materials and more recently, social media. They allow businesses to learn more about their target audiences and how to better cater to their needs, thus retaining customers and driving sales growth. CRM may be used with past, present or potential customers. The concepts, procedures, and rules that a corporation follows when communicating with its consumers are referred to as CRM. This complete connection covers direct contact with customers, such as sales and service-related operations, forecasting, and the analysis of consumer patterns and behaviors, from the perspective of the company. According to Gartner, the global CRM market size is estimated at $69 billion in 2020.\n\n"
    },
    {
      "id": "452018",
      "title": "Data definition language",
      "url": "https://en.wikipedia.org/wiki/Data_definition_language",
      "summary": "In  the context of SQL, data definition or data description language (DDL) is a syntax for creating and modifying database objects such as tables, indices, and users. DDL statements are similar to a computer programming language for defining data structures, especially database schemas. Common examples of DDL statements include CREATE, ALTER, and DROP."
    },
    {
      "id": "3361242",
      "title": "Palette (computing)",
      "url": "https://en.wikipedia.org/wiki/Palette_(computing)",
      "summary": "In computer graphics, a palette is the set of available colors from which an image can be made.  In some systems, the palette is fixed by the hardware design, and in others it is dynamic, typically implemented via a color lookup table (CLUT), a correspondence table in which selected colors from a certain color space's color reproduction range are assigned an index, by which they can be referenced. By referencing the colors via an index, which takes less information than needed to describe the actual colors in the color space, this technique aims to reduce data usage, including processing, transfer bandwidth, RAM usage, and storage. Images in which colors are indicated by references to a CLUT are called indexed color images.\n\n"
    },
    {
      "id": "911354",
      "title": "Data source name",
      "url": "https://en.wikipedia.org/wiki/Data_source_name",
      "summary": "In computing, a data source name (DSN, sometimes known as a database source name, though \"data sources\" can comprise other repositories apart from databases) is a string that has an associated data structure used to describe a connection to a data source. Most commonly used in connection with ODBC, DSNs also exist for JDBC and for other data access mechanisms.  The term often overlaps with \"connection string\".  Most systems do not make a distinction between DSNs or connection strings and the term can often be used interchangeably.DSN attributes may include, but are not limited to:\nthe name of the data source\nthe location of the data source\nthe name of a database driver which can access the data source\na user ID for data access (if required)\na user password for data access (if required)The system administrator of a client machine generally creates a separate DSN for each relevant data source.\nStandardizing DSNs offers a level of indirection; various applications (for example: Apache/PHP and IIS/ASP) can take advantage of this in accessing shared data sources."
    },
    {
      "id": "645139",
      "title": "Data dictionary",
      "url": "https://en.wikipedia.org/wiki/Data_dictionary",
      "summary": "A data dictionary, or metadata repository, as defined in the IBM Dictionary of Computing, is a \"centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format\". Oracle defines it as a collection of tables with metadata. The term can have one of several closely related meanings pertaining to databases and database management systems (DBMS):\n\nA document describing a database or collection of databases\nAn integral component of a DBMS that is required to determine its structure\nA piece of middleware that extends or supplants the native data dictionary of a DBMS"
    },
    {
      "id": "975347",
      "title": "Data manipulation language",
      "url": "https://en.wikipedia.org/wiki/Data_manipulation_language",
      "summary": "A data manipulation language (DML) is a computer programming language used for adding (inserting), deleting, and modifying (updating) data in a database. A DML is often a sublanguage of a broader database language such as SQL, with the DML comprising some of the operators in the language. Read-only selecting of data is sometimes distinguished as being part of a separate data query language (DQL), but it is closely related and sometimes also considered a component of a DML; some operators may perform both selecting (reading) and writing.\nA popular data manipulation language is that of Structured Query Language (SQL), which is used to retrieve and manipulate data in a relational database. Other forms of DML are those used by IMS/DLI, CODASYL databases, such as IDMS and others."
    },
    {
      "id": "13783336",
      "title": "Database-centric architecture",
      "url": "https://en.wikipedia.org/wiki/Database-centric_architecture",
      "summary": "Database-centric Architecture or data-centric architecture has several distinct meanings, generally relating to software architectures in which databases play a crucial role. Often this description is meant to contrast the design to an alternative approach. For example, the characterization of an architecture as \"database-centric\" may mean any combination of the following:\n\nusing a standard, general-purpose relational database management system, as opposed to customized in-memory or file-based data structures and access methods. With the evolution of sophisticated DBMS software, much of which is either free or included with the operating system, application developers have become increasingly reliant on standard database tools, especially for the sake of rapid application development.\nusing dynamic, table-driven logic, as opposed to logic embodied in previously compiled programs. The use of table-driven logic, i.e. behavior that is heavily dictated by the contents of a database, allows programs to be simpler and more flexible. This capability is a central feature of dynamic programming languages. See also control tables for tables that are normally coded and embedded within programs as data structures (i.e. not compiled statements) but could equally be read in from a flat file, database or even retrieved from a spreadsheet.\nusing stored procedures that run on database servers, as opposed to greater reliance on logic running in middle-tier application servers in a multi-tier architecture. The extent to which business logic should be placed at the back-end versus another tier is a subject of ongoing debate. For example, Toon Koppelaars presents a detailed analysis of alternative Oracle-based architectures that vary in the placement of business logic, concluding that a database-centric approach has practical advantages from the standpoint of ease of development and maintainability.\nusing a shared database as the basis for communicating between parallel processes in distributed computing applications, as opposed to direct inter-process communication via message passing functions and message-oriented middleware. A potential benefit of database-centric architecture in distributed applications is that it simplifies the design by utilizing DBMS-provided transaction processing and indexing to achieve a high degree of reliability, performance, and capacity. For example, Base One describes a database-centric distributed computing architecture for grid and cluster computing, and explains how this design provides enhanced security, fault-tolerance, and scalability.\nan overall enterprise architecture that favors shared data models over allowing each application to have its own, idiosyncratic data model.Even an extreme database-centric architecture called RDBMS-only architecture has been proposed, in which the three classic layers of an application are kept within the RDBMS. This architecture heavily uses the DBPL (Database Programming Language) of the RDBMS. An example of software with this architecture is Oracle Application Express (APEX)."
    },
    {
      "id": "1075035",
      "title": "Database abstraction layer",
      "url": "https://en.wikipedia.org/wiki/Database_abstraction_layer",
      "summary": "A database abstraction layer (DBAL or DAL) is an application programming interface which unifies the communication between a computer application and databases such as SQL Server, IBM Db2, MySQL, PostgreSQL, Oracle or SQLite. Traditionally, all database vendors provide their own interface that is tailored to their products. It is up to the application programmer to implement code for the database interfaces that will be supported by the application. Database abstraction layers reduce the amount of work by providing a consistent API to the developer and hide the database specifics behind this interface as much as possible. There exist many abstraction layers with different interfaces in numerous programming languages. If an application has such a layer built in, it is called database-agnostic."
    },
    {
      "id": "25386818",
      "title": "Database activity monitoring",
      "url": "https://en.wikipedia.org/wiki/Database_activity_monitoring",
      "summary": "Database activity monitoring (DAM, a.k.a. Enterprise database auditing and Real-time protection) is a database security technology for monitoring and analyzing database activity. DAM may combine data from network-based monitoring and native audit information to provide a comprehensive picture of database activity. The data gathered by DAM is used to analyze and report on database activity, support breach investigations, and alert on anomalies. DAM is typically performed continuously and in real-time.\nDatabase activity monitoring and prevention (DAMP) is an extension to DAM that goes beyond monitoring and alerting to also block unauthorized activities.\nDAM helps businesses address regulatory compliance mandates like the Payment Card Industry Data Security Standard (PCI DSS), the Health Insurance Portability and Accountability Act (HIPAA), the Sarbanes-Oxley Act (SOX), U.S. government regulations such as NIST 800-53, and EU regulations.\nDAM is also an important technology for protecting sensitive databases from external attacks by cybercriminals. According to the 2009 Verizon Business\u2019 Data Breach Investigations Report\u2014based on data analyzed from Verizon Business\u2019 caseload of 90 confirmed breaches involving 285 million compromised records during 2008\u201475 percent of all breached records came from compromised database servers.\nAccording to Gartner, \u201cDAM provides privileged user and application access monitoring that is independent of native database logging and audit functions. It can function as a compensating control for privileged user separation-of-duties issues by monitoring administrator activity. The technology also improves database security by detecting unusual database read and update activity from the application layer. Database event aggregation, correlation and reporting provide a database audit capability without the need to enable native database audit functions (which become resource-intensive as the level of auditing is increased).\u201dAccording to a survey by the Independent Oracle User Group (IOUG), \u201cMost organizations do not have mechanisms in place to prevent database administrators and other privileged database users from reading or tampering with sensitive information in financial, HR, or other business applications. Most are still unable to even detect such breaches or incidents.\u201d\nForrester refers to this category as \u201cdatabase auditing and real-time protection\u201d.\n\n"
    },
    {
      "id": "30055981",
      "title": "Database application",
      "url": "https://en.wikipedia.org/wiki/Database_application",
      "summary": "A database application is a computer program whose primary purpose is retrieving information from a computerized database. From here, information can be inserted, modified or deleted which is subsequently conveyed back into the database. Early examples of database applications were accounting systems and airline reservations systems, such as SABRE, developed starting in 1957.\nA characteristic of modern database applications is that they facilitate simultaneous updates and queries from multiple users. Systems in the 1970s might have accomplished this by having each user in front of a 3270 terminal to a mainframe computer. By the mid-1980s it was becoming more common to give each user a personal computer and have a program running on that PC that is connected to a database server. Information would be pulled from the database, transmitted over a network, and then arranged, graphed, or otherwise formatted by the program running on the PC. Starting in the mid-1990s it became more common to build database applications with a Web interface. Rather than develop custom software to run on a user's PC, the user would use the same Web browser program for every application.) A database application with a Web interface had the advantage that it could be used on devices of different sizes, with different hardware, and with different operating systems. Examples of early database applications with Web interfaces include amazon.com, which used the Oracle relational database management system, the photo.net online community, whose implementation on top of Oracle was described in the book Database-Backed Web Sites (Ziff-Davis Press; May 1997), and eBay, also running Oracle.Electronic medical records are referred to on emrexperts.com, in December 2010, as \"a software database application\". A 2005 O'Reilly book uses the term in its title: Database Applications and the Web.\nSome of the most complex database applications remain accounting systems, such as SAP, which may contain thousands of tables in only a single module. Many of today's most widely used computer systems are database applications, for example, Facebook, which was built on top of MySQL.The etymology of the phrase \"database application\" comes from the practice of dividing computer software into systems programs, such as the operating system, compilers, the file system, and tools such as the database management system, and application programs, such as a payroll check processor. On a standard PC running Microsoft Windows, for example, the Windows operating system contains all of the systems programs while games, word processors, spreadsheet programs, photo editing programs, etc. would be application programs. As \"application\" is short for \"application program\", \"database application\" is short for \"database application program\".\nNot every program that uses a database would typically be considered a \"database application\". For example, many physics experiments, e.g., the Large Hadron Collider, generate massive data sets that programs subsequently analyze. The data sets constitute a \"database\", though they are not typically managed with a standard relational database management system. The computer programs that analyze the data are primarily developed to answer hypotheses, not to put information back into the database and therefore the overall program would not be called a \"database application\".\n\n"
    },
    {
      "id": "1989841",
      "title": "Database audit",
      "url": "https://en.wikipedia.org/wiki/Database_audit",
      "summary": "Database auditing involves observing a database to be aware of the actions of database users. Database administrators and consultants often set up auditing for security purposes, for example, to ensure that those without the permission to access information do not access it.\n\n"
    },
    {
      "id": "14158342",
      "title": "Database caching",
      "url": "https://en.wikipedia.org/wiki/Database_caching",
      "summary": "Database caching is a process included in the design of computer applications which generate web pages on-demand (dynamically) by accessing backend databases.\nWhen these applications are deployed on multi-tier environments that involve browser-based clients, web application servers and backend databases, middle-tier database caching is used to achieve high scalability and performance.In a three tier architecture, the application software tier and data storage tier can be in different hosts. Throughput of an application can be limited by the network speed. This limitation can be minimized by having the database at the application tier. Because commercial database software makes extensive use of system resources, it is not always practical to have the application and the database at the same host. In this case, a more light-weight database application can be used to cache data from the commercial database management system.\n\n"
    },
    {
      "id": "8423925",
      "title": "Database connection",
      "url": "https://en.wikipedia.org/wiki/Database_connection",
      "summary": "A database connection is a facility in computer science that allows client software to talk to database server software, whether on the same machine or not.  A connection is required to send commands and receive answers, usually in the form of a result set.\nConnections are a key concept in data-centric programming. Since some DBMS engines require considerable time to connect, connection pooling was invented to improve performance.  No command can be performed against a database without an \"open and available\" connection to it.\nConnections are built by supplying an underlying driver or provider with a connection string, which is a way of addressing a specific database or server and instance as well as user authentication credentials (for example, Server=sql_box;Database=Common;User ID=uid;Pwd=password;).  Once a connection has been built it can be opened and closed at will, and properties (such as the command time-out length, or transaction, if one exists) can be set. The Connection String is composed of a set of key/value pairs as dictated by the data access interface and data provider being used.\nMany databases (such as PostgreSQL) only allow one operation to be performed at a time on each connection.  If a request for data (a SQL Select statement) is sent to the database and a result set is returned, the connection is open but not available for other operations until the client finishes consuming the result set. Other databases, like SQL Server 2005 (and later), do not impose this limitation. However, databases that provide multiple operations per connection usually incur far more overhead than those that permit only a single operation task at a time.\n\n"
    },
    {
      "id": "4675212",
      "title": "Database forensics",
      "url": "https://en.wikipedia.org/wiki/Database_forensics",
      "summary": "Database forensics is a branch of digital forensic science relating to the forensic study of databases and their related metadata.The discipline is similar to computer forensics, following the normal forensic process and applying investigative techniques to database contents and metadata. Cached information may also exist in a servers RAM requiring live analysis techniques.\nA forensic examination of a database may relate to the timestamps that apply to the update time of a row in a relational table being inspected and tested for validity in order to verify the actions of a database user.  Alternatively, a forensic examination may focus on identifying transactions within a database system or application that indicate evidence of wrongdoing, such as fraud.\nSoftware tools can be used to manipulate and analyse data. These tools also provide audit logging capabilities which provide documented proof of what tasks or analysis a forensic examiner performed on the database.\nCurrently many database software tools are in general not reliable and precise enough to be used for forensic work as demonstrated in the first paper published on database forensics.\nThere is currently a single book published in this field, though more are destined.\nAdditionally there is a subsequent SQL Server forensics book by Kevvie Fowler named SQL Server Forensics which is well regarded also.The forensic study of relational databases requires a knowledge of the standard used to encode data on the computer disk. A documentation of standards used to encode information in well-known brands of DB such as SQL Server and Oracle has been contributed to the public domain. Others include Apex Analytix.Because the forensic analysis of a database is not executed in isolation, the technological framework within which a subject database exists is crucial to understanding and resolving questions of data authenticity and integrity especially as it relates to database users.\n\n"
    },
    {
      "id": "1711076",
      "title": "Database index",
      "url": "https://en.wikipedia.org/wiki/Database_index",
      "summary": "A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.  Indexes are used to quickly locate data without having to search every row in a database table every time said table is accessed.  Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records.\nAn index is a copy of selected columns of data, from a table, that is designed to enable very efficient search.  An index normally includes a \"key\" or direct link to the original row of data from which it was copied, to allow the complete row to be retrieved efficiently.  Some databases extend the power of indexing by letting developers create indexes on column values that have been transformed by functions or expressions. For example, an index could be created on upper(last_name), which would only store the upper-case versions of the last_name field in the index. Another option sometimes supported is the use of partial index, where index entries are created only for those records that satisfy some conditional expression. A further aspect of flexibility is to permit indexing on user-defined functions, as well as expressions formed from an assortment of built-in functions."
    },
    {
      "id": "4916098",
      "title": "Database machine",
      "url": "https://en.wikipedia.org/wiki/Database_machine",
      "summary": "A database machines or back end processor is a computer or special hardware that stores and retrieves data from a database. It is specially designed for database access and is tightly coupled to the main (front-end) computer(s) by a high-speed channel, whereas a database server is a general-purpose computer that holds a database and it's loosely coupled via a local area network to its clients.\nDatabase machines can retrieve large amount of data using hundreds to thousands of microprocessors with database software. The front end processor asks the back end (typically sending a query expressed in a query language) the data and further processes it. The back end processor on the other hand analyzes and stores the data from the front end processor. Back end processors result in higher performance, increasing host main memory, increasing database recovery and security, and decreasing cost to manufacture.\n\nBritton-Lee (IDM), Tandem (Non-Stop System), and Teradata (DBC) all offered early commercial specialized database machines. A more recent example was Oracle Exadata."
    },
    {
      "id": "8640",
      "title": "Database normalization",
      "url": "https://en.wikipedia.org/wiki/Database_normalization",
      "summary": "Database normalization is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by British computer scientist Edgar F. Codd as part of his relational model.\nNormalization entails organizing the columns (attributes) and tables (relations) of a database to ensure that their dependencies are properly enforced by database integrity constraints. It is accomplished by applying some formal rules either by a process of synthesis (creating a new database design) or decomposition (improving an existing database design).\n\n"
    },
    {
      "id": "23454460",
      "title": "Database preservation",
      "url": "https://en.wikipedia.org/wiki/Database_preservation",
      "summary": "Database preservation usually involves converting the information stored in a database to a form likely to be accessible in the long term as technology changes, without losing the initial characteristics (context, content, structure, appearance and behaviour) of the data.With the prevalence of databases, different methods have been developed to aid in the preservation of databases and their contents. These methods vary depending on database characteristics and preservation needs.There are three basic methods of database preservation: migration, XML, and emulation. There are also certain tools, software, and projects which have been created to aid in the preservation of databases including SIARD, the Digital Preservation Toolkit, CHRONOS, and RODA."
    },
    {
      "id": "1500119",
      "title": "Database publishing",
      "url": "https://en.wikipedia.org/wiki/Database_publishing",
      "summary": "Database publishing is an area of automated media production in which specialized techniques are used to generate paginated documents from source data residing in traditional databases. Common examples are mail order catalogues, direct marketing, report generation, price lists and telephone directories. The database content can be in the form of text and pictures but can also contain metadata related to formatting and special rules that may apply to the document generation process. Database publishing can be incorporated into larger workflows as a component, where documents are created, approved, revised and released.\n\nThe basic idea is using database contents like article and price information to fill out pre-formatted template documents. Templates are typically created in a normal desktop layout application where certain boxes or text are designated as placeholders. These placeholders are then targeted with new content which flows in from the database. This allows for quick generation of final output and, in case of changes to the database, quickly perform updates, with limited or no manual intervention. \nAnother model of database publishing is found in many web-to-print sites where users browse templates from an online catalog (such as business cards or brochures), personalize the selected template by filling in a form and then view the rendered result. In this case the initial source of data is from user input, but it is captured in a database so that if the same user revisits the site later, they can resume editing where they left off. The form is then pre-filled from the database-stored variables the user entered before.\nThe main layout applications for this workflow are: Datalogics Pager, Adobe FrameMaker / InDesign, QuarkXPress, Xyvision, Arbortext Advanced Print Publisher (formerly 3B2) and  priint:suite. Generally, these layout applications have a corresponding server version, which receives commands via web interfaces rather than desktop interaction. QuarkXPress Server and Adobe InDesign Server both take full advantage of the design features available in their respective desktop versions.\nThese applications make their broad spectrum of features available for extension and integration with vertical products, that can be developed either internally, through some form of scripting (e.g. JavaScript or AppleScript for InDesign), or externally, through some API and corresponding developer kits.\nOther variants of database publishing are the rendering of content for direct PDF output. This approach prevents manual intervention on the final output, since PDF is not (comfortably) editable. This may not be perceived as a limitation in situations like report generation where manual editability is not needed or not desired."
    },
    {
      "id": "7514525",
      "title": "Database refactoring",
      "url": "https://en.wikipedia.org/wiki/Database_refactoring",
      "summary": "A database refactoring is a simple change to a database schema that improves its design while retaining both its behavioral and informational semantics.  Database refactoring does not change the way data is interpreted or used and does not fix bugs or add new functionality.  Every refactoring to a database leaves the system in a working state, thus not causing maintenance lags, provided the meaningful data exists in the production environment.   \nA database refactoring is conceptually more difficult than a code refactoring; code refactorings only need to maintain behavioral semantics while database refactorings also must maintain informational semantics.\nA database schema is typically refactored for one of several reasons:\n\nTo develop the schema in an evolutionary manner in parallel with the evolutionary design of the rest of the system.\nTo fix design problems with an existing legacy database schema. Database refactorings are often motivated by the desire for database normalization of an existing production database, typically to \"clean up\" the design of the database.\nTo implement what would be a large (and potentially risky) change as a series of small, low-risk changes."
    },
    {
      "id": "345937",
      "title": "Database schema",
      "url": "https://en.wikipedia.org/wiki/Database_schema",
      "summary": "The database schema is the structure of a database described in a formal language supported typically by a relational database management system (RDBMS). The term \"schema\" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of relational databases). The formal definition of a database schema is a set of formulas (sentences) called integrity constraints imposed on a database. These integrity constraints ensure compatibility between parts of the schema. All constraints are expressible in the same language. A database can be considered a structure in realization of the database language. The states of a created conceptual schema are transformed into an explicit mapping, the database schema. This describes how real-world entities are modeled in the database.\n\"A database schema specifies, based on the database administrator's knowledge of possible applications, the facts that can enter the database, or those of interest to the possible end-users.\" The notion of a database schema plays the same role as the notion of theory in predicate calculus. A model of this \"theory\" closely corresponds to a database, which can be seen at any instant of time as a mathematical object. Thus a schema can contain formulas representing integrity constraints specifically for an application and the constraints specifically for a type of database, all expressed in the same database language. In a relational database, the schema defines the tables, fields, relationships, views, indexes, packages, procedures, functions, queues, triggers, types, sequences, materialized views, synonyms, database links, directories, XML schemas, and other elements.\nA database generally stores its schema in a data dictionary. Although a schema is defined in text database language, the term is often used to refer to a graphical depiction of the database structure. In other words, schema is the structure of the database that defines the objects in the database.\nIn an Oracle Database system, the term \"schema\" has a slightly different connotation."
    },
    {
      "id": "4689778",
      "title": "Database security",
      "url": "https://en.wikipedia.org/wiki/Database_security",
      "summary": "Database security  concerns the use of a broad range of information security controls to protect databases  against compromises of their confidentiality, integrity and availability. It involves various types or categories of controls, such as technical, procedural or administrative, and physical. \nSecurity risks to database systems include, for example:\n\nUnauthorized or unintended activity or misuse by authorized database users, database administrators, or network/systems managers, or by unauthorized users or hackers (e.g. inappropriate access to sensitive data, metadata or functions within databases, or inappropriate changes to the database programs, structures or security configurations);\nMalware infections causing incidents such as unauthorized access, leakage or disclosure of personal or proprietary data, deletion of or damage to the data or programs, interruption or denial of authorized access to the database, attacks on other systems and the unanticipated failure of database services;\nOverloads, performance constraints and capacity issues resulting in the inability of authorized users to use databases as intended;\nPhysical damage to database servers caused by computer room fires or floods, overheating, lightning, accidental liquid spills, static discharge, electronic breakdowns/equipment failures and obsolescence;\nDesign flaws and programming bugs in databases and the associated programs and systems, creating various security vulnerabilities (e.g. unauthorized privilege escalation), data loss/corruption, performance degradation etc.;\nData corruption and/or loss caused by the entry of invalid data or commands, mistakes in database or system administration processes, sabotage/criminal damage etc.Ross J. Anderson has often said that by their nature large databases will never be free of abuse by breaches of security; if a large system is designed for ease of access it becomes insecure; if made watertight it becomes impossible to use. This is sometimes known as Anderson's Rule.Many layers and types of information security control are appropriate to databases, including:\n\nAccess control\nAuditing\nAuthentication\nEncryption\nIntegrity controls\nBackups\nApplication security\nDatabases have been largely secured against hackers through network security measures such as firewalls, and network-based intrusion detection systems. While network security controls remain valuable in this regard, securing the database systems themselves, and the programs/functions and data within them, has arguably become more critical as networks are increasingly opened to wider access, in particular access from the Internet. Furthermore, system, program, function and data access controls, along with the associated user identification, authentication and rights management functions, have always been important to limit and in some cases log the activities of authorized users and administrators. In other words, these are complementary approaches to database security, working from both the outside-in and the inside-out as it were.\nMany organizations develop their own \"baseline\" security standards and designs detailing basic security control measures for their database systems. These may reflect general information security requirements or obligations imposed by corporate information security policies and applicable laws and regulations (e.g. concerning privacy, financial management and reporting systems), along with generally accepted good database security practices (such as appropriate hardening of the underlying systems) and perhaps security recommendations from the relevant database system and software vendors. The security designs for specific database systems typically specify further security administration and management functions (such as administration and reporting of user access rights, log management and analysis, database replication/synchronization and backups) along with various business-driven information security controls within the database programs and functions (e.g. data entry validation and audit trails). Furthermore, various security-related activities (manual controls) are normally incorporated into the procedures, guidelines etc. relating to the design, development, configuration, use, management and maintenance of databases.\n\n"
    },
    {
      "id": "815760",
      "title": "Database server",
      "url": "https://en.wikipedia.org/wiki/Database_server",
      "summary": "A database server is a server which uses a database application that provides database services to other computer programs or to computers, as defined by the client\u2013server model. Database management systems (DBMSs) frequently provide database-server functionality, and some database management systems (such as MySQL) rely exclusively on the client\u2013server model for database access (while others, like SQLite, are meant for use as an embedded database).\nUsers access a database server either through a \"front end\" running on the user's computer \u2013 which displays requested data \u2013 or through the \"back end\", which runs on the server and handles tasks such as data analysis and storage.\nIn a master\u2013slave model, database master servers are central and primary locations of data while database slave servers are synchronized backups of the master acting as proxies.\nMost database applications respond to a query language. Each database understands its query language and converts each submitted query to server-readable form and executes it to retrieve results.\nExamples of proprietary database applications include Oracle, IBM Db2, Informix, and Microsoft SQL Server. Examples of free software database applications include PostgreSQL; and under the GNU General Public Licence include Ingres and MySQL. Every server uses its own query logic and structure. The SQL (Structured Query Language) query language is more or less the same on all relational database applications.\nFor clarification, a database server is simply a server that maintains services related to clients via database applications.\nDB-Engines lists over 300 DBMSs in its ranking."
    },
    {
      "id": "10983365",
      "title": "Database storage structures",
      "url": "https://en.wikipedia.org/wiki/Database_storage_structures",
      "summary": "Database tables and indexes may be stored on disk in one of a number of forms, including ordered/unordered flat files, ISAM, heap files, hash buckets, or B+ trees. Each form has its own particular advantages and disadvantages. The most commonly used forms are B-trees and ISAM. Such forms or structures are one aspect of the overall schema used by a database engine to store information."
    },
    {
      "id": "1093623",
      "title": "Database theory",
      "url": "https://en.wikipedia.org/wiki/Database_theory",
      "summary": "Database theory encapsulates a broad range of topics related to the study and research of the theoretical realm of databases and database management  systems.\nTheoretical aspects of data management include, among other areas, the foundations of query languages, computational complexity and expressive power of queries, finite model theory, database design theory, dependency theory, foundations of concurrency control and database recovery, deductive databases, temporal and spatial databases, real-time databases, managing uncertain data and probabilistic databases, and Web data.\nMost research work has traditionally been based on the relational model, since this model is usually considered the simplest and most foundational model of interest. Corresponding results for other data models, such as object-oriented or semi-structured models, or, more recently, graph data models and XML, are often derivable from those for the relational model.Database theory helps one to understand the complexity and power of query languages and their connection to logic. Starting from relational algebra and first-order logic (which are equivalent by Codd's theorem) and the insight that important queries such as graph reachability are not expressible in this language, more powerful language based on logic programming and fixpoint logic such as Datalog were studied. The theory also explores foundations of query optimization and data integration. Here most work studied conjunctive queries, which admit query optimization even under constraints using the chase algorithm.\nThe main research conferences in the area are the ACM Symposium on Principles of Database Systems (PODS) and the International Conference on Database Theory (ICDT)."
    },
    {
      "id": "233953",
      "title": "Database transaction",
      "url": "https://en.wikipedia.org/wiki/Database_transaction",
      "summary": "A database transaction symbolizes a unit of work, performed within a database management system (or similar system) against a database, that is treated in a coherent and reliable way independent of other transactions. A transaction generally represents any change in a database. Transactions in a database environment have two main purposes:\n\nTo provide reliable units of work that allow correct recovery from failures and keep a database consistent even in cases of system failure. For example: when execution prematurely and unexpectedly stops (completely or partially) in which case many operations upon a database remain uncompleted, with unclear status.\nTo provide isolation between programs accessing a database concurrently. If this isolation is not provided, the programs' outcomes are possibly erroneous.In a database management system, a transaction is a single unit of logic or work, sometimes made up of multiple operations. Any logical calculation done in a consistent mode in a database is known as a transaction. One example is a transfer from one bank account to another: the complete transaction requires subtracting the amount to be transferred from one account and adding that same amount to the other.\nA database transaction, by definition, must be atomic (it must either be complete in its entirety or have no effect whatsoever), consistent (it must conform to existing constraints in the database), isolated (it must not affect other transactions) and durable (it must get written to persistent storage). Database practitioners often refer to these properties of database transactions using the acronym ACID.\n\n"
    },
    {
      "id": "1384316",
      "title": "Database trigger",
      "url": "https://en.wikipedia.org/wiki/Database_trigger",
      "summary": "A database trigger is procedural code that is automatically executed in response to certain events on a particular table or view in a database. The trigger is mostly used for maintaining the integrity of the information on the database. For example, when a new record (representing a new worker) is added to the employees table, new records should also be created in the tables of the taxes, vacations and salaries. Triggers can also be used to log historical data, for example to keep track of employees' previous salaries."
    },
    {
      "id": "4452966",
      "title": "Database tuning",
      "url": "https://en.wikipedia.org/wiki/Database_tuning",
      "summary": "Database tuning describes a group of activities used to optimize and homogenize the performance of a database. It usually overlaps with query tuning, but refers to design of the database files, selection of the database management system (DBMS) application, and configuration of the database's environment (operating system, CPU, etc.).\nDatabase tuning aims to maximize use of system resources to perform work as efficiently and rapidly as possible. Most systems are designed to manage their use of system resources, but there is still much room to improve their efficiency by customizing their settings and configuration for the database and the DBMS."
    },
    {
      "id": "21852501",
      "title": "Database virtualization",
      "url": "https://en.wikipedia.org/wiki/Database_virtualization",
      "summary": "Database virtualization is the decoupling of the database layer, which lies between the storage and application layers within the application stack. Virtualization of the database layer enables a shift away from the physical, toward the logical or virtual.  \nVirtualization enables compute and storage resources to be pooled and allocated on demand. This enables both the sharing of single server resources for multi-tenancy, as well as the pooling of server resources into a single logical database or cluster. In both cases, database virtualization provides increased flexibility, more granular and efficient allocation of pooled resources, and more scalable computing."
    },
    {
      "id": "21463262",
      "title": "Datasource",
      "url": "https://en.wikipedia.org/wiki/Datasource",
      "summary": "DataSource is a name given to the connection set up to a database from a server. The name is commonly used when creating a query to the database. The data source name (DSN) need not be the same as the filename for the database. For example, a database file named friends.mdb could be set up with a DSN of school. Then DSN school would be used to refer to the database when performing a query."
    },
    {
      "id": "334641",
      "title": "Directory service",
      "url": "https://en.wikipedia.org/wiki/Directory_service",
      "summary": "In computing, a directory service or name service maps the names of network resources to their respective network addresses. It is a shared information infrastructure for locating, managing, administering and organizing everyday items and network resources, which can include volumes, folders, files, printers, users, groups, devices, telephone numbers and other objects. A directory service is a critical component of a network operating system. A directory server or name server is a server which provides such a service. Each resource on the network is considered an object by the directory server. Information about a particular resource is stored as a collection of attributes associated with that resource or object.\nA directory service defines a namespace for the network. The namespace is used to assign a name (unique identifier) to each of the objects. Directories typically have a set of rules determining how network resources are named and identified, which usually includes a requirement that the identifiers be unique and unambiguous. When using a directory service, a user does not have to remember the physical address of a network resource; providing a name locates the resource. Some directory services include access control provisions, limiting the availability of directory information to authorized users.\n\n"
    },
    {
      "id": "41054",
      "title": "Distributed database",
      "url": "https://en.wikipedia.org/wiki/Distributed_database",
      "summary": "A distributed database is a database in which data is stored across different physical locations. It may be stored in multiple computers located in the same physical location (e.g. a data centre); or maybe dispersed over a network of interconnected computers. Unlike parallel systems, in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.\nSystem administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on organised network servers or decentralised independent computers on the Internet, on corporate intranets or extranets, or on other organisation networks. Because distributed databases store data across multiple computers, distributed databases may improve performance at end-user worksites by allowing transactions to be processed on many machines, instead of being limited to one.Two processes ensure that the distributed databases remain up-to-date and current: replication and duplication.\n\nReplication involves using specialized software that looks for changes in the distributive database. Once the changes have been identified, the replication process makes all the databases look the same. The replication process can be complex and time-consuming, depending on the size and number of the distributed databases. This process can also require much time and computer resources.\nDuplication, on the other hand, has less complexity. It identifies one database as a master and then duplicates that database. The duplication process is normally done at a set time after hours. This is to ensure that each distributed location has the same data.  In the duplication process, users may change only the master database. This ensures that local data will not be overwritten.Both replication and duplication can keep the data current in all distributive locations.Besides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous, and asynchronous distributed database technologies. The implementation of these technologies can and do depend on the needs of the business and the sensitivity/confidentiality of the data stored in the database and the price the business is willing to spend on ensuring data security, consistency and integrity.\nWhen discussing access to distributed databases, Microsoft favors the term distributed query, which it defines in protocol-specific manner as \"[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources\".Oracle provides a more language-centric view in which distributed queries and distributed transactions form part of distributed SQL.\n\n"
    },
    {
      "id": "40149201",
      "title": "Docker (software)",
      "url": "https://en.wikipedia.org/wiki/Docker_(software)",
      "summary": "Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. \nThe service has both free and premium tiers. The software that hosts the containers is called Docker Engine. It was first released in 2013 and is developed by Docker, Inc.Docker is a tool that is used to automate the deployment of applications in lightweight containers so that applications can work efficiently in different environments in isolation.\n\n"
    },
    {
      "id": "15002414",
      "title": "Document-oriented database",
      "url": "https://en.wikipedia.org/wiki/Document-oriented_database",
      "summary": "A document-oriented database, or document store, is a computer program and data storage system designed for storing, retrieving and managing document-oriented information, also known as semi-structured data.Document-oriented databases are one of the main categories of NoSQL databases, and the popularity of the term \"document-oriented database\" has grown with the use of the term NoSQL itself. XML databases are a subclass of document-oriented databases that are optimized to work with XML documents. Graph databases are similar, but add another layer, the relationship, which allows them to link documents for rapid traversal.\nDocument-oriented databases are inherently a subclass of the key-value store, another NoSQL database concept. The difference lies in the way the data is processed; in a key-value store, the data is considered to be inherently opaque to the database, whereas a document-oriented system relies on internal structure in the document in order to extract metadata that the database engine uses for further optimization. Although the difference is often negligible due to tools in the systems, conceptually the document-store is designed to offer a richer experience with modern programming techniques.\nDocument databases contrast strongly with the traditional relational database (RDB). Relational databases generally store data in separate tables that are defined by the programmer, and a single object may be spread across several tables. Document databases store all information for a given object in a single instance in the database, and every stored object can be different from every other. This eliminates the need for object-relational mapping while loading data into the database.\n\n"
    },
    {
      "id": "14554451",
      "title": "Embarcadero Technologies",
      "url": "https://en.wikipedia.org/wiki/Embarcadero_Technologies",
      "summary": "Embarcadero Technologies, Inc. is an American computer software company that develops, manufactures, licenses and supports products and services related to software through several product divisions. It was founded in 1993, went public in 2000 and private in 2007, and became a division of Idera, Inc. in 2015.\n\n"
    },
    {
      "id": "752991",
      "title": "Entity\u2013relationship model",
      "url": "https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model",
      "summary": "An entity\u2013relationship model (or ER model) describes interrelated things of interest in a specific domain of knowledge. A basic ER model is composed of entity types (which classify the things of interest) and specifies relationships that can exist between entities (instances of those entity types).\nIn software engineering, an ER model is commonly formed to represent things a business needs to remember in order to perform business processes. Consequently, the ER model becomes an abstract data model, that defines a data or information structure which can be implemented in a database, typically a relational database.\nEntity\u2013relationship modeling was developed for database and design by Peter Chen and published in a 1976 paper, with variants of the idea existing previously, but today it is commonly used for teaching students the basics of data base structure. Some ER models show super and subtype entities connected by generalization-specialization relationships, and an ER model can be used also in the specification of domain-specific ontologies."
    },
    {
      "id": "9310",
      "title": "Enterprise resource planning",
      "url": "https://en.wikipedia.org/wiki/Enterprise_resource_planning",
      "summary": "Enterprise resource planning (ERP) is the integrated management of main business processes, often in real time and mediated by software and technology. ERP is usually referred to as a category of business management software\u2014typically a suite of integrated applications\u2014that an organization can use to collect, store, manage and interpret data from many business activities. ERP systems can be local-based or cloud-based. Cloud-based applications have grown in recent years due to the increased efficiencies arising from information being readily available from any location with Internet access.\nERP provides an integrated and continuously updated view of core business processes using common databases maintained by a database management system. ERP systems track business resources\u2014cash, raw materials, production capacity\u2014and the status of business commitments: orders, purchase orders, and payroll. The applications that make up the system share data across various departments (manufacturing, purchasing, sales, accounting, etc.) that provide the data. ERP facilitates information flow between all business functions and manages connections to outside stakeholders.According to Gartner, the global ERP market size is estimated at $35 billion in 2021. Though early ERP systems focused on large enterprises, smaller enterprises increasingly use ERP systems.The ERP system integrates varied organizational systems and facilitates error-free transactions and production, thereby enhancing the organization's efficiency. However, developing an ERP system differs from traditional system development.\nERP systems run on a variety of computer hardware and network configurations, typically using a database as an information repository.\n\n"
    },
    {
      "id": "1270246",
      "title": "Federated database system",
      "url": "https://en.wikipedia.org/wiki/Federated_database_system",
      "summary": "A federated database system (FDBS) is a type of meta-database management system (DBMS), which transparently maps multiple autonomous database systems into a single federated database. The constituent databases are interconnected via a computer network and may be geographically decentralized. Since the constituent database systems remain autonomous, a federated database system is a contrastable alternative to the (sometimes daunting) task of merging several disparate databases. A federated database, or virtual database, is a composite of all constituent databases in a federated database system.  There is no actual data integration in the constituent disparate databases as a result of data federation.\nThrough data abstraction, federated database systems can provide a uniform user interface, enabling users and clients to store and retrieve data from multiple noncontiguous databases with a single query\u2014even if the constituent databases are heterogeneous. To this end, a federated database system must be able to decompose the query into subqueries for submission to the relevant constituent DBMSs, after which the system must composite the result sets of the subqueries. Because various database management systems employ different query languages, federated database systems can apply wrappers to the subqueries to translate them into the appropriate query languages."
    },
    {
      "id": "259065",
      "title": "Foreign key",
      "url": "https://en.wikipedia.org/wiki/Foreign_key",
      "summary": "A foreign key is a set of attributes in a table that refers to the primary key of another table, linking these two tables. In the context of relational databases, a foreign key is subject to a inclusion dependency constraint that the tuples consisting of the foreign key attributes in one relation, R, must also exist in some other (not necessarily distinct) relation, S; furthermore that those attributes must also be a candidate key in S.In other words, a foreign key is a set of attributes that references a candidate key. For example, a table called TEAM may have an attribute, MEMBER_NAME, which is a foreign key referencing a candidate key, PERSON_NAME, in the PERSON table. Since MEMBER_NAME is a foreign key, any value existing as the name of a member in TEAM must also exist as a person's name in the PERSON table; in other words, every member of a TEAM is also a PERSON.\nImportant points to note:-\n\nThe reference relation should already be created.\nThe referenced attribute must be a part of primary key of the referenced relation.\nData type and size of referenced and referencing attribute must be same.\n\n"
    },
    {
      "id": "24799509",
      "title": "Graph database",
      "url": "https://en.wikipedia.org/wiki/Graph_database",
      "summary": "A graph database (GDB) is a database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data. A key concept of the system is the graph (or edge or relationship). The graph relates the data items in the store to a collection of nodes and edges, the edges representing the relationships between the nodes. The relationships allow data in the store to be linked together directly and, in many cases, retrieved with one operation. Graph databases hold the relationships between data as a priority. Querying relationships is fast because they are perpetually stored in the database. Relationships can be intuitively visualized using graph databases, making them useful for heavily inter-connected data.Graph databases are commonly referred to as a NoSQL. Graph databases are similar to 1970s network model databases in that both represent general graphs, but network-model databases operate at a lower level of abstraction and lack easy traversal over a chain of edges.The underlying storage mechanism of graph databases can vary. Relationships are a first-class citizen in a graph database and can be labelled, directed, and given properties. Some depend on a relational engine and \"store\" the graph data in a table (although a table is a logical element, therefore this approach imposes another level of abstraction between the graph database, the graph database management system and the physical devices where the data is actually stored). Others use a key\u2013value store or document-oriented database for storage, making them inherently NoSQL structures.\nAs of 2021, no universal graph query language has been adopted in the same way as SQL was for relational databases, and there are a wide variety of systems, most often tightly tied to one product. Some early standardization efforts lead to multi-vendor query languages like Gremlin, SPARQL, and Cypher. In September 2019 a proposal for a project to create a new standard graph query language (ISO/IEC 39075 Information Technology \u2014 Database Languages \u2014 GQL) was approved by members of ISO/IEC Joint Technical Committee 1(ISO/IEC JTC 1). GQL is intended to be a declarative database query language, like SQL. In addition to having query language interfaces, some graph databases are accessed through application programming interfaces (APIs).\nGraph databases differ from graph compute engines. Graph databases are technologies that are translations of the relational online transaction processing (OLTP) databases. On the other hand, graph compute engines are used in online analytical processing (OLAP) for bulk analysis. Graph databases attracted considerable attention in the 2000s, due to the successes of major technology corporations in using proprietary graph databases, along with the introduction of open-source graph databases.\nOne study concluded that an RDBMS was \"comparable\" in performance to existing graph analysis engines at executing graph queries."
    },
    {
      "id": "10210385",
      "title": "GridApp Systems",
      "url": "https://en.wikipedia.org/wiki/GridApp_Systems",
      "summary": "GridApp Systems, Inc. was a database automation software company.  It was purchased by BMC Software in December, 2010.Founded in 2002 and headquartered in New York City, GridApp Systems was the brainchild of five former employees of Register.com, Rob Gardos, Shamoun Murtza, Matthew Zito, Dan Cohen, and Eric Gross. The five realized that 85% of the routine tasks performed by database administrators could be automated, decreasing critical errors and improving productivity; all five functioned as GridApp's CEO, CTO, Chief Scientist, Director of Development, and Mr. Database respectively.\nGridApp's flagship product is GridApp Clarity, which has won the following awards and recognition:\n\nSearchSQLServer \u2015 2006 \u2015 \"Performance and Tuning\" Category \u2015 Silver \nServerWatch \u2015 2008 \u2015 \"Automation and Compliance\" category \u2015 Silver\nCODiE \u2015 2008 \u2015 \"Best Database Management Software\" category \u2015 Finalist\nEMA \u2015 2008 \u2015 \"EMA Rising Star\""
    },
    {
      "id": "18825039",
      "title": "Halloween Problem",
      "url": "https://en.wikipedia.org/wiki/Halloween_Problem",
      "summary": "In computing, the Halloween Problem refers to  a phenomenon in databases in which an update operation causes a change in the physical location of a row, potentially allowing the row to be visited again later in the same update operation. This could even cause an infinite loop in some cases where updates continually place the updated record ahead of the scan performing the update operation.\nThe potential for this database error was first discovered by Don Chamberlin, Pat Selinger, and Morton Astrahan in 1976, on Halloween day while working on a query that was supposed to give a ten percent raise to every employee who earned less than $25,000. This query would run successfully, with no errors, but when finished all the employees in the database earned at least $25,000, because it kept giving them a raise until they reached that level.  The expectation was that the query would iterate over each of the employee records with a salary less than $25,000 precisely once.  In fact, because even updated records were visible to the query execution engine and so continued to match the query's criteria, salary records were matching multiple times and each time being given a 10% raise until they were all greater than $25,000.\nThe name is not descriptive of the nature of the problem but rather was given due to the day it was discovered on.  As recounted by Don Chamberlin:\n\nPat and Morton discovered this problem on Halloween... I remember they came into my office and said, \"Chamberlin, look at this. We have to make sure that when the optimizer is making a plan for processing an update, it doesn't use an index that is based on the field that is being updated. How are we going to do that?\" It happened to be on a Friday, and we said, \"Listen, we are not going to be able to solve this problem this afternoon. Let's just give it a name. We'll call it the Halloween Problem and we'll work on it next week.\" And it turns out it has been called that ever since."
    },
    {
      "id": "247075",
      "title": "Hierarchical database model",
      "url": "https://en.wikipedia.org/wiki/Hierarchical_database_model",
      "summary": "A hierarchical database model is a data model in which the data are organized into a tree-like structure. The data are stored as records which are connected to one another through links. A record is a collection of fields, with each field containing only one value. The  type of a record defines which fields the record contains.\nThe hierarchical database model mandates that each child record has only one parent, whereas each parent record can have one or more child records. In order to retrieve data from a hierarchical database, the whole tree needs to be traversed starting from the root node. This model is recognized as the first database model created by IBM in the 1960s.\n\n"
    },
    {
      "id": "142983",
      "title": "IBM Db2",
      "url": "https://en.wikipedia.org/wiki/IBM_Db2",
      "summary": "Db2 is a family of data management products, including database servers, developed by IBM. It initially supported the relational model, but was extended to support object\u2013relational features and non-relational structures like JSON and XML. The brand name was originally styled as DB/2, then DB2 until 2017 and finally changed to its present form."
    },
    {
      "id": "1942477",
      "title": "In-memory database",
      "url": "https://en.wikipedia.org/wiki/In-memory_database",
      "summary": "An in-memory database (IMDb, or main memory database system (MMDB) or memory resident database) is a database management system that primarily relies on main memory for computer data storage. It is contrasted with database management systems that employ a disk storage mechanism. In-memory databases are faster than disk-optimized databases because disk access is slower than memory access and the internal optimization algorithms are simpler and execute fewer CPU instructions. Accessing data in memory eliminates seek time when querying the data, which provides faster and more predictable performance than disk.Applications where response time is critical, such as those running telecommunications network equipment and mobile advertising networks, often use main-memory databases. IMDBs have gained much traction, especially in the data analytics space, starting in the  mid-2000s \u2013 mainly due to multi-core processors that can address large memory and due to less expensive RAM.A potential technical hurdle with in-memory data storage is the volatility of RAM. Specifically in the event of a power loss, intentional or otherwise, data stored in volatile RAM is lost. With the introduction of non-volatile random-access memory technology, in-memory databases will be able to run at full speed and maintain data in the event of power failure.\n\n"
    },
    {
      "id": "494528",
      "title": "Query language",
      "url": "https://en.wikipedia.org/wiki/Query_language",
      "summary": "A query language, also known as data query language or database query language (DQL), is a computer language used to make queries in databases and information systems. In database systems, query languages rely on strict theory to retrieve information. A well known example is the Structured Query Language (SQL).\n\n"
    },
    {
      "id": "8514669",
      "title": "Intelligent database",
      "url": "https://en.wikipedia.org/wiki/Intelligent_database",
      "summary": "Until the 1980s, databases were viewed as computer systems that stored record-oriented and business data such as manufacturing inventories, bank records, and sales transactions. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information (rather than data) in a way that appears natural to users and which goes beyond simple record keeping.\nThe term was introduced in 1989 by the book Intelligent Databases by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. The concept postulated three levels of intelligence for such systems: high level tools, the user interface and the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation.\nIn the twenty-first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x-ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis."
    },
    {
      "id": "26257672",
      "title": "Java Database Connectivity",
      "url": "https://en.wikipedia.org/wiki/Java_Database_Connectivity",
      "summary": "Java Database Connectivity (JDBC) is an application programming interface (API) for the Java programming language which defines how a client may access a database. It is a Java-based data access technology used for Java database connectivity. It is part of the Java Standard Edition platform, from Oracle Corporation. It provides methods to query and update data in a database, and is oriented toward relational databases. A JDBC-to-ODBC bridge enables connections to any ODBC-accessible data source in the Java virtual machine (JVM) host environment.\n\n"
    },
    {
      "id": "37704882",
      "title": "Key\u2013value database",
      "url": "https://en.wikipedia.org/wiki/Key%E2%80%93value_database",
      "summary": "A key\u2013value database, or key\u2013value store, is a data storage paradigm designed for storing, retrieving, and managing associative arrays, and a data structure more commonly known today as a dictionary or hash table. Dictionaries contain a collection of objects, or records, which in turn have many different fields within them, each containing data. These records are stored and retrieved using a key that uniquely identifies the record, and is used to find the data within the database. \n\nKey\u2013value databases work in a very different fashion from the better known relational databases (RDB). RDBs predefine the data structure in the database as a series of tables containing fields with well defined data types. Exposing the data types to the database program allows it to apply a number of optimizations. In contrast, key\u2013value systems treat the data as a single opaque collection, which may have different fields for every record. This offers considerable flexibility and more closely follows modern concepts like object-oriented programming. Because optional values are not represented by placeholders or input parameters, as in most RDBs, key\u2013value databases often use far less memory to store the same data, which can lead to large performance gains in certain workloads.Performance, a lack of standardization and other issues have limited key\u2013value systems to niche uses for many years, but the rapid move to cloud computing after 2010 has led to a renaissance as part of the broader NoSQL movement. Some graph databases, such as ArangoDB, are also key\u2013value databases internally, adding the concept of the relationships (pointers) between records as a first class data type.\n\n"
    },
    {
      "id": "43291963",
      "title": "Kubernetes",
      "url": "https://en.wikipedia.org/wiki/Kubernetes",
      "summary": "Kubernetes (, commonly abbreviated K8s) is an open-source container orchestration system for automating software deployment, scaling, and management. Originally designed by Google, the project is now maintained by the Cloud Native Computing Foundation.\nThe name Kubernetes originates from Ancient Greek, meaning 'helmsman' or 'pilot'. Kubernetes is often abbreviated as K8s, counting the eight letters between the K and the s (a numeronym).Kubernetes works with various container runtimes, such as containerd and CRI-O. Its suitability for running and managing large cloud-native workloads has led to its widespread adoption in the data center. There are multiple distributions of this platform \u2013 from independent software vendors (ISVs) as well as hosted-on-cloud offerings from all the major public cloud vendors.\n\n"
    },
    {
      "id": "1005923",
      "title": "List of academic databases and search engines",
      "url": "https://en.wikipedia.org/wiki/List_of_academic_databases_and_search_engines",
      "summary": "This article contains a representative list of notable databases and search engines useful in an academic setting for finding and accessing articles in academic journals, institutional repositories, archives, or other collections of scientific and other articles. Databases and search engines differ substantially in terms of coverage and retrieval qualities. Users need to account for qualities and limitations of databases and search engines, especially those searching systematically for records such as in systematic reviews or meta-analyses. As the distinction between a database and a search engine is unclear for these complex document retrieval systems, see:\n\nthe general list of search engines for all-purpose search engines that can be used for academic purposes\nthe article about bibliographic databases for information about databases giving bibliographic information about finding books and journal articles.The terms \"free\", \"subscription\", and \"free & subscription\" will refer to the availability of the website as well as the journal articles used. Furthermore, some programs are only partly free (for example, accessing abstracts or a small number of items), whereas complete access is prohibited (login or institutional subscription required).\nThe \"Size\" column denotes the number of documents (articles, publications, datasets, preprints) rather than the number of citations or references. The database itself should be the primary source of statistics, and if it is not accessible, the independent estimates released as journal papers should be. Notably, Google Scholar does not offer such detail, but the database's size has been calculated.\n\n"
    },
    {
      "id": "6331838",
      "title": "List of biodiversity databases",
      "url": "https://en.wikipedia.org/wiki/List_of_biodiversity_databases",
      "summary": "This is a list of biodiversity databases. Biodiversity databases store taxonomic information alone or more commonly also other information like distribution (spatial) data and ecological data, which provide information on the biodiversity of a particular area or group of living organisms. They may store specimen-level information, species-level information, information on nomenclature, or any combination of the above. Most are available online.\nSpecimen-focused databases contain data about individual specimens, as represented by vouchered museum specimens, collections of specimen photographs, data on field-based specimen observations and morphological or genetic data. Species-focused databases contain information summarised at the species-level. Some species-focused databases attempt to compile comprehensive data about particular species (FishBase), while others focus on particular species attributes, such as checklists of species in a given area (FEOW) or the conservation status of species (CITES or IUCN Red List). Nomenclators act as summaries of taxonomic revisions and set  a key between specimen-focused and species-focused databases. They do this because taxonomic revisions use specimen data to determine species limits.\n\n"
    },
    {
      "id": "29717355",
      "title": "List of biological databases",
      "url": "https://en.wikipedia.org/wiki/List_of_biological_databases",
      "summary": "Biological databases are stores of biological information. The journal Nucleic Acids Research regularly publishes special issues on biological databases and has a list of such databases. The 2018 issue has a list of about 180 such databases and updates to previously described databases. Omics Discovery Index can be used to browse and search several biological databases. Furthermore, the NIAID Data Ecosystem Discovery Portal developed by the National Institute of Allergy and Infectious Diseases (NIAID) enables searching across databases.\n\n"
    },
    {
      "id": "42500102",
      "title": "List of column-oriented DBMSes",
      "url": "https://en.wikipedia.org/wiki/List_of_column-oriented_DBMSes",
      "summary": "This article is a list of column-oriented database management system software."
    },
    {
      "id": "50833317",
      "title": "List of facial expression databases",
      "url": "https://en.wikipedia.org/wiki/List_of_facial_expression_databases",
      "summary": "A facial expression database is a collection of images or video clips with facial expressions of a range of emotions.\nWell-annotated (emotion-tagged) media content of facial behavior is essential for training, testing, and validation of algorithms for the development of expression recognition systems. The emotion annotation can be done in discrete emotion labels or on a continuous scale. Most of the databases are usually based on the basic emotions theory (by Paul Ekman) which assumes the existence of six discrete basic emotions (anger, fear, disgust, surprise, joy, sadness). However, some databases include the emotion tagging in continuous arousal-valence scale.\nIn posed expression databases, the participants are asked to display different basic emotional expressions, while in spontaneous expression database, the expressions are natural. Spontaneous expressions differ from posed ones remarkably in terms of intensity, configuration, and duration. Apart from this, synthesis of some AUs are barely achievable without undergoing the associated emotional state. Therefore, in most cases, the posed expressions are exaggerated, while the spontaneous ones are subtle and differ in appearance.\nMany publicly available databases are categorized here. Here are some details of the facial expression databases. \n\n"
    },
    {
      "id": "42447955",
      "title": "List of in-memory databases",
      "url": "https://en.wikipedia.org/wiki/List_of_in-memory_databases",
      "summary": "Notable in-memory database system software includes:"
    },
    {
      "id": "8621397",
      "title": "List of online databases",
      "url": "https://en.wikipedia.org/wiki/List_of_online_databases",
      "summary": "This is a list of online databases accessible via the Internet.\n\n"
    },
    {
      "id": "3967073",
      "title": "List of online music databases",
      "url": "https://en.wikipedia.org/wiki/List_of_online_music_databases",
      "summary": "Below is a table of online music databases that are largely free of charge.  Many of the sites provide a specialized service or focus on a particular music genre.  Some of these operate as an online music store or purchase referral service in some capacity. Among the sites that have information on the largest number of entities are those sites that focus on discographies of composing and performing artists.\nPerformance rights organisations (PRO) typically have their own databases as per country they represent, in accordance with CISAC, to help domestic artists collect royalties.  Information available on these portals include songwriting credits, publishing percentage splits, and alternate titles for different distribution channels. It is one of the most accurate and official types of databases because it involves direct communication between the artists, record labels, distributors, legal teams, publishers and a global governing body regulating PRO's. Many countries that observe copyright have an organisation established, currently there are 119 CISAC members, and they may be not-for-profit. The databases are typically known as 'repertory searches' or 'searching works' and may require an account while others are open to view for free as public including the USA's ASCAP Songview and Ace  services, Canada's SOCAN, South Korea's KOMCA, France's SACEM, and Israel's ACUM.\n\n"
    },
    {
      "id": "52714158",
      "title": "List of online real estate databases",
      "url": "https://en.wikipedia.org/wiki/List_of_online_real_estate_databases",
      "summary": "This is an alphabetical  list of online real estate databases.\n\n"
    },
    {
      "id": "1568820",
      "title": "List of relational database management systems",
      "url": "https://en.wikipedia.org/wiki/List_of_relational_database_management_systems",
      "summary": "This is a list of relational database management systems."
    },
    {
      "id": "67986706",
      "title": "Lists of databases",
      "url": "https://en.wikipedia.org/wiki/Lists_of_databases",
      "summary": "This is a list of lists of databases or databanks:\n\nList of academic databases and search engines\nList of biodiversity databases\nList of biological databases\nList of chemical databases\nList of databases for oncogenomic research\nList of Drosophila databases\nList of genealogy databases\nList of long non-coding RNA databases\nList of neuroscience databases\nList of online databases\nList of online music databases\n\n"
    },
    {
      "id": "15219872",
      "title": "Load file",
      "url": "https://en.wikipedia.org/wiki/Load_file",
      "summary": "A load file in the litigation community is commonly referred to as the file used to import data (coded, captured or extracted data from ESI processing) into a database; or the file used to link images.  These load files carry commands, commanding the software to carry out certain functions with the data found in them.\nLoad files are usually ASCII text files that have delimited fields of information.  Such load files may have data about documents to be imported into a document management software such as Concordance or Summation.  Or they may have the path or directory where images may reside so that the software can link such images to their corresponding records.\nSome database programs take one load file for importing images and another for importing data while others take only one load file for both pieces of information.\nOCR or Search-able Text which is considered \"data\" is also imported into most database programs via the same load files.  Though some people prefer to load the OCR into their databases by running a separate command to search and find the desired text.\nCommonly used databases and their corresponding file extensions are: Summation (DII\n, CSV), Concordance (OPT, DAT), Sanction (SDT), IPRO (LFP), Ringtail (MDB) and DB/TextWorks (TXT)."
    },
    {
      "id": "689470",
      "title": "Record locking",
      "url": "https://en.wikipedia.org/wiki/Record_locking",
      "summary": "Record locking is the technique of preventing simultaneous access to data in a database, to prevent inconsistent results.\nThe classic example is demonstrated by two bank clerks attempting to update the same bank account for two different transactions.  Clerks 1 and 2 both retrieve (i.e., copy) the account's record.  Clerk 1 applies and saves a transaction.  Clerk 2 applies a different transaction to his saved copy, and saves the result, based on the original record and his changes, overwriting the transaction entered by clerk 1. The record no longer reflects the first transaction, as if it had never taken place.\nA simple way to prevent this is to lock the file whenever a record is being modified by any user, so that no other user can save data. This prevents records from being overwritten incorrectly, but allows only one record to be processed at a time, locking out other users who need to edit records at the same time.\nTo allow several users to edit a database table at the same time and also prevent inconsistencies created by  unrestricted access, a single record can be locked when retrieved for editing or updating.  Anyone attempting to retrieve the same record for editing is denied write access because of the lock (although, depending on the implementation, they may be able to view the record without editing it).  Once the record is saved or edits are canceled, the lock is released. Records can never be saved so as to overwrite other changes, preserving data integrity.\nIn database management theory, locking is used to implement isolation among multiple database users.  This is the \"I\" in the acronym ACID.\nA thorough and authoritative description of locking was written by Jim Gray.\n\n"
    },
    {
      "id": "21064035",
      "title": "Locks with ordered sharing",
      "url": "https://en.wikipedia.org/wiki/Locks_with_ordered_sharing",
      "summary": "In databases and transaction processing the term Locks with ordered sharing comprises several variants of the two-phase locking (2PL) concurrency control protocol generated by changing the blocking semantics of locks upon conflicts. One variant is identical to strict commitment ordering (SCO). Further softening of locks eliminates thrashing.\n\n"
    },
    {
      "id": "20722131",
      "title": "Log shipping",
      "url": "https://en.wikipedia.org/wiki/Log_shipping",
      "summary": "Log shipping is the process of automating the backup of transaction log files on a primary (production) database server, and then restoring them onto a standby server. This technique is supported by Microsoft SQL Server, 4D Server, MySQL, and PostgreSQL.  Similar to replication, the primary purpose of log shipping is to increase database availability by maintaining a backup server that can replace a production server quickly. Other databases such as Adaptive Server Enterprise and Oracle Database support the technique but require the Database Administrator to write code or scripts to perform the work.\nAlthough the actual failover mechanism in log shipping is manual, this implementation is often chosen due to its low cost in human and server resources, and ease of implementation.  In comparison, SQL server clusters enable automatic failover, but at the expense of much higher storage costs.  Compared to database replication, log shipping does not provide as much in terms of reporting capabilities, but backs up system tables along with data tables, and locks the standby server from users' modifications. A replicated server can be modified (e.g. views) and is therefore unsuitable for failover purposes.\n\n"
    },
    {
      "id": "19961416",
      "title": "Microsoft Azure",
      "url": "https://en.wikipedia.org/wiki/Microsoft_Azure",
      "summary": "Microsoft Azure, often referred to as Azure (/\u02c8\u00e6\u0292\u0259r, \u02c8e\u026a\u0292\u0259r/ AZH-\u0259r, AY-zh\u0259r, UK also /\u02c8\u00e6zj\u028a\u0259r, \u02c8e\u026azj\u028a\u0259r/ AZ-ure, AY-zure), is a cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers. It also provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.\nAzure was first introduced at the Professional Developers Conference (PDC) in October 2008 under the codename \"Project Red Dog.\" It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014.\n\n"
    },
    {
      "id": "13989994",
      "title": "Microsoft SQL Server",
      "url": "https://en.wikipedia.org/wiki/Microsoft_SQL_Server",
      "summary": "Microsoft SQL Server (Structured Query Language) is a proprietary relational database management system developed by Microsoft. As a database server, it is a software product with the primary function of storing and retrieving data as requested by other software applications\u2014which may run either on the same computer or on another computer across a network (including the Internet). Microsoft markets at least a dozen different editions of Microsoft SQL Server, aimed at different audiences and for workloads ranging from small single-machine applications to large Internet-facing applications with many concurrent users.\n\n"
    },
    {
      "id": "18984",
      "title": "Mongols",
      "url": "https://en.wikipedia.org/wiki/Mongols",
      "summary": "The Mongols are an East Asian ethnic group native to Mongolia, Inner Mongolia in China, and the Republic of Buryatia of the Russian Federation. The Mongols are the principal member of the large family of Mongolic peoples. The Oirats in Western Mongolia as well as the Buryats and Kalmyks of Russia are classified either as distinct ethno-linguistic groups or subgroups of Mongols.\nThe Mongols are bound together by a common heritage and ethnic identity. Their indigenous dialects are collectively known as the Mongolian language.  The contiguous geographical area in which the Mongols primarily live is referred to as the Mongol heartland, especially in history books. The ancestors of the modern-day Mongols are referred to as Proto-Mongols."
    },
    {
      "id": "44971098",
      "title": "Multi-model database",
      "url": "https://en.wikipedia.org/wiki/Multi-model_database",
      "summary": "In the field of database design, a multi-model database is a database management system designed to support multiple data models against a single, integrated backend. In contrast, most database management systems are organized around a single data model that determines how data can be organized, stored, and manipulated. Document, graph, relational, and key\u2013value models are examples of data models that may be supported by a multi-model database.\n\n"
    },
    {
      "id": "622805",
      "title": "Navigational database",
      "url": "https://en.wikipedia.org/wiki/Navigational_database",
      "summary": "A navigational database is a type of database in which records or objects are found primarily by following references from other objects. The term was popularized by the title of Charles Bachman's 1973 Turing Award paper, The Programmer as Navigator. This paper emphasized the fact that the new disk-based database systems allowed the programmer to choose arbitrary navigational routes following relationships from record to record, contrasting this with the constraints of earlier magnetic-tape and punched card systems where data access was strictly sequential.\nOne of the earliest navigational databases was Integrated Data Store (IDS), which was developed by Bachman for General Electric in the 1960s. IDS became the basis for the CODASYL database model in 1969.\nAlthough Bachman described the concept of navigation in abstract terms, the idea of navigational access came to be associated strongly with the procedural design of the CODASYL Data Manipulation Language. Writing in 1982, for example, Tsichritzis and Lochovsky state that \"The notion of currency is central to the concept of navigation.\" By the notion of currency, they refer to the idea that a program maintains (explicitly or implicitly) a current position in any sequence of records that it is processing, and that operations such as GET NEXT and GET PRIOR retrieve records relative to this current position, while also changing the current position to the record that is retrieved.\nNavigational database programming thus came to be seen as intrinsically procedural; and moreover to depend on the maintenance of an implicit set of global variables (currency indicators) holding the current state. As such, the approach was seen as diametrically opposed to the declarative programming style used by the relational model. The declarative nature of relational languages such as SQL offered better programmer productivity and a higher level of data independence (that is, the ability of programs to continue working as the database structure evolves.) Navigational interfaces, as a result, were gradually eclipsed during the 1980s by declarative query languages.\nDuring the 1990s it started becoming clear that for certain applications handling complex data (for example, spatial databases and engineering databases), the relational calculus had limitations. At that time, a reappraisal of the entire database market began, with several companies describing the new systems using the marketing term NoSQL. Many of these systems introduced data manipulation languages which, while far removed from the CODASYL DML with its currency indicators, could be understood as implementing Bachman's \"navigational\" vision. Some of these languages are procedural; others (such as XPath) are entirely declarative. Offshoots of the navigational concept, such as the graph database, found new uses in modern transaction processing workloads."
    },
    {
      "id": "12024861",
      "title": "Negative database",
      "url": "https://en.wikipedia.org/wiki/Negative_database",
      "summary": "In database security, a negative database is a database that saves attributes that cannot be associated with a certain entry.A negative database is a kind of database that contains huge amount of data consisting of simulating data.\nWhen anyone tries to get access to such databases both the actual and the negative data sets will be retrieved even if they steal the entire database. \nFor example, instead of storing just the personal details you store personal details that members don't have.Negative databases can avoid inappropriate queries and inferences. They also support allowable operations.\nUnder this scenario, it is desirable that the database support only the allowable queries while protecting the privacy of individual records, say from inspection by an insider.\nCollection of negative data has been referred to as \"negative sousveillance\":\n\n\"Negative databases (Esponda, 2006), is the keeping records which if stolen do not reveal the identity of individuals. Negative databases achieve this by storing the complement of the set of what is being tracked. Essentially the database shows what isn\u2019t of concern. Extending his approach the negative intelligence gatherer would seek to understand what websites, infrastructure systems, environmental sensors or documents have become unavailable. The negative sousveillance concept then is to record, track, or infer what isn\u2019t there.\""
    },
    {
      "id": "185098",
      "title": "Network model",
      "url": "https://en.wikipedia.org/wiki/Network_model",
      "summary": "In computing, the network model is a database model conceived as a flexible way of representing objects and their relationships. Its distinguishing feature is that the schema, viewed as a graph in which object types are nodes and relationship types are arcs, is not restricted to being a hierarchy or lattice.\nThe network model was adopted by the CODASYL Data Base Task Group in 1969 and underwent a major update in 1971. It is sometimes known as the CODASYL model for this reason. A number of network database systems became popular on mainframe and minicomputers through the 1970s before being widely replaced by relational databases in the 1980s.\n\n"
    },
    {
      "id": "2002540",
      "title": "Null (SQL)",
      "url": "https://en.wikipedia.org/wiki/Null_(SQL)",
      "summary": "In SQL, null or NULL is a special marker used to indicate that a data value does not exist in the database. Introduced by the creator of the relational database model, E. F. Codd, SQL null serves to fulfil the requirement that all true relational database management systems (RDBMS) support a representation of \"missing information and inapplicable information\".  Codd also introduced the use of the lowercase Greek omega (\u03c9) symbol to represent null in database theory.  In SQL, NULL is a reserved word used to identify this marker.\nA null should not be confused with a value of 0. A null indicates a lack of a value, which is not the same thing as a zero value. For example, consider the question \"How many books does Adam own?\" The answer may be \"zero\" (we know that he owns none) or \"null\" (we do not know how many he owns). In a database table, the column reporting this answer would start out with no value (marked by null), and it would not be updated with the value zero until it is ascertained that Adam owns no books.\nIn SQL, null is a marker, not a value. This usage is quite different from most programming languages, where a null value of a reference means it is not pointing to any object."
    },
    {
      "id": "22826",
      "title": "Object database",
      "url": "https://en.wikipedia.org/wiki/Object_database",
      "summary": "An object database or object-oriented database  is a database management system in which information is represented in the form of objects as used in object-oriented programming. Object databases are different from relational databases which are table-oriented. A third type, object\u2013relational databases, is a hybrid of both approaches.\nObject databases have been considered since the early 1980s."
    },
    {
      "id": "5646487",
      "title": "Object model",
      "url": "https://en.wikipedia.org/wiki/Object_model",
      "summary": "In computing, object model has two related but distinct meanings:\n\nThe properties of objects in general in a specific computer programming language, technology, notation or methodology that uses them. Examples are the object models of Java, the Component Object Model (COM), or Object-Modeling Technique (OMT).  Such object models are usually defined using concepts such as class, generic function, message, inheritance, polymorphism, and encapsulation.  There is an extensive literature on formalized object models as a subset of the formal semantics of programming languages.\nA collection of objects or classes through which a program can examine and manipulate some specific parts of its world. In other words, the object-oriented interface to some service or system.  Such an interface is said to be the object model of the represented service or system.  For example, the Document Object Model (DOM) is a collection of objects that represent a page in a web browser, used by script programs to examine and dynamically change the page.  There is a Microsoft Excel object model [1] for controlling Microsoft Excel from another program, and the ASCOM Telescope Driver is an object model for controlling an astronomical telescope.An object model consists of the following important features:\n\nObject reference\nObjects can be accessed via object references. To invoke a method in an object, the object reference and method name are given, together with any arguments.Interfaces\nAn interface provides a definition of the signature of a set of methods without specifying their implementation. An object will provide a particular interface if its class contains code that implement the method of that interface. An interface also defines types that can be used to declare the type of variables or parameters and return values of methods.Actions\nAn action in object-oriented programming (OOP) is initiated by an object invoking a method in another object. An invocation can include additional information needed to carry out the method. The receiver executes the appropriate method and then returns control to the invoking object, sometimes supplying a result.Exceptions\nPrograms can encounter various errors and unexpected conditions of varying seriousness. During the execution of the method many different problems may be discovered. Exceptions provide a clean way to deal with error conditions without complicating the code. A block of code may be defined to throw an exception whenever particular unexpected conditions or errors arise. This means that control passes to another block of code that catches the exception.\n\n"
    },
    {
      "id": "68960",
      "title": "Object\u2013relational database",
      "url": "https://en.wikipedia.org/wiki/Object%E2%80%93relational_database",
      "summary": "An object\u2013relational database (ORD), or object\u2013relational database management system (ORDBMS), is a database management system (DBMS) similar to a relational database, but with an object-oriented database model: objects, classes and inheritance are directly supported in database schemas and in the query language.  In addition, just as with pure relational systems, it supports extension of the data model with custom data types and methods.\n\nAn object\u2013relational database can be said to provide a middle ground between relational databases and object-oriented databases. In object\u2013relational databases, the approach is essentially that of relational databases: the data resides in the database and is manipulated collectively with queries in a query language; at the other extreme are OODBMSes in which the database is essentially a persistent object store for software written in an object-oriented programming language, with a programming API for storing and retrieving objects, and little or no specific support for querying.\n\n"
    },
    {
      "id": "59202",
      "title": "Object\u2013relational mapping",
      "url": "https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping",
      "summary": "Object\u2013relational mapping (ORM, O/RM, and O/R mapping tool) in computer science is a programming technique for converting data between a relational database and the heap of an object-oriented programming language. This creates, in effect, a virtual object database that can be used from within the programming language.\nIn object-oriented programming, data-management tasks act on objects that combine scalar values into objects. For example, consider an address book entry that represents a single person along with zero or more phone numbers and zero or more addresses. This could be modeled in an object-oriented implementation by a \"Person object\" with an attribute/field to hold each data item that the entry comprises: the person's name, a list of phone numbers, and a list of addresses. The list of phone numbers would itself contain \"PhoneNumber objects\" and so on. Each such address-book entry is treated as a single object by the programming language (it can be referenced by a single variable containing a pointer to the object, for instance). Various methods can be associated with the object, such as methods to return the preferred phone number, the home address, and so on.\nBy contrast, relational databases, such as SQL, group scalars into tuples, which are then enumerated in tables. Tuples and objects have some general similarity, in that they are both ways to collect values into named fields such that the whole collection can be manipulated as a single compound entity. They have many differences, though, in particular: lifecycle management (row insertion and deletion, versus garbage collection or reference counting), references to other entities (object references, versus foreign key references), and inheritance (non-existent in relational databases). As well, objects are managed on-heap and are under full control of a single process, while database tuples are shared and must incorporate locking, merging, and retry. Object\u2013relational mapping provides automated support for mapping tuples to objects and back, while accounting for all of these differences.The heart of the problem involves translating the logical representation of the objects into an atomized form that is capable of being stored in the database while preserving the properties of the objects and their relationships so that they can be reloaded as objects when needed. If this storage and retrieval functionality is implemented, the objects are said to be persistent."
    },
    {
      "id": "168701",
      "title": "Open Database Connectivity",
      "url": "https://en.wikipedia.org/wiki/Open_Database_Connectivity",
      "summary": "In computing, Open Database Connectivity (ODBC) is a standard application programming interface (API) for accessing database management systems (DBMS). The designers of ODBC aimed to make it independent of database systems and operating systems. An application written using ODBC can be ported to other platforms, both on the client and server side, with few changes to the data access code.\nODBC accomplishes DBMS independence by using an ODBC driver as a translation layer between the application and the DBMS. The application uses ODBC functions through an ODBC driver manager with which it is linked, and the driver passes the query to the DBMS. An ODBC driver can be thought of as analogous to a printer driver or other driver, providing a standard set of functions for the application to use, and implementing DBMS-specific functionality. An application that can use ODBC is referred to as \"ODBC-compliant\". Any ODBC-compliant application can access any DBMS for which a driver is installed.  Drivers exist for all major DBMSs, many other data sources like address book systems and Microsoft Excel, and even for text or comma-separated values (CSV) files.\nODBC was originally developed by Microsoft and Simba Technologies during the early 1990s, and became the basis for the Call Level Interface (CLI) standardized by SQL Access Group in the Unix and mainframe field. ODBC retained several features that were removed as part of the CLI effort. Full ODBC was later ported back to those platforms, and became a de facto standard considerably better known than CLI. The CLI remains similar to ODBC, and applications can be ported from one platform to the other with few changes.\n\n"
    },
    {
      "id": "2504464",
      "title": "Oracle Applications",
      "url": "https://en.wikipedia.org/wiki/Oracle_Applications",
      "summary": "Oracle Applications comprise the applications software or business software of the Oracle Corporation both in the cloud and on-premises. The term refers to the non-database and non-middleware parts. The suite of applications includes enterprise resource planning, enterprise performance management, supply chain & manufacturing, human capital management, and advertising and customer experience.Oracle initially launched its application suite with financials software in the late 1980s. By 2009, the offering extended to supply chain management, human-resource management, warehouse-management, customer-relationship management, call-center services, product-lifecycle management, and many other areas. Both in-house expansion and the acquisition of other companies have vastly expanded Oracle's application software business.\nIn February 2007, Oracle released Oracle E-Business Suite (EBS/e-BS) Release 12 (R12) \u2013 a bundling of several Oracle Applications. The release date coincided with new releases of other Oracle-owned products: JD Edwards EnterpriseOne, Siebel Systems and PeopleSoft.\nOracle also has a portfolio of enterprise applications for the cloud (SaaS) known as Oracle Fusion Cloud Applications. These cloud applications include Oracle Cloud ERP, Oracle Cloud EPM, Oracle Cloud HCM, Oracle Cloud SCM, and Oracle Advertising and CX."
    },
    {
      "id": "22591",
      "title": "Oracle Corporation",
      "url": "https://en.wikipedia.org/wiki/Oracle_Corporation",
      "summary": "Oracle Corporation is an American multinational computer technology company headquartered in Austin, Texas, United States. In 2020, Oracle was the third-largest software company in the world by revenue and market capitalization. The company sells database software (particularly the Oracle Database) and cloud computing. Oracle's core application software is a suite of enterprise software products, such as enterprise resource planning (ERP) software, human capital management (HCM) software, customer relationship management (CRM) software, enterprise performance management (EPM) software, Customer Experience Commerce(CX Commerce) and supply chain management (SCM) software.\n\n"
    },
    {
      "id": "323725",
      "title": "Oracle Database",
      "url": "https://en.wikipedia.org/wiki/Oracle_Database",
      "summary": "Oracle Database (commonly referred to as Oracle DBMS, Oracle Autonomous Database, or simply as Oracle) is a proprietary multi-model database management system produced and marketed by Oracle Corporation.\nIt is a database commonly used for running online transaction processing (OLTP), data warehousing (DW) and mixed (OLTP & DW) database workloads. Oracle Database is available by several service providers on-prem, on-cloud, or as a hybrid cloud installation.  It may be run on third party servers as well as on Oracle hardware (Exadata on-prem, on Oracle Cloud or at Cloud at Customer).Oracle Database uses SQL query language for database updating and retrieval.\n\n"
    },
    {
      "id": "35855099",
      "title": "Outline of databases",
      "url": "https://en.wikipedia.org/wiki/Outline_of_databases",
      "summary": "The following is provided as an overview of and topical guide to databases:\nDatabase \u2013 organized collection of data, today typically in digital form. The data are typically organized to model relevant aspects of reality (for example, the availability of rooms in hotels), in a way that supports processes requiring this information (for example, finding a hotel with vacancies).\n\n"
    },
    {
      "id": "1983101",
      "title": "Partition (database)",
      "url": "https://en.wikipedia.org/wiki/Partition_(database)",
      "summary": "A partition is a division of a logical database or its constituent elements into distinct independent parts. Database partitioning is normally done for manageability, performance or availability reasons, or for load balancing. It is popular in distributed database management systems, where each partition may be spread over multiple nodes, with users at the node performing local transactions on the partition. This increases performance for sites that have regular transactions involving certain views of data, whilst maintaining availability and security."
    },
    {
      "id": "154864",
      "title": "PeopleSoft",
      "url": "https://en.wikipedia.org/wiki/PeopleSoft",
      "summary": "PeopleSoft, Inc. is a company that provides human resource management systems (HRMS), financial management solutions (FMS), supply chain management (SCM), customer relationship management (CRM), and enterprise performance management (EPM) software, as well as software for manufacturing, and student administration to large corporations, governments, and organizations. It existed as an independent corporation until its acquisition by Oracle Corporation in 2005. The PeopleSoft name and product line are now marketed by Oracle.\nPeopleSoft Financial Management Solutions (FMS) and Supply Chain Management (SCM) are part of the same package, commonly known as Financials and Supply Chain Management (FSCM).\nPeopleSoft Campus Solutions (CS) is a separate package developed as a student information system for colleges and universities.\n\n"
    },
    {
      "id": "23824",
      "title": "PostgreSQL",
      "url": "https://en.wikipedia.org/wiki/PostgreSQL",
      "summary": "PostgreSQL (, POHST-gres kyoo el), also known as Postgres, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance.\nPostgreSQL features transactions with atomicity, consistency, isolation, durability (ACID) properties, automatically updatable views, materialized views, triggers, foreign keys, and stored procedures.\nIt is supported on all major operating systems, including Linux, FreeBSD, OpenBSD, macOS, and Windows, and handles a range of workloads from single machines to data warehouses or web services with many concurrent users.\nThe PostgreSQL Global Development Group focuses only on developing a database engine and closely related components.\nThis core is, technically, what comprises PostgreSQL itself, but there is an extensive developer community and ecosystem that provides other important feature sets that might, traditionally, be provided by a proprietary software vendor.\nThese include special-purpose database engine features, like those needed to support a geospatial or temporal database or features which emulate other database products. \nAlso available from third parties are a wide variety of user and machine interface features, such as graphical user interfaces or load balancing and high availability toolsets.\nThe large third-party PostgreSQL support network of people, companies, products, and projects, even though not part of The PostgreSQL Development Group, are essential to the PostgreSQL database engine's adoption and use and make up the PostgreSQL ecosystem writ large.PostgreSQL was originally named POSTGRES, referring to its origins as a successor to the Ingres database developed at the University of California, Berkeley. In 1996, the project was renamed PostgreSQL to reflect its support for SQL. After a review in 2007, the development team decided to keep the name PostgreSQL and the alias Postgres."
    },
    {
      "id": "15395806",
      "title": "Process management (computing)",
      "url": "https://en.wikipedia.org/wiki/Process_management_(computing)",
      "summary": "A process is a program in execution, and an integral part of any modern-day operating system (OS). The OS must allocate resources to processes, enable processes to share and exchange information, protect the resources of each process from other processes and enable synchronization among processes. To meet these requirements, the OS  must maintain a data structure for each process, which describes the state and resource ownership of that process, and which enables the OS to exert control over each process.\n\n"
    },
    {
      "id": "43435003",
      "title": "Query rewriting",
      "url": "https://en.wikipedia.org/wiki/Query_rewriting",
      "summary": "Query rewriting is a typically automatic transformation that takes a set of database tables, views, and/or queries, usually indices, often gathered data and query statistics, and other metadata, and yields a set of different queries, which produce the same results but execute with better performance (for example, faster, or with lower memory use). Query rewriting can be based on relational algebra or an extension thereof (e.g. multiset relational algebra with sorting, aggregation and three-valued predicates i.e. NULLs as in the case of SQL). The equivalence rules of relational algebra are exploited, in other words, different query structures and orderings can be mathematically proven to yield the same result. For example, filtering on fields A and B, or cross joining R and S can be done in any order, but there can be a performance difference. Multiple operations may be combined, and operation orders may be altered.\nThe result of query rewriting may not be at the same abstraction level or application programming interface (API) as the original set of queries (though often is). For example, the input queries may be in relational algebra or SQL, and the rewritten queries may be closer to the physical representation of the data, e.g. array operations. Query rewriting can also involve materialization of views and other subqueries; operations that may or may not be available to the API user. The query rewriting transformation can be aided by creating indices from which the optimizer can choose (some database systems create their own indexes if deemed useful), mandating the use of specific indices, creating materialized and/or denormalized views, or helping a database system gather statistics on the data and query use, as the optimality depends on patterns in data and typical query usage.\nQuery rewriting may be rule based or optimizer based. Some sources discuss query rewriting as a distinct step prior to optimization, operating at the level of the user accessible algebra API (e.g. SQL).There are other, largely unrelated concepts also named similarly, for example, query rewriting by search engines.\n\n"
    },
    {
      "id": "3480761",
      "title": "Query optimization",
      "url": "https://en.wikipedia.org/wiki/Query_optimization",
      "summary": "Query optimization is a feature of many relational database management systems and other databases such as NoSQL and graph databases. The query optimizer attempts to determine the most efficient way to execute a given query by considering the possible query plans.Generally, the query optimizer cannot be accessed directly by users: once queries are submitted to the database server, and parsed by the parser, they are then passed to the query optimizer where optimization occurs. However, some database engines allow guiding the query optimizer with hints.\nA query is a request for information from a database. It can be as simple as \"find the address of a person with Social Security number 123-45-6789,\" or more complex like \"find the average salary of all the employed married men in California between the ages 30 to 39 who earn less than their spouses.\" The result of a query is generated by processing the rows in a database in a way that yields the requested information. Since database structures are complex, in most cases, and especially for not-very-simple queries, the needed data for a query can be collected from a database by accessing it in different ways, through different data-structures, and in different orders. Each different way typically requires different processing time. Processing times of the same query may have large variance, from a fraction of a second to hours, depending on the chosen method. The purpose of query optimization, which is an automated process, is to find the way to process a given query in minimum time. The large possible variance in time justifies performing query optimization, though finding the exact optimal query plan, among all possibilities, is typically very complex, time-consuming by itself, may be too costly, and often practically impossible. Thus query optimization typically tries to approximate the optimum by comparing several common-sense alternatives to provide in a reasonable time a \"good enough\" plan which typically does not deviate much from the best possible result.\n\n"
    },
    {
      "id": "1710792",
      "title": "Query plan",
      "url": "https://en.wikipedia.org/wiki/Query_plan",
      "summary": "A query plan (or query execution plan) is a sequence of steps used to access data in a SQL relational database management system. This is a specific case of the relational model concept of access plans.\nSince SQL is declarative, there are typically many alternative ways to execute a given query, with widely varying performance. When a query is submitted to the database, the query optimizer evaluates some of the different, correct possible plans for executing the query and returns what it considers the best option. Because query optimizers are imperfect, database users and administrators sometimes need to manually examine and tune the plans produced by the optimizer to get better performance."
    },
    {
      "id": "42437",
      "title": "Quest Software",
      "url": "https://en.wikipedia.org/wiki/Quest_Software",
      "summary": "Quest Software, also known as Quest, is a privately held software company headquartered in Aliso Viejo, California, United States. Quest provides cloud management, software as a service, security, workforce mobility, and backup & recovery. The company was founded in 1987 and has 53 offices in 24 countries.\n\n"
    },
    {
      "id": "445718",
      "title": "Referential integrity",
      "url": "https://en.wikipedia.org/wiki/Referential_integrity",
      "summary": "Referential integrity is a property of data stating that all its references are valid.  In the context of relational databases, it requires that if a value of one attribute (column) of a relation (table) references a value of another attribute (either in the same or a different relation), then the referenced value must exist.For referential integrity to hold in a relational database, any column in a base table that is declared a foreign key can only contain either null values or values from a parent table's primary key or a candidate key. In other words, when a foreign key value is used it must reference a valid, existing primary key in the parent table. For instance, deleting a record that contains a value referred to by a foreign key in another table would break referential integrity. Some relational database management systems (RDBMS) can enforce referential integrity, normally either by deleting the foreign key rows as well to maintain integrity, or by returning an error and not performing the delete. Which method is used may be determined by a referential integrity constraint defined in a data dictionary.\nThe adjective 'referential' describes the action that a foreign key performs, 'referring' to a linked column in another table. In simple terms, 'referential integrity' guarantees that the target 'referred' to will be found. A lack of referential integrity in a database can lead relational databases to return incomplete data, usually with no indication of an error.\n\n"
    },
    {
      "id": "23720058",
      "title": "Relation (database)",
      "url": "https://en.wikipedia.org/wiki/Relation_(database)",
      "summary": "In database theory, a relation, as originally defined by E. F. Codd, is a set of tuples (d1, d2, ..., dn), where each element dj is a member of Dj, a data domain. Codd's original definition notwithstanding, and contrary to the usual definition in mathematics, there is no ordering to the elements of the tuples of a relation.  Instead, each element is termed an attribute value. An attribute is a name paired with a domain (nowadays more commonly referred to as a type or data type). An attribute value is an attribute name paired with an element of that attribute's domain, and a tuple is a set of attribute values in which no two distinct elements have the same name. Thus, in some accounts, a tuple is described as a function, mapping names to values.\nA set of attributes in which no two distinct elements have the same name is called a heading. It follows from the above definitions that to every tuple there corresponds a unique heading, being the set of names from the tuple, paired with the domains from which the tuple elements' domains are taken. A set of tuples that all correspond to the same heading is called a body. A relation is thus a heading paired with a body, the heading of the relation being also the heading of each tuple in its body. The number of attributes constituting a heading is called the degree, which term also applies to tuples and relations. The term n-tuple refers to a tuple of degree n (n \u2265 0).\nE. F. Codd used the term \"relation\" in its mathematical sense of a finitary relation, a set of tuples on some set of n  sets S1, S2, .... ,Sn. Thus, an n-ary relation is interpreted, under the Closed-World Assumption, as the extension of some n-adic predicate: all and only those n-tuples whose values, substituted for corresponding free variables in the predicate, yield propositions that hold true, appear in the relation.\nThe term relation schema refers to a heading paired with a set of constraints defined in terms of that heading. A relation can thus be seen as an instantiation of a relation schema if it has the heading of that schema and it satisfies the applicable constraints.\nSometimes a relation schema is taken to include a name. A relational database definition (database schema, sometimes referred to as a relational schema) can thus be thought of as a collection of named relation schemas.In implementations, the domain of each attribute is effectively a data type and a named relation schema is effectively a relation variable (relvar for short).\nIn SQL, a database language for relational databases, relations are represented by tables, where each row of a table represents a single tuple, and where the values of each attribute form a column.\n\n"
    },
    {
      "id": "175285",
      "title": "Relational algebra",
      "url": "https://en.wikipedia.org/wiki/Relational_algebra",
      "summary": "In database theory, relational algebra is a theory that uses algebraic structures for modeling data, and defining queries on it with a well founded semantics. The theory was introduced by Edgar F. Codd.\nThe main application of relational algebra is to provide a theoretical foundation for relational databases, particularly query languages for such databases, chief among which is SQL. Relational databases store tabular data represented as relations. Queries over relational databases often likewise return tabular data represented as relations. \nThe main purpose of relational algebra is to define operators that transform one or more input relations to an output relation. Given that these operators accept relations as input and produce relations as output, they can be combined and used to express complex queries that transform multiple input relations (whose data are stored in the database) into a single output relation (the query results). \nUnary operators accept a single relation as input. Examples include operators to filter certain attributes (columns) or tuples (rows) from an input relation. Binary operators accept two relations as input and combine them into a single output relation. For example, taking all tuples found in either relation (union), removing tuples from the first relation found in the second relation (difference), extending the tuples of the first relation with tuples in the second relation matching certain conditions, and so forth.\nOther more advanced operators can also be included, where the inclusion or exclusion of certain operators gives rise to a family of algebras."
    },
    {
      "id": "175769",
      "title": "Relational calculus",
      "url": "https://en.wikipedia.org/wiki/Relational_calculus",
      "summary": "The relational calculus consists of two calculi, the tuple relational calculus and the domain relational calculus, that is part of the relational model for databases and provide a declarative way to specify database queries. The raison d'\u00eatre of relational calculus is the formalization of query optimization, which is finding more efficient manners to execute the same query in a database. \nThe relational calculus is similar to the relational algebra, which is also part of the relational model: While the relational calculus is meant as a declarative language that prescribes no execution order on the subexpressions of a relational calculus expression, the relational algebra is meant as an imperative language: the sub-expressions of a relational algebraic expression are meant to be executed from left-to-right and inside-out following their nesting. \nPer Codd's theorem, the relational algebra and the domain-independent relational calculus are logically equivalent.\n\n"
    },
    {
      "id": "26220",
      "title": "Relational model",
      "url": "https://en.wikipedia.org/wiki/Relational_model",
      "summary": "The relational model (RM) is an approach to managing data using a structure and language consistent with first-order predicate logic, first described in 1969 by English computer scientist Edgar F. Codd, where all data is represented in terms of tuples, grouped into relations. A database organized in terms of the relational model is a relational database.\nThe purpose of the relational model is to provide a declarative method for specifying data and queries: users directly state what information the database contains and what information they want from it, and let the database management system software take care of describing data structures for storing the data and retrieval procedures for answering queries.\nMost relational databases use the SQL data definition and query language; these systems implement what can be regarded as an engineering approximation to the relational model. A table in a SQL database schema corresponds to a predicate variable; the contents of a table to a relation; key constraints, other constraints, and SQL queries correspond to predicates. However, SQL databases deviate from the relational model in many details, and Codd fiercely argued against deviations that compromise the original principles."
    },
    {
      "id": "2722905",
      "title": "Replication (computing)",
      "url": "https://en.wikipedia.org/wiki/Replication_(computing)",
      "summary": "Replication in computing involves sharing information so as to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance, or accessibility.\n\n"
    },
    {
      "id": "1041142",
      "title": "Row (database)",
      "url": "https://en.wikipedia.org/wiki/Row_(database)",
      "summary": "In the context of a relational database, a row\u2014also called a tuple\u2014represents a single, implicitly structured data item in a table. In simple terms, a database table can be thought of as consisting of rows and columns. Each row in a table represents a set of related data, and every row in the table has the same structure.\nFor example, in a table that represents companies, each row would represent a single company. Columns might represent things like company name, company street address, whether the company is publicly held, its VAT number, etc. In a table that represents the association of employees with departments, each row would associate one employee with one department.\nThe implicit structure of a row, and the meaning of the data values in a row, requires that the row be understood as providing a succession of data values, one in each column of the table. The row is then interpreted as a relvar composed of a set of tuples, with each tuple consisting of the two items: the name of the relevant column and the value this row provides for that column.\nEach column expects a data value of a particular type. For example, one column might require a unique identifier, another might require text representing a person's name, another might require an integer representing hourly pay in dollars.\n\n"
    },
    {
      "id": "2688402",
      "title": "Runbook",
      "url": "https://en.wikipedia.org/wiki/Runbook",
      "summary": "In a computer system or network, a runbook is a compilation of routine procedures and operations that the system administrator or operator carries out. System administrators in IT departments and NOCs use runbooks as a reference. \nRunbooks can be in either electronic or in physical book form. Typically, a runbook contains procedures to begin, stop, supervise, and debug the system. It may also describe procedures for handling special requests and contingencies. An effective runbook allows other operators, with prerequisite expertise, to effectively manage and troubleshoot a system. \nThrough runbook automation, these processes can be carried out using software tools in a predetermined manner.  In addition to automating IT specific processes, the results of the runbook can be presented on-screen back to the user or Service Desk engineer.  Multiple runbooks can be linked together using a Decision Tree to provide users with interactive troubleshooting and guided procedures.\n\n"
    },
    {
      "id": "2728460",
      "title": "SQL Plus",
      "url": "https://en.wikipedia.org/wiki/SQL_Plus",
      "summary": "SQL Plus is the most basic Oracle Database utility, with a basic command-line interface, commonly used by users, administrators, and programmers.\n\n"
    },
    {
      "id": "19467605",
      "title": "Shard (database architecture)",
      "url": "https://en.wikipedia.org/wiki/Shard_(database_architecture)",
      "summary": "A database shard, or simply a shard, is a horizontal partition of data in a database or search engine. Each shard is held on a separate database server instance, to spread load.\nSome data within a database remains present in all shards, but some appear only in a single shard. Each shard (or server) acts as the single source for this subset of data.\n\n"
    },
    {
      "id": "2816674",
      "title": "Component-based software engineering",
      "url": "https://en.wikipedia.org/wiki/Component-based_software_engineering",
      "summary": "Component-based software engineering (CBSE), also called component-based development (CBD), is a style of software engineering that aims to build software out of loosely-coupled, modular components. It emphasizes the separation of concerns among different parts of a software system.\n\n"
    },
    {
      "id": "14109784",
      "title": "Special Interest Group on Information Retrieval",
      "url": "https://en.wikipedia.org/wiki/Special_Interest_Group_on_Information_Retrieval",
      "summary": "SIGIR is the Association for Computing Machinery's Special Interest Group on Information Retrieval. The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage, retrieval and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.\n\n"
    },
    {
      "id": "277251",
      "title": "Stored procedure",
      "url": "https://en.wikipedia.org/wiki/Stored_procedure",
      "summary": "A stored procedure (also termed proc, storp, sproc, StoPro, StoredProc, StoreProc, sp, or SP) is a subroutine available to applications that access a relational database management system (RDBMS). Such procedures are stored in the database data dictionary.\nUses for stored procedures include data-validation (integrated into the database) or access-control mechanisms.  Furthermore, stored procedures can consolidate and centralize logic that was originally implemented in applications. To save time and memory, extensive or complex processing that requires execution of several SQL statements can be saved into stored procedures, and all applications call the procedures. One can use nested stored procedures by executing one stored procedure from within another.\nStored procedures may return result sets, i.e., the results of a SELECT statement.  Such result sets can be processed using cursors, by other stored procedures, by associating a result-set locator, or by applications. Stored procedures may also contain declared variables for processing data and cursors that allow it to loop through multiple rows in a table. Stored-procedure flow-control statements typically include IF, WHILE, LOOP, REPEAT, and CASE statements, and more. Stored procedures can receive variables, return results or modify variables and return them, depending on how and where the variable is declared.\n\n"
    },
    {
      "id": "12796220",
      "title": "Stratavia",
      "url": "https://en.wikipedia.org/wiki/Stratavia",
      "summary": "Stratavia, formerly known as ExtraQuest, was a software company that specialized in enterprise Database and Data Center Automation, and private cloud computing enablement. Stratavia was founded by Venkat Devraj and Rainier Luistro in 2001.The company held two patents for automating standard operating procedures in database administration, and was included in the analyst firm Gartner's list of Cool Vendors for 2009.The company received venture funding from Vista Ventures, Asset Management Company and Adams Street Partners and is headquartered in Denver, Colorado, with offices in San Francisco, New York City, Atlanta, Boston and London, United Kingdom. In Aug. 2010, HP announced the acquisition of Stratavia to bolster its cloud computing and automation software portfolio.HP Software Division planned to integrate Stratavia technology into its cloud computing portfolio, including HP Cloud Service Automation Software."
    },
    {
      "id": "1137952",
      "title": "Superkey",
      "url": "https://en.wikipedia.org/wiki/Superkey",
      "summary": "In the relational data model a superkey is a set of attributes that uniquely identifies each tuple of a relation. Because superkey values are unique, tuples with the same superkey value must also have the same non-key attribute values. That is, non-key attributes are functionally dependent on the superkey.\nThe set of all attributes is always a superkey (the trivial superkey). Tuples in a relation are by definition unique, with duplicates removed after each operation, so the set of all attributes is always uniquely valued for every tuple. A candidate key (or minimal superkey) is a superkey that can't be reduced to a simpler superkey by removing an attribute.For example, in an employee schema with attributes employeeID, name, job, and departmentID, if employeeID values are unique then employeeID combined with any or all of the other attributes can uniquely identify tuples in the table. Each combination, {employeeID}, {employeeID, name}, {employeeID, name, job}, and so on is a superkey. {employeeID} is a candidate key, since no subset of its attributes is also a superkey. {employeeID, name, job, departmentID} is the trivial superkey.\nIf attribute set K is a superkey of relation R, then at all times it is the case that the projection of R over K has the same cardinality as R itself."
    },
    {
      "id": "472950",
      "title": "Surrogate key",
      "url": "https://en.wikipedia.org/wiki/Surrogate_key",
      "summary": "A surrogate key (or synthetic key, pseudokey, entity identifier, factless key, or technical key) in a database is a unique identifier for either an entity in the modeled world or an object in the database. The surrogate key is not derived from application data, unlike a natural (or business) key.\n\n"
    },
    {
      "id": "1701163",
      "title": "Table (database)",
      "url": "https://en.wikipedia.org/wiki/Table_(database)",
      "summary": "A table is a collection of related data held in a table format within a database. It consists of columns and rows.\nIn relational databases, and flat file databases, a table is a set of data elements (values) using a model of vertical columns (identifiable by name) and horizontal rows, the cell being the unit where a row and column intersect. A table has a specified number of columns, but can have any number of rows. Each row is identified by one or more values appearing in a particular column subset. A specific choice of columns which uniquely identify rows is called the primary key.\n\"Table\" is another term for \"relation\"; although there is the difference in that a table is usually a multiset (bag) of rows where a relation is a set and does not allow duplicates. Besides the actual data rows, tables generally have associated with them some metadata, such as constraints on the table or on the values within particular columns.The data in a table does not have to be physically stored in the database. Views also function as relational tables, but their data are calculated at query time. External tables (in Informix\nor Oracle,\nfor example) can also be thought of as views.\n\nIn many systems for computational statistics, such as R and Python's pandas, a data frame or data table is a data type supporting the table abstraction. Conceptually, it is a list of records or observations all containing the same fields or columns. The implementation consists of a list of arrays or vectors, each with a name.\n\n"
    },
    {
      "id": "32399",
      "title": "Video game developer",
      "url": "https://en.wikipedia.org/wiki/Video_game_developer",
      "summary": "A video game developer is a software developer specializing in video game development \u2013 the process and related disciplines of creating video games. A game developer can range from one person who undertakes all tasks to a large business with employee responsibilities split between individual disciplines, such as programmers, designers, artists, etc. Most game development companies have video game publisher financial and usually marketing support. Self-funded developers are known as independent or indie developers and usually make indie games.A developer may specialize in specific game engines or specific video game consoles, or may develop for several systems (including personal computers and mobile devices). Some focus on porting games from one system to another, or translating games from one language to another. Less commonly, some do software development work in addition to games.\nMost video game publishers maintain development studios (such as Electronic Arts's EA Canada, Square Enix's studios, Activision's Radical Entertainment, Nintendo EPD and Sony's Polyphony Digital and Naughty Dog). However, since publishing is still their primary activity they are generally described as \"publishers\" rather than \"developers\". Developers may be private as well.\n\n"
    },
    {
      "id": "245974",
      "title": "Transaction log",
      "url": "https://en.wikipedia.org/wiki/Transaction_log",
      "summary": "In the field of databases in computer science, a transaction log (also transaction journal, database log, binary log or audit trail) is a history of actions executed by a database management system used to guarantee ACID properties over crashes or hardware failures. Physically, a log is a file listing changes to the database, stored in a stable storage format.\nIf, after a start, the database is found in an inconsistent state or not been shut down properly, the database management system reviews the database logs for uncommitted transactions and rolls back the changes made by these transactions. Additionally, all transactions that are already committed but whose changes were not yet materialized in the database are re-applied. Both are done to ensure atomicity and durability of transactions.\nThis term is not to be confused with other, human-readable logs that a database management system usually provides.\nIn database management systems, a journal is the record of data altered by a given process.\n\n"
    },
    {
      "id": "212409",
      "title": "Transaction processing",
      "url": "https://en.wikipedia.org/wiki/Transaction_processing",
      "summary": "In computer science, transaction  processing is information processing  that is divided into individual, indivisible operations called transactions. Each transaction must succeed or fail as a complete unit; it can never be only partially complete.\nFor example, when you purchase a book from an online bookstore, you exchange money (in the form of credit) for a book. If your credit is good, a series of related operations ensures that you get the book and the bookstore gets your money. However, if a single operation in the series fails during the exchange, the entire exchange fails. You do not get the book and the bookstore does not get your money. The technology responsible for making the exchange balanced and predictable is called transaction processing. Transactions ensure that data-oriented resources are not permanently updated unless all operations within the transactional unit complete successfully. By combining a set of related operations into a unit that either completely succeeds or completely fails, one can simplify error recovery and make one's application more reliable.\nTransaction processing systems consist of computer hardware and software hosting a transaction-oriented application that performs the routine transactions necessary to conduct business. Examples include systems that manage sales order entry, airline reservations, payroll, employee records, manufacturing, and shipping.\nSince most, though not necessarily all, transaction processing today is interactive, the term is often treated as synonymous with online transaction processing."
    },
    {
      "id": "244602",
      "title": "Two-phase locking",
      "url": "https://en.wikipedia.org/wiki/Two-phase_locking",
      "summary": "In databases and transaction processing, two-phase locking (2PL) is a pessimistic concurrency control method that guarantees serializability.\nIt is also the name of the resulting set of database transaction schedules (histories). The protocol uses locks, applied by a transaction to data, which may block (interpreted as signals to stop) other transactions from accessing the same data during the transaction's life.\nBy the 2PL protocol, locks are applied and removed in two phases:\n\nExpanding phase: locks are acquired and no locks are released.\nShrinking phase: locks are released and no locks are acquired.Two types of locks are used by the basic protocol: Shared and Exclusive locks. Refinements of the basic protocol may use more lock types. Using locks that block processes, 2PL may be subject to deadlocks that result from the mutual blocking of two or more transactions.\n\n"
    },
    {
      "id": "6850099",
      "title": "Unique key",
      "url": "https://en.wikipedia.org/wiki/Unique_key",
      "summary": "In relational database management systems, a unique key is a candidate key. All the candidate keys of a relation can uniquely identify the records of the relation, but only one of them is used as the primary key of the relation. The remaining candidate keys are called unique keys because they can uniquely identify a record in a relation. Unique keys can consist of multiple columns. Unique keys are also called alternate keys. Unique keys are an alternative to the primary key of the relation. In SQL, the unique keys have a UNIQUE constraint assigned to them in order to prevent duplicates (a duplicate entry is not valid in a unique column). Alternate keys may be used like the primary key when doing a single-table select or when filtering in a where clause, but are not typically used to join multiple tables.\n\n"
    },
    {
      "id": "1960226",
      "title": "View (SQL)",
      "url": "https://en.wikipedia.org/wiki/View_(SQL)",
      "summary": "In a database, a view is the result set of a stored query, which can be queried in the same manner as a persistent database collection object. This pre-established query command is kept in the data dictionary. Unlike ordinary base tables in a relational database, a view does not form part of the physical schema: as a result set, it is a virtual table computed or collated dynamically from data in the database when access to that view is requested. Changes applied to the data in a relevant underlying table are reflected in the data shown in subsequent invocations of the view.\nViews can provide advantages over tables:\n\nViews can represent a subset of the data contained in a table. Consequently, a view can limit the degree of exposure of the underlying tables to the outer world: a given user may have permission to query the view, while denied access to the rest of the base table.\nViews can join and simplify multiple tables into a single virtual table.\nViews can act as aggregated tables, where the database engine aggregates data (sum, average, etc.) and presents the calculated results as part of the data.\nViews can hide the complexity of data. For example, a view could appear as Sales2020 or Sales2021, transparently partitioning the actual underlying table.\nViews take very little space to store; the database contains only the definition of a view, not a copy of all the data that it presents.\nStructures data in a way that classes of users find natural and intuitive.Just as a function (in programming) can provide abstraction, so can a database view. In another parallel with functions, database users can manipulate nested views, thus one view can aggregate data from other views. Without the use of views, the normalization of databases above second normal form would become much more difficult. Views can make it easier to create lossless join decomposition.\nJust as rows in a base table lack any defined ordering, rows available through a view do not appear with any default sorting.  A view is a relational table, and the relational model defines a table as a set of rows. Since sets are not ordered \u2014 by definition \u2014 neither are the rows of a view. Therefore, an ORDER BY clause in the view definition is meaningless; the SQL standard (SQL:2003) does not allow an ORDER BY clause in the subquery of a CREATE VIEW command, just as it is refused in a CREATE TABLE statement. However, sorted data can be obtained from a view, in the same way as any other table \u2014 as part of a query statement on that view. Nevertheless, some DBMS (such as Oracle Database) do not abide by this SQL standard restriction.\n\n"
    },
    {
      "id": "47393845",
      "title": "Wide-column store",
      "url": "https://en.wikipedia.org/wiki/Wide-column_store",
      "summary": "A wide-column store (or extensible record store) is a column-oriented DBMS and therefore a special type of NoSQL database. It uses tables, rows, and columns, but unlike a relational database, the names and format of the columns can vary from row to row in the same table. A wide-column store can be interpreted as a two-dimensional key\u2013value store.Google's Bigtable is one of the prototypical examples of a wide-column store.\n\n"
    },
    {
      "id": "32238027",
      "title": "XQuery API for Java",
      "url": "https://en.wikipedia.org/wiki/XQuery_API_for_Java",
      "summary": "XQuery API for Java (XQJ) refers to the common Java API for the W3C XQuery 1.0 specification.\nThe XQJ API enables Java programmers to execute XQuery against an XML data source (e.g. an XML database) while reducing or eliminating vendor lock in.\nThe XQJ API provides Java developers with an interface to the XQuery Data Model. Its design is similar to the JDBC API which has a client/server feel and as such lends itself well to Server-based XML Databases and less well to client-side XQuery processors, although the \"connection\" part is a very minor part of the entire API. Users of the XQJ API can bind Java values to XQuery expressions, preventing code injection attacks. Also, multiple XQuery expressions can be executed as part of an atomic transaction.\n\n"
    },
    {
      "id": "24714635",
      "title": "AAAI Conference on Artificial Intelligence",
      "url": "https://en.wikipedia.org/wiki/AAAI_Conference_on_Artificial_Intelligence",
      "summary": "The AAAI Conference on Artificial Intelligence (AAAI) is one of the leading international academic conference in artificial intelligence held annually. It ranks 4th in terms of H5 Index in Google Scholar's list of top AI publications, after ICLR, NeurIPS, and ICML. It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.During AAAI-20 conference, AI pioneers and 2018 Turing Award winners Yann LeCun and Yoshua Bengio among eight other researchers were honored as the AAAI 2020 Fellows.Along with other conferences such as NeurIPS, ICML, AAAI uses artificial intelligence algorithm to assign papers to reviewers."
    },
    {
      "id": "3192516",
      "title": "ACM Computing Classification System",
      "url": "https://en.wikipedia.org/wiki/ACM_Computing_Classification_System",
      "summary": "The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area."
    },
    {
      "id": "24559562",
      "title": "ACM Computing Surveys",
      "url": "https://en.wikipedia.org/wiki/ACM_Computing_Surveys",
      "summary": "ACM Computing Surveys is peer-reviewed quarterly scientific journal and is published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.According to the Journal Citation Reports, the journal has a 2022 impact factor of 16.6. In a 2008 ranking of computer science journals, ACM Computing Surveys received the highest rank \"A*\".\n\n"
    },
    {
      "id": "50785023",
      "title": "AI alignment",
      "url": "https://en.wikipedia.org/wiki/AI_alignment",
      "summary": "In the field of artificial intelligence (AI), AI alignment research aims to steer AI systems towards humans' intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances its intended objectives. A misaligned AI system pursues some objectives, but not the intended ones.It can be challenging for AI designers to align an AI system because it can be difficult for them to specify the full range of desired and undesired behavior. To avoid this difficulty, they typically use simpler proxy goals, such as gaining human approval. But that approach can create loopholes, overlook necessary constraints, or reward the AI system for merely appearing aligned.Misaligned AI systems can malfunction or cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful ways (reward hacking). They may also develop unwanted instrumental strategies, such as seeking power or survival, because such strategies help them achieve their final given goals. Furthermore, they may develop undesirable emergent goals that may be hard to detect before the system is in deployment, where it faces new situations and data distributions.Today, these problems affect existing commercial systems such as language models, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected, since these problems partially result from the systems being highly capable.Many leading AI scientists, including Geoffrey Hinton and Stuart Russell, argue that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI) and could endanger human civilization if misaligned.AI alignment is a subfield of AI safety, the study of how to build safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, avoiding deceptive AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and the social sciences."
    },
    {
      "id": "75934668",
      "title": "AI era",
      "url": "https://en.wikipedia.org/wiki/AI_era",
      "summary": "The AI era, also known as the AI revolution, is the ongoing period of global transition of the human economy and society towards post-scarcity economics and post-labor society enabled by artificial intelligence. Many have suggested that this period started around the early 2020s, with the release of generative AI models including large language models such as ChatGPT, which replicated aspects of human cognition, reasoning, attention, creativity and general intelligence commonly associated with human abilities. This enabled software programs that were capable of replacing or augmenting humans in various domains that traditionally required human reasoning and cognition, such as writing, translation, and computer programming.The AI revolution could mark a major turning point in human history, comparable to the invention of the Internet, electricity and the Industrial Revolution, technologies which have impacted virtually every industry and facet of life. It may also mark a major turning point in the billions of years of history of life and evolution towards the development of artificial life, artificial general intelligence and superintelligence, comparable to the Cambrian explosion or the evolution of multicellular life.\n\n"
    },
    {
      "id": "72360809",
      "title": "AI safety",
      "url": "https://en.wikipedia.org/wiki/AI_safety",
      "summary": "AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety."
    },
    {
      "id": "813176",
      "title": "AI takeover",
      "url": "https://en.wikipedia.org/wiki/AI_takeover",
      "summary": "An AI takeover is a scenario in which artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Stories of AI takeovers are very popular throughout science fiction. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.\n\n"
    },
    {
      "id": "3548574",
      "title": "AI winter",
      "url": "https://en.wikipedia.org/wiki/AI_winter",
      "summary": "In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research.  The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or even decades later.\nThe term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the \"American Association of Artificial Intelligence\"). Roger Schank and Marvin Minsky\u2014two leading AI researchers who experienced the \"winter\" of the 1970s\u2014warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. They described a chain reaction, similar to a \"nuclear winter\", that would begin with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. Three years later the billion-dollar AI industry began to collapse.\nThere were two major winters approximately 1974\u20131980 and 1987\u20132000 and several smaller episodes, including the following:\n\n1966: failure of machine translation\n1969: criticism of perceptrons (early, single-layer artificial neural networks)\n1971\u201375: DARPA's frustration with the Speech Understanding Research program at Carnegie Mellon University\n1973: large decrease in AI research in the United Kingdom in response to the Lighthill report\n1973\u201374: DARPA's cutbacks to academic AI research in general\n1987: collapse of the LISP machine market\n1988: cancellation of new spending on AI by the Strategic Computing Initiative\n1990s: many expert systems were abandoned\n1990s: end of the Fifth Generation computer project's original goalsEnthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment, leading to the current (as of 2023) AI boom.\n\n"
    },
    {
      "id": "3874825",
      "title": "AT&T Labs",
      "url": "https://en.wikipedia.org/wiki/AT%26T_Labs",
      "summary": "AT&T Labs, Inc. is the research & development division of AT&T, the telecommunications company. It employs some 1,800 people in various locations, including: Bedminster NJ; Middletown, NJ; Manhattan, NY; Warrenville, IL; Austin, TX; Dallas, TX; Atlanta, GA; San Francisco, CA; San Ramon, CA; and Redmond, WA. The main research division, made up of around 450 people, is based across the Bedminster, Middletown, San Francisco, and Manhattan locations.\nAT&T Labs traces its history from AT&T Bell Labs. Much research is in areas traditionally associated with networks and systems, ranging from the physics of optical transmission to foundational topics in computing and communications. Other research areas address the technical challenges of large operational networks and the resulting large data sets."
    },
    {
      "id": "5033373",
      "title": "Action selection",
      "url": "https://en.wikipedia.org/wiki/Action_selection",
      "summary": "Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats\u2014artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\nOne problem for understanding action selection is determining the level of abstraction used for specifying an \"act\". At the most basic level of abstraction, an atomic act could be anything from contracting a muscle cell to provoking a war. Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed.\nMost researchers working in this field place high demands on their agents:\n\nThe acting agent typically must select its action in dynamic and unpredictable environments.\nThe agents typically act in real time; therefore they must make decisions in a timely fashion.\nThe agents are normally created to perform several different tasks. These tasks may conflict for resource allocation (e.g. can the agent put out a fire and deliver a cup of coffee at the same time?)\nThe environment the agents operate in may include humans, who may make things more difficult for the agent (either intentionally or by attempting to assist.)\nThe agents themselves are often intended to model animals or humans, and animal/human behaviour is quite complicated.For these reasons action selection is not trivial and attracts a good deal of research.\n\n"
    },
    {
      "id": "14179835",
      "title": "Activation function",
      "url": "https://en.wikipedia.org/wiki/Activation_function",
      "summary": "The activation function of a node in an artificial neural network is a function that calculates the output of the node based on its individual inputs and their weights. Nontrivial problems can be solved using only a few nodes if the activation function is nonlinear. Modern activation functions include the smooth version of the ReLU, the GELU, which was used in the 2018 BERT model, the logistic (sigmoid) function used in the 2012 speech recognition model developed by Hinton et al, the ReLU used in the 2012 AlexNet computer vision model and in the 2015 ResNet model. \n\n"
    },
    {
      "id": "28801798",
      "title": "Active learning (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Active_learning_(machine_learning)",
      "summary": "Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary.  In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.\nThere are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer.Large-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.\n\n"
    },
    {
      "id": "23924341",
      "title": "Adaptive website",
      "url": "https://en.wikipedia.org/wiki/Adaptive_website",
      "summary": "An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.\n\n"
    },
    {
      "id": "45049676",
      "title": "Adversarial machine learning",
      "url": "https://en.wikipedia.org/wiki/Adversarial_machine_learning",
      "summary": "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.Most machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption.\nMost common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction."
    },
    {
      "id": "233942",
      "title": "Affective computing",
      "url": "https://en.wikipedia.org/wiki/Affective_computing",
      "summary": "Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science. While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing and her book Affective Computing published by MIT Press. One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions."
    },
    {
      "id": "627",
      "title": "Agriculture",
      "url": "https://en.wikipedia.org/wiki/Agriculture",
      "summary": "Agriculture encompasses crop and livestock production, aquaculture, fisheries, and forestry for food and non-food products. Agriculture was the key development in the rise of sedentary human civilization, whereby farming of domesticated species created food surpluses that enabled people to live in cities. While humans started gathering grains at least 105,000 years ago, nascent farmers only began planting them around 11,500 years ago. Sheep, goats, pigs, and cattle were domesticated around 10,000 years ago. Plants were independently cultivated in at least 11 regions of the world. In the 20th century, industrial agriculture based on large-scale monocultures came to dominate agricultural output.\nAs of 2021, small farms produce about one-third of the world's food, but large farms are prevalent. The largest 1% of farms in the world are greater than 50 hectares (120 acres) and operate more than 70% of the world's farmland. Nearly 40% of agricultural land is found on farms larger than 1,000 hectares (2,500 acres). However, five of every six farms in the world consist of less than 2 hectares (4.9 acres) and take up only around 12% of all agricultural land. Farms and farming greatly influence rural economics and greatly shape rural society, effecting both the direct agricultural workforce and broader businesses that support the farms and farming populations.\nThe major agricultural products can be broadly grouped into foods, fibers, fuels, and raw materials (such as rubber). Food classes include cereals (grains), vegetables, fruits, cooking oils, meat, milk, eggs, and fungi. Global agricultural production amounts to approximately 11 billion tonnes of food, 32 million tonnes of natural fibres and 4 billion m3 of wood. However, around 14% of the world's food is lost from production before reaching the retail level.Modern agronomy, plant breeding, agrochemicals such as pesticides and fertilizers, and technological developments have sharply increased crop yields, but also contributed to ecological and environmental damage. Selective breeding and modern practices in animal husbandry have similarly increased the output of meat, but have raised concerns about animal welfare and environmental damage. Environmental issues include contributions to climate change, depletion of aquifers, deforestation, antibiotic resistance, and other agricultural pollution. Agriculture is both a cause of and sensitive to environmental degradation, such as biodiversity loss, desertification, soil degradation, and climate change, all of which can cause decreases in crop yield. Genetically modified organisms are widely used, although some countries ban them."
    },
    {
      "id": "5906926",
      "title": "Alan Mackworth",
      "url": "https://en.wikipedia.org/wiki/Alan_Mackworth",
      "summary": "Alan Mackworth is a professor emeritus in the Department of Computer Science at the University of British Columbia. He is known as \"The Founding Father\" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI) and former Canada Research Chair in Artificial Intelligence from 2001 to 2014.\n\n"
    },
    {
      "id": "50568903",
      "title": "Alex Graves (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Alex_Graves_(computer_scientist)",
      "summary": "Alex Graves is a computer scientist. Before working as a research scientist at DeepMind, he earned a BSc in Theoretical Physics from the University of Edinburgh and a PhD in artificial intelligence under J\u00fcrgen Schmidhuber at IDSIA. He was also a postdoc under Schmidhuber at the Technical University of Munich and under Geoffrey Hinton at the University of Toronto.\nAt IDSIA, Graves trained long short-term memory neural networks by a novel method called connectionist temporal classification (CTC). This method outperformed traditional speech recognition models in certain applications. In 2009, his CTC-trained LSTM was the first recurrent neural network to win pattern recognition contests, winning several competitions in connected handwriting recognition.Google uses CTC-trained LSTM for speech recognition on the smartphone.Graves is also the creator of neural Turing machines and the closely related differentiable neural computer.\nIn 2023, he published the paper Bayesian Flow Networks.\n\n"
    },
    {
      "id": "55817338",
      "title": "Algorithmic bias",
      "url": "https://en.wikipedia.org/wiki/Algorithmic_bias",
      "summary": "Algorithmic bias describes systematic and repeatable errors in a computer system that create \"unfair\" outcomes, such as \"privileging\" one category over another in ways different from the intended function of the algorithm.\nBias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination. This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (2018) and the proposed Artificial Intelligence Act (2021).\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\n\n"
    },
    {
      "id": "145128",
      "title": "Algorithmic efficiency",
      "url": "https://en.wikipedia.org/wiki/Algorithmic_efficiency",
      "summary": "In computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\nFor maximum efficiency it is desirable to minimize resource usage. However, different resources such as time and space complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important.\nFor example, bubble sort and timsort are both algorithms to sort a list of items from smallest to largest. Bubble sort sorts the list in time proportional to the number of elements squared (\n  \n    \n      \n        O\n        (\n        \n          n\n          \n            2\n          \n        \n        )\n      \n    \n    {\\textstyle O(n^{2})}\n  , see Big O notation), but only requires a small amount of extra memory which is constant with respect to the length of the list (\n  \n    \n      \n        O\n        (\n        1\n        )\n      \n    \n    {\\textstyle O(1)}\n  ). Timsort sorts the list in time linearithmic (proportional to a quantity times its logarithm) in the list's length (\n  \n    \n      \n        O\n        (\n        n\n        log\n        \u2061\n        n\n        )\n      \n    \n    {\\textstyle O(n\\log n)}\n  ), but has a space requirement linear in the length of the list (\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\textstyle O(n)}\n  ). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice."
    },
    {
      "id": "52773150",
      "title": "Algorithmic transparency",
      "url": "https://en.wikipedia.org/wiki/Algorithmic_transparency",
      "summary": "Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\nThe phrases \"algorithmic transparency\" and \"algorithmic accountability\" are sometimes used interchangeably \u2013 especially since they were coined by the same people \u2013 but they have subtly different meanings. Specifically, \"algorithmic transparency\" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair.  \"Algorithmic accountability\" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.Current research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a \"right to explanation\" of decisions made by algorithms, though it is unclear what this means. Furthermore, the European Union founded The European Center for Algoritmic Transparency (ECAT).\n\n"
    },
    {
      "id": "59766171",
      "title": "AlphaFold",
      "url": "https://en.wikipedia.org/wiki/AlphaFold",
      "summary": "AlphaFold is an artificial intelligence (AI) program developed by DeepMind, a subsidiary of Alphabet, which performs predictions of protein structure. The program is designed as a deep learning system.AlphaFold software has had two major versions. A team of researchers that used AlphaFold 1 (2018) placed first in the overall rankings of the 13th Critical Assessment of Structure Prediction (CASP) in December 2018. The program was particularly successful at predicting the most accurate structure for targets rated as the most difficult by the competition organisers, where no existing template structures were available from proteins with a partially similar sequence. A team that used AlphaFold 2 (2020) repeated the placement in the CASP14 competition in November 2020. The team achieved a level of accuracy much higher than any other group. It scored above 90 for around two-thirds of the proteins in CASP's global distance test (GDT), a test that measures the degree to which a computational program predicted structure is similar to the lab experiment determined structure, with 100 being a complete match, within the distance cutoff used for calculating GDT.AlphaFold 2's results at CASP14 were described as \"astounding\" and \"transformational.\" Some researchers noted that the accuracy is not high enough for a third of its predictions, and that it does not reveal the mechanism or rules of protein folding for the protein folding problem to be considered solved. Nevertheless, there has been widespread respect for the technical achievement, and analysis suggests that AlphaFold 2 is accurate enough to predict even single-mutation effects. On 15 July 2021 the AlphaFold 2 paper was published in Nature as an advance access publication alongside open source software and a searchable database of species proteomes. A more advanced version of AlphaFold is currently under development. It allows modeling of protein complexes with nucleic acids, small ligands, ions, and modified residues.\n\n"
    },
    {
      "id": "55981499",
      "title": "AlphaZero",
      "url": "https://en.wikipedia.org/wiki/AlphaZero",
      "summary": "AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.  This algorithm uses an approach similar to AlphaGo Zero. \nOn December 5, 2017, the DeepMind team released a preprint paper introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, Elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use. AlphaZero was trained solely via self-play using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws). The trained algorithm played on a single machine with four TPUs. \nDeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018; however, the AlphaZero program itself has not been made available to the public. In 2019, DeepMind published a new paper detailing MuZero, a new algorithm able to generalise AlphaZero's work, playing both Atari and board games without knowledge of the rules or representations of the game.\n\n"
    },
    {
      "id": "1691376",
      "title": "Amazon Web Services",
      "url": "https://en.wikipedia.org/wiki/Amazon_Web_Services",
      "summary": "Amazon Web Services, Inc. (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Clients will often use this in combination with autoscaling (a process that allows a client to use more computing in times of high application usage, and then scale down to reduce costs when there is less traffic). These cloud computing web services provide various services related to networking, compute, storage, middleware, IoT and other processing capacity, as well as software tools via AWS server farms.  This frees clients from managing, scaling, and patching hardware and operating systems. \nOne of the foundational services is Amazon Elastic Compute Cloud (EC2), which allows users to have at their disposal a virtual cluster of computers, with extremely high availability, which can be interacted with over the internet via REST APIs, a CLI or the AWS console.  AWS's virtual computers emulate most of the attributes of a real computer, including hardware central processing units (CPUs) and graphics processing units (GPUs) for processing; local/RAM memory; Hard-disk(HDD)/SSD storage; a choice of operating systems; networking; and pre-loaded application software such as web servers, databases, and customer relationship management (CRM).\nAWS services are delivered to customers via a network of AWS server farms located throughout the world. Fees are based on a combination of usage (known as a \"Pay-as-you-go\" model), hardware, operating system, software, or networking features chosen by the subscriber require availability, redundancy, security, and service options. Subscribers can pay for a single virtual AWS computer, a dedicated physical computer, or clusters of either. Amazon provides select portions of security for subscribers (e.g. physical security of the data centers) while other aspects of security are the responsibility of the subscriber (e.g. account management, vulnerability scanning, patching). AWS operates from many global geographical regions including seven in North America.Amazon markets AWS to subscribers as a way of obtaining large-scale computing capacity more quickly and cheaply than building an actual physical server farm. All services are billed based on usage, but each service measures usage in varying ways. As of 2021 Q4, AWS has 33% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 21%, and 10% respectively, according to Synergy Group.\n\n"
    },
    {
      "id": "2230",
      "title": "Analysis of algorithms",
      "url": "https://en.wikipedia.org/wiki/Analysis_of_algorithms",
      "summary": "In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms\u2014the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.\nThe term \"analysis of algorithms\" was coined by Donald Knuth. Algorithm analysis is an important part of a broader computational complexity theory, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms.\nIn theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end. For instance, binary search is said to run in a number of steps proportional to the logarithm of the size n of the sorted list being searched, or in O(log n), colloquially \"in logarithmic time\". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency. However the efficiencies of any two \"reasonable\" implementations of a given algorithm are related by a constant multiplicative factor  called a hidden constant.\nExact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called model of computation. A model of computation may be defined in terms of an abstract computer, e.g. Turing machine, and/or by postulating that certain operations are executed in unit time.\nFor example, if the sorted list to which we apply binary search has n elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log2(n) + 1 time units are needed to return an answer.\n\n"
    },
    {
      "id": "15345868",
      "title": "Angoss",
      "url": "https://en.wikipedia.org/wiki/Angoss",
      "summary": "Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC.In June 2013, the private equity firm Peterson Partners acquired Angoss for $8.4 million.\n\n"
    },
    {
      "id": "8190902",
      "title": "Anomaly detection",
      "url": "https://en.wikipedia.org/wiki/Anomaly_detection",
      "summary": "In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.\nThree broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.\n\n"
    },
    {
      "id": "6206236",
      "title": "Anthropic",
      "url": "https://en.wikipedia.org/wiki/Anthropic",
      "summary": "Anthropic PBC is an American artificial intelligence (AI) startup company, founded by former members of OpenAI. Anthropic develops general AI systems and large language models. It is a public-benefit corporation, and has been connected to the effective altruism movement.\nAs of July 2023, Anthropic had raised US$1.5 billion in funding. In September, Amazon announced an investment of up to US$4 billion, followed by a $2 billion commitment from Google the following month."
    },
    {
      "id": "18706674",
      "title": "Apache Mahout",
      "url": "https://en.wikipedia.org/wiki/Apache_Mahout",
      "summary": "Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark. Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.\n\n"
    },
    {
      "id": "49338480",
      "title": "Apache SystemDS",
      "url": "https://en.wikipedia.org/wiki/Apache_SystemDS",
      "summary": "Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. \nSystemDS's distinguishing characteristics are:\n\nAlgorithm customizability via R-like and Python-like languages.\nMultiple execution modes, including Standalone, Spark Batch, Spark MLContext, Hadoop Batch, and JMLC.\nAutomatic optimization based on data and cluster characteristics to ensure both efficiency and scalability.\n\n"
    },
    {
      "id": "2571015",
      "title": "Application security",
      "url": "https://en.wikipedia.org/wiki/Application_security",
      "summary": "Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.Web application security is a branch of information security that deals specifically with the security of websites, web applications, and web services. At a high level, web application security draws on the principles of application security but applies them specifically to the internet and web systems.Web Application Security Tools are specialized tools for working with HTTP traffic, e.g., Web application firewalls.\n\n"
    },
    {
      "id": "15893057",
      "title": "Applications of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
      "summary": "Artificial intelligence (AI) has been used in applications throughout industry and academia. Similar to electricity or computers, AI serves as a general-purpose technology that has numerous applications. Its applications span language translation, image recognition, credit scoring, e-commerce and various other domains.\n\n"
    },
    {
      "id": "19463198",
      "title": "Apprenticeship learning",
      "url": "https://en.wikipedia.org/wiki/Apprenticeship_learning",
      "summary": "In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert. It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.\n\n"
    },
    {
      "id": "49277634",
      "title": "Approximate computing",
      "url": "https://en.wikipedia.org/wiki/Approximate_computing",
      "summary": "Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design. It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose. One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.\nThe key requirement in approximate computing is that approximation can be introduced only in non-critical data, since approximating critical data (e.g., control operations) can lead to disastrous consequences, such as program crash or erroneous output.\n\n"
    },
    {
      "id": "2052",
      "title": "Array (data structure)",
      "url": "https://en.wikipedia.org/wiki/Array_(data_structure)",
      "summary": "In computer science, an array is a data structure consisting of a collection of elements (values or variables), of same memory size, each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.\nFor example, an array of ten 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as ten words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i \u00d7 4).\nThe memory address of the first element of an array is called first address, foundation address, or base address.\nBecause the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called \"matrices\". In some cases the term \"vector\" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word \"table\" is sometimes used as a synonym of array.\nArrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.\nArrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.\nThe term \"array\" may also refer to an array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.\nThe term is also used, especially in the description of algorithms, to mean associative array or \"abstract array\", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays."
    },
    {
      "id": "10809677",
      "title": "Arthur Samuel (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)",
      "summary": "Arthur Lee Samuel (December 5, 1901 \u2013 July 29, 1990) was an American pioneer in the field of computer gaming and artificial intelligence. He popularized the term \"machine learning\" in 1959. The Samuel Checkers-playing Program was among the world's first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI). He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983."
    },
    {
      "id": "566680",
      "title": "Artificial Intelligence: A Modern Approach",
      "url": "https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach",
      "summary": "Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995 and the fourth edition of the book was released on 28 April 2020. It has been called \"the most popular artificial intelligence textbook in the world\", and is considered the standard text in the field of artificial intelligence. As of 2023, it is used in over 1500 universities worldwide, and it has over 59,000 citations on Google Scholar.\nThe book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.\n\n"
    },
    {
      "id": "586357",
      "title": "Artificial general intelligence",
      "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
      "summary": "An artificial general intelligence (AGI) is a hypothetical type of intelligent agent which, if realized, could learn to accomplish any intellectual task that human beings or animals can perform. Alternatively, AGI has been defined as an autonomous system that surpasses human capabilities in the majority of economically valuable tasks.Creating AGI is a primary goal of some artificial intelligence research and of companies such as OpenAI, DeepMind, and Anthropic. AGI is a common topic in science fiction and futures studies.\nThe timeline for AGI development remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; and a minority believe it may never be achieved. There is debate on the exact definition of AGI, and regarding whether modern large language models (LLMs) such as GPT-4 are early, incomplete forms of AGI.Contention exists over the potential for AGI to pose a threat to humanity; for example, OpenAI treats it as an existential risk, while others find the development of AGI to be too remote to present a risk.A 2020 survey identified 72 active AGI R&D projects spread across 37 countries."
    },
    {
      "id": "1589987",
      "title": "Artificial immune system",
      "url": "https://en.wikipedia.org/wiki/Artificial_immune_system",
      "summary": "In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving."
    },
    {
      "id": "60639760",
      "title": "Artificial intelligence art",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_art",
      "summary": "Artificial intelligence art is any visual artwork created through the use of artificial intelligence (AI) programs.Artists began to create AI art in the mid to late-20th century, when the discipline was founded. In the early 21st century, the availability of AI art tools to the general public increased, providing opportunities for use outside of academia and professional artists. Throughout its history, artificial intelligence art has raised many philosophical concerns, including those related to copyright, deception, and its impact on traditional artists, including their incomes. \n\n"
    },
    {
      "id": "59539440",
      "title": "Artificial intelligence in government",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_government",
      "summary": "Artificial intelligence (AI) has a range of uses in government. It can be used to further public policy objectives (in areas such as emergency services, health and welfare), as well as assist the public to interact with the government  (through the use of virtual assistants, for example). According to the Harvard Business Review, \"Applications of artificial intelligence to the public sector are broad and growing, with early experiments taking place around the world.\" Hila Mehr from the Ash Center for Democratic Governance and Innovation at Harvard University notes that AI in government is not new, with postal services using machine methods in the late 1990s to recognise handwriting on envelopes to automatically route letters. The use of AI in government comes with significant benefits, including efficiencies resulting in cost savings (for instance by reducing the number of front office staff), and reducing the opportunities for corruption. However, it also carries risks.\n\n"
    },
    {
      "id": "52588198",
      "title": "Artificial intelligence in healthcare",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare",
      "summary": "Artificial intelligence in healthcare is a term used to describe the use of machine-learning algorithms and software, or artificial intelligence (AI), to copy human cognition in the analysis, presentation, and understanding of complex medical and health care data, or to exceed human capabilities by providing new ways to diagnose, treat, or prevent disease. Specifically, AI is the ability of computer algorithms to approximate conclusions based solely on input data.\nThe primary aim of health-related AI applications is to analyze relationships between clinical data and patient outcomes. AI programs are applied to practices such as diagnostics, treatment protocol development, drug development, personalized medicine, and patient monitoring and care. What differentiates AI technology from traditional technologies in healthcare is the ability to gather larger and more diverse data, process it, and produce a well-defined output to the end-user. AI does this through machine learning algorithms and deep learning. Because radiographs are the most common imaging tests conducted in most radiology departments, the potential for AI to help with triage and interpretation of traditional radiographs (X-ray pictures) is particularly noteworthy. These processes can recognize patterns in behavior and create their own logic. To gain useful insights and predictions, machine learning models must be trained using extensive amounts of input data. AI algorithms behave differently from humans in two ways: (1) algorithms are literal: once a goal is set, the algorithm learns exclusively from the input data and can only understand what it has been programmed to do, (2) and some deep learning algorithms are black boxes; algorithms can predict with extreme precision, but offer little to no comprehensible explanation to the logic behind its decisions aside from the data and type of algorithm used.As widespread use of AI in healthcare is relatively new, research is ongoing into its application in various fields of medicine and industry. Additionally, greater consideration is being given to the unprecedented ethical concerns related to its practice such as data privacy, automation of jobs, and representation biases. Furthermore, new technologies brought about by AI in healthcare are often resisted by healthcare leaders, leading to slow and erratic adoption."
    },
    {
      "id": "54008163",
      "title": "Artificial intelligence in industry",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_industry",
      "summary": "Industrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, cost reduction, site optimization, predictive analysis  and insight discovery.Artificial intelligence and machine learning  have become key enablers to leverage data in production in recent years due to a number of different factors: More affordable sensors and the automated process of data acquisition; More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost; Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing.\n\n"
    },
    {
      "id": "72771661",
      "title": "Artificial intelligence in mental health",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_mental_health",
      "summary": "Artificial Intelligence (AI) in mental health refers to the use of advanced computational technologies and algorithms to enhance the understanding, diagnosis, and treatment of mental health disorders. \n\n"
    },
    {
      "id": "7872324",
      "title": "Artificial intelligence systems integration",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_systems_integration",
      "summary": "The core idea of artificial intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.\nMost artificial intelligence systems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that of speech recognition. However, in recent years, there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Th\u00f3risson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.\n\n"
    },
    {
      "id": "21523",
      "title": "Artificial neural network",
      "url": "https://en.wikipedia.org/wiki/Artificial_neural_network",
      "summary": "Artificial neural networks (ANNs, also shortened to neural networks (NNs) or neural nets) are a branch of machine learning models that are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains.An ANN is made of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. These are connected by edges, which model the synapses in a biological brain. An artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection.\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling, adaptive control, and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information."
    },
    {
      "id": "349771",
      "title": "Artificial neuron",
      "url": "https://en.wikipedia.org/wiki/Artificial_neuron",
      "summary": "An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks. The artificial neuron is a function that receives one or more inputs, applies weights to these inputs, and sums them to produce an output. \nThe design of the artificial neuron was inspired by neural circuitry. Its inputs are analogous to excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites, or activation, its weights are analogous to synaptic weight, and its output is analogous to a neuron's action potential which is transmitted along its axon.\nUsually, each input is separately weighted, and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU-like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.The artificial neuron transfer function should not be confused with a linear system's transfer function.\nSimple artificial neurons, such as the McCulloch\u2013Pitts model, are sometimes described as \"caricature models\", since they are intended to reflect one or more neurophysiological observations, but without regard to realism. Artificial neurons can also refer to artificial cells in neuromorphic engineering (see below) that are similar to natural physical neurons.\n\n"
    },
    {
      "id": "644058",
      "title": "Association for Computational Linguistics",
      "url": "https://en.wikipedia.org/wiki/Association_for_Computational_Linguistics",
      "summary": "The Association for Computational Linguistics (ACL) is a scientific and professional organization for people working on natural language processing. Its namesake conference is one of the primary high impact conferences for natural language processing research, along with EMNLP. The conference is held each summer in locations where significant computational linguistics research is carried out.\nIt was founded in 1962, originally named the Association for Machine Translation and Computational Linguistics (AMTCL). It became the ACL in 1968. The ACL has a European (EACL), a North American (NAACL), and an Asian (AACL) chapter.\n\n"
    },
    {
      "id": "577053",
      "title": "Association rule learning",
      "url": "https://en.wikipedia.org/wiki/Association_rule_learning",
      "summary": "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n  \n    \n      \n        {\n        \n          o\n          n\n          i\n          o\n          n\n          s\n          ,\n          p\n          o\n          t\n          a\n          t\n          o\n          e\n          s\n        \n        }\n        \u21d2\n        {\n        \n          b\n          u\n          r\n          g\n          e\n          r\n        \n        }\n      \n    \n    {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.\nIn addition to the above example from market basket analysis, association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\nThe association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand."
    },
    {
      "id": "28326718",
      "title": "Astroinformatics",
      "url": "https://en.wikipedia.org/wiki/Astroinformatics",
      "summary": "Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies. The field is closely related to astrostatistics.\n\n"
    },
    {
      "id": "66001552",
      "title": "Attention (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "summary": "Machine learning-based attention is a mechanism which intuitively mimicks cognitive attention. It calculates \"soft\" weights for each word, more precisely for its embedding, in the context window. These weights can be computed either in parallel (such as in transformers) or sequentially (such as recurrent neural networks). \"Soft\" weights can change during each runtime, in contrast to \"hard\" weights, which are (pre-)trained and fine-tuned and remain frozen afterwards.  \nAttention was developed to address the weaknesses of leveraging information from the hidden outputs of recurrent neural networks.  Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence is expected to be attenuated.  Attention allows the calculation of the hidden representation of a token equal access to any part of a sentence directly, rather than only through the previous hidden state.  \nEarlier uses attached this mechanism to a serial recurrent neural network's language translation system (below), but later uses in Transformers large language models removed the recurrent neural network and relied heavily on the faster parallel attention scheme."
    },
    {
      "id": "6836612",
      "title": "Autoencoder",
      "url": "https://en.wikipedia.org/wiki/Autoencoder",
      "summary": "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction.\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data)."
    },
    {
      "id": "103356",
      "title": "Automata theory",
      "url": "https://en.wikipedia.org/wiki/Automata_theory",
      "summary": "Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science with close connections to mathematical logic. The word automata comes from the Greek word \u03b1\u1f50\u03c4\u03cc\u03bc\u03b1\u03c4\u03bf\u03c2, which means \"self-acting, self-willed, self-moving\". An automaton (automata in plural) is an abstract self-propelled computing device which follows a predetermined sequence of operations automatically. An automaton with a finite number of states is called a Finite Automaton (FA) or Finite-State Machine (FSM). The figure on the right illustrates a finite-state machine, which is a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the previous state and current input symbol as its arguments.\nAutomata theory is closely related to formal language theory. In this context, automata are used as finite representations of formal languages that may be infinite. Automata are often classified by the class of formal languages they can recognize, as in the Chomsky hierarchy, which describes a nesting relationship between major classes of automata. Automata play a major role in the theory of computation, compiler construction, artificial intelligence, parsing and formal verification.\n\n"
    },
    {
      "id": "55843837",
      "title": "Automated machine learning",
      "url": "https://en.wikipedia.org/wiki/Automated_machine_learning",
      "summary": "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. \nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\n\n"
    },
    {
      "id": "9034035",
      "title": "Computer-aided diagnosis",
      "url": "https://en.wikipedia.org/wiki/Computer-aided_diagnosis",
      "summary": "Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, Endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.\nCAD also has potential future applications in digital pathology with the advent of whole-slide imaging and machine learning algorithms. So far its application has been limited to quantifying immunostaining but is also being investigated for the standard H&E stain.CAD is an interdisciplinary technology combining elements of artificial intelligence and computer vision with radiological and pathology image processing. A typical application is the detection of a tumor. For instance, some hospitals use CAD to support preventive medical check-ups in mammography (diagnosis of breast cancer), the detection of polyps in Colonoscopy, and lung cancer.\nComputer-aided detection (CADe) systems are usually confined to marking conspicuous structures and sections. Computer-aided diagnosis (CADx) systems evaluate the conspicuous structures. For example, in mammography CAD highlights microcalcification clusters and hyperdense structures in the soft tissue. This allows the radiologist to draw conclusions about the condition of the pathology. Another application is CADq, which quantifies, e.g., the size of a tumor or the tumor's behavior in contrast medium uptake. Computer-aided simple triage (CAST) is another type of CAD, which performs a fully automatic initial interpretation and triage of studies into some meaningful categories (e.g. negative and positive). CAST is particularly applicable in emergency diagnostic imaging, where a prompt diagnosis of critical, life-threatening condition is required.\nAlthough CAD has been used in clinical environments for over 40 years, CAD usually does not substitute the doctor or other professional, but rather plays a supporting role. The professional (generally a radiologist) is generally responsible for the final interpretation of a medical image. However, the goal of some CAD systems is to detect earliest signs of abnormality in patients that human professionals cannot, as in diabetic retinopathy, architectural distortion in mammograms, ground-glass nodules in thoracic CT, and non-polypoid (\u201cflat\u201d) lesions in CT colonography.\n\n"
    },
    {
      "id": "1505641",
      "title": "Automated planning and scheduling",
      "url": "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
      "summary": "Automated planning and scheduling, sometimes denoted as simply AI planning, is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\nIn known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages."
    },
    {
      "id": "2546",
      "title": "Automated theorem proving",
      "url": "https://en.wikipedia.org/wiki/Automated_theorem_proving",
      "summary": "Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science."
    },
    {
      "id": "734787",
      "title": "Automatic differentiation",
      "url": "https://en.wikipedia.org/wiki/Automatic_differentiation",
      "summary": "In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program.\nAutomatic differentiation exploits the fact that every computer calculation, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor of more arithmetic operations than the original program."
    },
    {
      "id": "245926",
      "title": "Self-driving car",
      "url": "https://en.wikipedia.org/wiki/Self-driving_car",
      "summary": "A self-driving car, also known as an autonomous car (AC), driverless car, or robotic car (robo-car), is a car that is capable of driving without human input. Self-driving cars are responsible all driving activities including perceiving the environment, monitoring important systems, and controlling the vehicle, including navigating from origin to destination.The perception system processes visual and audio data from outside and inside the car to create a local model of the vehicle, the road, traffic, traffic controls and other observable objects, and their relative motion. The control system then takes actions to move the vehicle, considering the local model, road map, and driving regulations.ACs have the potential to impact the automotive industry, mobility costs, health, welfare, urban planning, traffic, insurance, labor market, and other domains. Appropriate regulations are necessary to integrate ACs into the existing driving environment.\nAutonomous ground vehicle capabilities have been classified by SAE International (SAE J3016).Multiple vendors are pursuing autonomy, although as of early 2024, no system had achieved full autonomy. Waymo was the first to offer rides in self-driving taxis (\"robotaxis\") to the general public. It offers services in various US cities. Cruise offered taxi service in San Francisco, but suspended service in 2023. Honda was the first manufacturer to sell an SAE Level 3 car, followed by Mercedes-Benz, BMW Group and Kia. Nuro offers autonomous commercial delivery service in California. Palo Alto, California certified Nuro at Level 4. DeepRoute.ai launched a robotaxi service in Shenzhen in 2021."
    },
    {
      "id": "1434444",
      "title": "Autoregressive model",
      "url": "https://en.wikipedia.org/wiki/Autoregressive_model",
      "summary": "In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation. Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive\u2013moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.\nContrary to the moving-average (MA) model, the autoregressive model is not always stationary as it may contain a unit root."
    },
    {
      "id": "62026514",
      "title": "BERT (language model)",
      "url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
      "summary": "Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"BERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters, and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words).\n\n"
    },
    {
      "id": "83443",
      "title": "Birth",
      "url": "https://en.wikipedia.org/wiki/Birth",
      "summary": "Birth is the act or process of bearing or bringing forth offspring, also referred to in technical contexts as parturition. In mammals, the process is initiated by hormones which cause the muscular walls of the uterus to contract, expelling the fetus at a developmental stage when it is ready to feed and breathe.\nIn some species, the offspring is precocial and can move around almost immediately after birth but in others, it is altricial and completely dependent on parenting.\nIn marsupials, the fetus is born at a very immature stage after a short gestation and develops further in its mother's womb  pouch.\nIt is not only mammals that give birth. Some reptiles, amphibians, fish and invertebrates carry their developing young inside them. Some of these are ovoviviparous, with the eggs being hatched inside the mother's body, and others are viviparous, with the embryo developing inside their body, as in the case of mammals.\n\n"
    },
    {
      "id": "71889610",
      "title": "BLOOM (language model)",
      "url": "https://en.wikipedia.org/wiki/BLOOM_(language_model)",
      "summary": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model, as well as the code base and the data used to train it, are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was lead by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS), on which it was trained.\nBLOOM's training corpus, named ROOTS, combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages."
    },
    {
      "id": "360788",
      "title": "Backdoor (computing)",
      "url": "https://en.wikipedia.org/wiki/Backdoor_(computing)",
      "summary": "A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a \"homunculus computer\"\u2014a tiny computer-within-a-computer such as that found in Intel's AMT technology). Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptosystems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.\nA backdoor may take the form of a hidden part of a program, a separate program (e.g. Back Orifice may subvert the system through a rootkit), code in the firmware of the hardware, or parts of an operating system such as Windows. Trojan horses can be used to create vulnerabilities in a device. A Trojan horse may appear to be an entirely legitimate program, but when executed, it triggers an activity that may install a backdoor. Although some are secretly installed, other backdoors are deliberate and widely known. These kinds of backdoors have \"legitimate\" uses such as providing the manufacturer with a way to restore user passwords.\nMany systems that store information within the cloud fail to create accurate security measures. If many systems are connected within the cloud, hackers can gain access to all other platforms through the most vulnerable system. Default passwords (or other default credentials) can function as backdoors if they are not changed by the user. Some debugging features can also act as backdoors if they are not removed in the release version. In 1993, the United States government attempted to deploy an encryption system, the Clipper chip, with an explicit backdoor for law enforcement and national security access. The chip was unsuccessful.Recent proposals to counter backdoors include creating a database of backdoors' triggers and then using neural networks to detect them.\n\n"
    },
    {
      "id": "1360091",
      "title": "Backpropagation",
      "url": "https://en.wikipedia.org/wiki/Backpropagation",
      "summary": "In machine learning, backpropagation is a gradient estimation method used to train neural network models. The gradient estimate is used by the optimization algorithm to compute the network parameter updates.\nIt is an efficient application of the Leibniz chain rule (1673) to such networks. It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970). The term \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this, even though Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory.Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input\u2013output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming. Gradient descent, or variants such as stochastic gradient descent, are commonly used.\nStrictly the term backpropagation refers only to the algorithm for computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm \u2013 including how the gradient is used, such as by stochastic gradient descent. In 1986 David E. Rumelhart et al. published an experimental analysis of the technique. This contributed to the popularization of backpropagation and helped to initiate an active period of research in multilayer perceptrons."
    },
    {
      "id": "486777",
      "title": "Bank fraud",
      "url": "https://en.wikipedia.org/wiki/Bank_fraud",
      "summary": "Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution. In many instances, bank fraud is a criminal offence. \nWhile the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.\n\n"
    },
    {
      "id": "73363598",
      "title": "Gemini (chatbot)",
      "url": "https://en.wikipedia.org/wiki/Gemini_(chatbot)",
      "summary": "Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name and developed as a direct response to the meteoric rise of OpenAI's ChatGPT, it was launched in a limited capacity in March 2023 to lukewarm responses before expanding to other countries in May.\nThe chatbot was previously based on PaLM, and initially the LaMDA family of large language models. LaMDA was developed and announced in 2021, but was not released to the public out of an abundance of caution. OpenAI's launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard and sent them into a panic, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in February 2023; it took center stage during the 2023 Google I/O keynote in May, and was upgraded to the Gemini LLM in December. Bard and Duet AI were unified under the Gemini brand in February 2024."
    },
    {
      "id": "1274090",
      "title": "Basic Books",
      "url": "https://en.wikipedia.org/wiki/Basic_Books",
      "summary": "Basic Books is a book publisher founded in 1950 and located in New York City, now an imprint of Hachette Book Group. It publishes books in the fields of psychology, philosophy, economics, science, politics, sociology, current affairs, and history."
    },
    {
      "id": "50909",
      "title": "Basis function",
      "url": "https://en.wikipedia.org/wiki/Basis_function",
      "summary": "In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.\nIn numerical analysis and approximation theory, basis functions are also called blending functions, because of their use in interpolation: In this application, a mixture of the basis functions provides an interpolating function (with the \"blend\" depending on the evaluation of the basis functions at the data points).\n\n"
    },
    {
      "id": "19892153",
      "title": "Online machine learning",
      "url": "https://en.wikipedia.org/wiki/Online_machine_learning",
      "summary": "In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction. Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches."
    },
    {
      "id": "57222123",
      "title": "Batch normalization",
      "url": "https://en.wikipedia.org/wiki/Batch_normalization",
      "summary": "Batch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. However, at initialization, batch normalization in fact induces severe gradient explosion in deep networks, which is only alleviated by skip connections in residual networks. Others maintain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."
    },
    {
      "id": "49571",
      "title": "Bayesian inference",
      "url": "https://en.wikipedia.org/wiki/Bayesian_inference",
      "summary": "Bayesian inference ( BAY-zee-\u0259n or  BAY-zh\u0259n) is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Fundamentally, Bayesian inference uses prior knowledge, in the form of a prior distribution in order to estimate posterior probabilities. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called \"Bayesian probability\"."
    },
    {
      "id": "203996",
      "title": "Bayesian network",
      "url": "https://en.wikipedia.org/wiki/Bayesian_network",
      "summary": "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\nEfficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
    },
    {
      "id": "40973765",
      "title": "Bayesian optimization",
      "url": "https://en.wikipedia.org/wiki/Bayesian_optimization",
      "summary": "Bayesian optimization is a sequential design strategy for global optimization of black-box functions  that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions.\n\n"
    },
    {
      "id": "540801",
      "title": "Behaviorism",
      "url": "https://en.wikipedia.org/wiki/Behaviorism",
      "summary": "Behaviorism (also spelled behaviourism) is a systematic approach to understand the behavior of humans and other animals. It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli. Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events. The cognitive revolution of the late 20th century largely replaced behaviorism as an explanatory theory with cognitive psychology, which unlike behaviorism examines internal mental states.\nBehaviorism emerged in the early 1900s as a reaction to depth psychology and other traditional forms of psychology, which often had difficulty making predictions that could be tested experimentally, but derived from earlier research in the late nineteenth century, such as when Edward Thorndike pioneered the law of effect, a procedure that involved the use of consequences to strengthen or weaken behavior.\nWith a 1924 publication, John B. Watson devised methodological behaviorism, which rejected introspective methods and sought to understand behavior by only measuring observable behaviors and events. It was not until the 1930s that B. F. Skinner suggested that covert behavior\u2014including cognition and emotions\u2014is subject to the same controlling variables as observable behavior, which became the basis for his philosophy called radical behaviorism. While Watson and Ivan Pavlov investigated how (conditioned) neutral stimuli elicit reflexes in respondent conditioning, Skinner assessed the reinforcement histories of the discriminative (antecedent) stimuli that emits behavior; the technique became known as operant conditioning.\nThe application of radical behaviorism\u2014known as applied behavior analysis\u2014is used in a variety of contexts, including, for example, applied animal behavior and organizational behavior management to treatment of mental disorders, such as autism and substance abuse. In addition, while behaviorism and cognitive schools of psychological thought do not agree theoretically, they have complemented each other in the cognitive-behavior therapies, which have demonstrated utility in treating certain pathologies, including simple phobias, PTSD, and mood disorders."
    },
    {
      "id": "40678189",
      "title": "Bias\u2013variance tradeoff",
      "url": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff",
      "summary": "In statistics and machine learning, the bias\u2013variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the model's estimated parameters.\nThe bias\u2013variance dilemma or bias\u2013variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\nThe bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\nThe variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias\u2013variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.\n\n"
    },
    {
      "id": "205393",
      "title": "Binary classification",
      "url": "https://en.wikipedia.org/wiki/Binary_classification",
      "summary": "Binary classification is the task of classifying the elements of a set into one of two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:\n\nMedical testing to determine if a patient has certain disease or not;\nQuality control in industry, deciding whether a specification has been met;\nIn information retrieval, deciding whether a page should be in the result set of a search or not.Binary classification is dichotomization applied to a practical situation. In many practical binary classification problems, the two groups are not symmetric, and rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative).\n\n"
    },
    {
      "id": "4214",
      "title": "Bioinformatics",
      "url": "https://en.wikipedia.org/wiki/Bioinformatics",
      "summary": "Bioinformatics ( ) is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex. Bioinformatics uses biology, chemistry, physics, computer science, computer programming, information engineering, mathematics and statistics to analyze and interpret biological data. The subsequent process of analyzing and interpreting data is referred to as computational biology. \nComputational, statistical, and computer programming techniques have been used  for computer simulation analyses of biological queries. They include reused specific analysis \"pipelines\", particularly in the field of genomics, such as by the identification of genes and single nucleotide polymorphisms (SNPs). These pipelines are used to better understand the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. Bioinformatics also includes proteomics, which tries to understand the organizational principles within nucleic acid and protein sequences.Image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics, it aids in sequencing and annotating genomes and their observed mutations. Bioinformatics includes text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in comparing, analyzing and interpreting genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions."
    },
    {
      "id": "1729542",
      "title": "Neural network",
      "url": "https://en.wikipedia.org/wiki/Neural_network",
      "summary": "A neural network, also called a biological neural network, is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems.\nArtificial neural networks are machine learning models inspired by biological neural networks. They consist of artificial neurons, which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits.\n\n"
    },
    {
      "id": "12207",
      "title": "Geology",
      "url": "https://en.wikipedia.org/wiki/Geology",
      "summary": "Geology (from Ancient Greek  \u03b3\u1fc6 (g\u00ea) 'earth', and  \u03bbo\u03b3\u03af\u03b1 (-log\u00eda) 'study of, discourse') is a branch of natural science concerned with the Earth and other astronomical objects, the rocks of which they are composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology. It is integrated with Earth system science and planetary science.\nGeology describes the structure of the Earth on and beneath its surface and the processes that have shaped that structure. Geologists study the mineralogical composition of rocks in order to get insight into their history of formation. Geology determines the relative ages of rocks found at a given location; geochemistry (a branch of geology) determines their absolute ages. By combining various petrological, crystallographic, and paleontological tools, geologists are able to chronicle the geological history of the Earth as a whole. One aspect is to demonstrate the age of the Earth. Geology provides evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.\nGeologists broadly study the properties and processes of Earth and other terrestrial planets. Geologists use a wide variety of methods to understand the Earth's structure and evolution, including fieldwork, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding natural hazards, remediating environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.\n\n"
    },
    {
      "id": "90500",
      "title": "Boosting (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Boosting_(machine_learning)",
      "summary": "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\nRobert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm [\u2026] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.\n\n"
    },
    {
      "id": "1307911",
      "title": "Bootstrap aggregating",
      "url": "https://en.wikipedia.org/wiki/Bootstrap_aggregating",
      "summary": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach."
    },
    {
      "id": "19009110",
      "title": "Rain",
      "url": "https://en.wikipedia.org/wiki/Rain",
      "summary": "Rain is water droplets that have condensed from atmospheric water vapor and then fall under gravity. Rain is a major component of the water cycle and is responsible for depositing most of the fresh water on the Earth. It provides water for hydroelectric power plants, crop irrigation, and suitable conditions for many types of ecosystems.\nThe major cause of rain production is moisture moving along three-dimensional zones of temperature and moisture contrasts known as weather fronts. If enough moisture and upward motion is present, precipitation falls from convective clouds (those with strong upward vertical motion) such as cumulonimbus (thunder clouds) which can organize into narrow rainbands. In mountainous areas, heavy precipitation is possible where upslope flow is maximized within windward sides of the terrain at elevation which forces moist air to condense and fall out as rainfall along the sides of mountains. On the leeward side of mountains, desert climates can exist due to the dry air caused by downslope flow which causes heating and drying of the air mass. The movement of the monsoon trough, or intertropical convergence zone, brings rainy seasons to savannah climes.\nThe urban heat island effect leads to increased rainfall, both in amounts and intensity, downwind of cities. Global warming is also causing changes in the precipitation pattern globally, including wetter conditions across eastern North America and drier conditions in the tropics. Antarctica is the driest continent. The globally averaged annual precipitation over land is 715 mm (28.1 in), but over the whole Earth, it is much higher at 990 mm (39 in). Climate classification systems such as the K\u00f6ppen classification system use average annual rainfall to help differentiate between differing climate regimes. Rainfall is measured using rain gauges. Rainfall amounts can be estimated by weather radar.\n\n"
    },
    {
      "id": "623686",
      "title": "Brain\u2013computer interface",
      "url": "https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface",
      "summary": "A brain\u2013computer interface (BCI), sometimes called a brain\u2013machine interface (BMI) or smartbrain, is a direct communication pathway between the brain's electrical activity and an external device, most commonly a computer or robotic limb. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions. They are often conceptualized as a human\u2013machine interface that skips the intermediary component of the physical movement of body parts, although they also raise the possibility of the erasure of the discreteness of brain and machine. Implementations of BCIs range from non-invasive (EEG, MEG, MRI) and partially invasive (ECoG and endovascular) to invasive (microelectrode array), based on how close electrodes get to brain tissue.Research on BCIs began in the 1970s by Jacques Vidal at the University of California, Los Angeles (UCLA) under a grant from the National Science Foundation, followed by a contract from DARPA. Vidal's 1973 paper marks the first appearance of the expression brain\u2013computer interface in scientific literature.\nDue to the cortical plasticity of the brain, signals from implanted prostheses can, after adaptation, be handled by the brain like natural sensor or effector channels. Following years of animal experimentation, the first neuroprosthetic devices implanted in humans appeared in the mid-1990s.\nRecently, studies in human-computer interaction via the application of machine learning to statistical temporal features extracted from the frontal lobe (EEG brainwave) data has had high levels of success in classifying mental states (relaxed, neutral, concentrating), mental emotional states (negative, neutral, positive), and thalamocortical dysrhythmia."
    },
    {
      "id": "22643107",
      "title": "CURE algorithm",
      "url": "https://en.wikipedia.org/wiki/CURE_algorithm",
      "summary": "CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances."
    },
    {
      "id": "53631046",
      "title": "Caffe (software)",
      "url": "https://en.wikipedia.org/wiki/Caffe_(software)",
      "summary": "Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.\n\n"
    },
    {
      "id": "363900",
      "title": "Canonical correlation",
      "url": "https://en.wikipedia.org/wiki/Canonical_correlation",
      "summary": "In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y that have a maximum correlation with each other. T. R. Knapp notes that \"virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables.\" The method was first introduced by Harold Hotelling in 1936, although in the context of angles between flats the mathematical concept was published by Jordan in 1875.CCA is now a cornerstone of multivariate statistics and multi-view learning, and a great number of interpretations and extensions have been proposed, such as probabilistic CCA, sparse CCA, multi-view CCA, and Deep CCA. Unfortunately, perhaps because of its popularity, the literature can be inconsistent with notation, we attempt to highlight such inconsistencies in this article to help the reader make best use of the existing literature and techniques available.\nLike its sister method PCA, CCA can be viewed in population form (corresponding to random vectors and their covariance matrices) or in sample form (corresponding to datasets and their sample covariance matrices). These two forms are almost exact analogues of each other, which is why their distinction is often overlooked, but they can behave very differently in high dimensional settings. We next give explicit mathematical definitions for the population problem and highlight the different objects in the so-called canonical decomposition - understanding the differences between this objects is crucial for interpretation of the technique."
    },
    {
      "id": "1012548",
      "title": "Chartered Financial Analyst",
      "url": "https://en.wikipedia.org/wiki/Chartered_Financial_Analyst",
      "summary": "The Chartered Financial Analyst (CFA) program is a postgraduate professional certification offered internationally by the US-based CFA Institute (formerly the Association for Investment Management and Research, or AIMR) to investment and financial professionals. The program teaches a wide range of subjects relating to advanced investment analysis\u2014including security analysis, statistics, probability theory, fixed income, derivatives, economics, financial analysis, corporate finance, alternative investments, portfolio management\u2014and provides a generalist knowledge of other areas of finance.\nA candidate who successfully completes the program and meets other professional requirements is awarded the \"CFA charter\" and becomes a \"CFA charter-holder\". As of November 2022, at least 190,000 people are charter-holders globally, growing 6% annually since 2012 (including effects of the pandemic). Successful candidates take an average of four years to earn their CFA charter.The top employers of CFA charter-holders globally include JPMorgan Chase, UBS, Royal Bank of Canada, and Bank of America.\n\n"
    },
    {
      "id": "27809",
      "title": "Chemical synapse",
      "url": "https://en.wikipedia.org/wiki/Chemical_synapse",
      "summary": "Chemical synapses are biological junctions through which neurons' signals can be sent to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.\nAt a chemical synapse, one neuron releases neurotransmitter molecules into a small space (the synaptic cleft) that is adjacent to another neuron. The neurotransmitters are contained within small sacs called synaptic vesicles, and are released into the synaptic cleft by exocytosis. These molecules then bind to neurotransmitter receptors on the postsynaptic cell. Finally, the neurotransmitters are cleared from the synapse through one of several potential mechanisms including enzymatic degradation or re-uptake by specific transporters either on the presynaptic cell or on some other neuroglia to terminate the action of the neurotransmitter.\nThe adult human brain is estimated to contain from 1014 to 5 \u00d7 1014 (100\u2013500 trillion) synapses. Every cubic millimeter of cerebral cortex contains roughly a billion (short scale, i.e. 109) of them. The number of synapses in the  human cerebral cortex has separately been estimated at 0.15 quadrillion (150 trillion)The word \"synapse\" was introduced by Sir Charles Scott Sherrington in 1897. Chemical synapses are not the only type of biological synapse: electrical and immunological synapses also exist. Without a qualifier, however, \"synapse\" commonly refers to chemical synapses."
    },
    {
      "id": "575697",
      "title": "Cheminformatics",
      "url": "https://en.wikipedia.org/wiki/Cheminformatics",
      "summary": "Cheminformatics (also known as chemoinformatics) refers to the use of physical chemistry theory with computer and information science techniques\u2014so called \"in silico\" techniques\u2014in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.\n\n"
    },
    {
      "id": "72752788",
      "title": "Chinchilla AI",
      "url": "https://en.wikipedia.org/wiki/Chinchilla_AI",
      "summary": "Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March of 2022. It is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models, it has been determined that if one doubles the model size, one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla AI by DeepMind. Similar to Gopher in terms of cost, Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding), which is 7% higher than Gopher\u2019s performance. Chinchilla AI is still in the testing phase as of January 12, 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling, meaning that using larger, higher-quality training datasets can lead to better results on downstream tasks."
    },
    {
      "id": "6216",
      "title": "Chinese room",
      "url": "https://en.wikipedia.org/wiki/Chinese_room",
      "summary": "The Chinese room argument holds that a digital computer executing a program cannot have a \"mind\", \"understanding\", or \"consciousness\", regardless of how intelligently or human-like the program may make the computer behave. The argument was presented by philosopher John Searle in his paper \"Minds, Brains, and Programs\", published in Behavioral and Brain Sciences in 1980. Similar arguments were presented by Gottfried Leibniz (1714), Anatoly Dneprov (1961), Lawrence Davis (1974) and Ned Block (1978). Searle's version has been widely discussed in the years since. The centerpiece of Searle's argument is a thought experiment known as the Chinese room.The argument is directed against the philosophical positions of functionalism and computationalism which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls the strong AI hypothesis: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"Although it was originally presented in reaction to the statements of artificial intelligence (AI) researchers, it is not an argument against the goals of mainstream AI research because it does not show a limit in the amount of \"intelligent\" behavior a machine can display. The argument applies only to digital computers running programs and does not apply to machines in general."
    },
    {
      "id": "8760516",
      "title": "Christopher Bishop",
      "url": "https://en.wikipedia.org/wiki/Christopher_Bishop",
      "summary": "Christopher Michael Bishop (born 7 April 1959)  is a British computer scientist. He is a Microsoft Technical Fellow and Director of Microsoft Research AI4Science. He is also Honorary Professor of Computer Science at the University of Edinburgh, and a Fellow of Darwin College, Cambridge. Chris was a founding member of the UK AI Council, and in 2019 he was appointed to the Prime Minister\u2019s Council for Science and Technology.\n\n"
    },
    {
      "id": "555480",
      "title": "Chromosome (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Chromosome_(genetic_algorithm)",
      "summary": "In genetic algorithms (GA), or more general, evolutionary algorithms (EA), a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution of the problem that the evolutionary algorithm is trying to solve. The set of all solutions, also called individuals according to the biological model, is known as the population. The genome of an individual consists of one, more rarely of several, chromosomes and corresponds to the genetic representation of the task to be solved. A chromosome is composed of a set of genes, where a gene consists of one or more semantically connected parameters, which are often also called decision variables. They determine one or more phenotypic characteristics of the individual or at least have an influence on them.  In the basic form of genetic algorithms, the chromosome is represented as a binary string, while in later variants and in EAs in general, a wide variety of other data structures are used.\n\n"
    },
    {
      "id": "158949",
      "title": "CiteSeerX",
      "url": "https://en.wikipedia.org/wiki/CiteSeerX",
      "summary": "CiteSeerX (formerly called CiteSeer) is a public search engine and digital library for scientific and academic papers, primarily in the fields of computer and information science.\nCiteSeer's goal is to improve the dissemination and access of academic and scientific literature. As a non-profit service that can be freely used by anyone, it has been considered as part of the open access movement that is attempting to change academic and scientific publishing to allow greater access to scientific literature. CiteSeer freely provided Open Archives Initiative metadata of all indexed documents and links indexed documents when possible to other sources of metadata such as DBLP and the ACM Portal. To promote open data, CiteSeerX shares its data for non-commercial purposes under a Creative Commons license.CiteSeer is considered as a predecessor of academic search tools such as Google Scholar and Microsoft Academic Search. CiteSeer-like engines and archives usually only harvest documents from publicly available websites and do not crawl publisher websites. For this reason, authors whose documents are freely available are more likely to be represented in the index.\nCiteSeer changed its name to ResearchIndex at one point and then changed it back."
    },
    {
      "id": "2155752",
      "title": "Citizen science",
      "url": "https://en.wikipedia.org/wiki/Citizen_science",
      "summary": "Citizen science (similar to community science, crowd science, crowd-sourced science, civic science, participatory monitoring, or volunteer monitoring) is research conducted with participation from the general public, or amateur/nonprofessional researchers or participants for science, social science and many other disciplines. There are variations in the exact definition of citizen science, with different individuals and organizations having their own specific interpretations of what citizen science encompasses. Citizen science is used in a wide range of areas of study including ecology, biology and conservation, health and medical research, astronomy, media and communications and information science.There are different applications and functions of citizen science in research projects.  Citizen science can be used as a methodology where public volunteers help in collecting and classifying data, improving the scientific community's capacity.  Citizen science can also involve more direct involvement from the public, with communities initiating projects researching environment and health hazards in their own communities. Participation in citizen science projects also educates the public about the scientific process and increases awareness about different topics.  Some schools have students participate in citizen science projects for this purpose as a part of the teaching curriculums."
    },
    {
      "id": "142440",
      "title": "Climatology",
      "url": "https://en.wikipedia.org/wiki/Climatology",
      "summary": "Climatology (from Greek \u03ba\u03bb\u03af\u03bc\u03b1, klima, \"slope\"; and -\u03bb\u03bf\u03b3\u03af\u03b1, -logia) or climate science is the scientific study of Earth's climate, typically defined as weather conditions averaged over a period of at least 30 years. Climate concerns the atmospheric condition during an extended to indefinite period of time; weather is the condition of the atmosphere during a relative brief period of time. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change. This topic of study is regarded as part of the atmospheric sciences and a subdivision of physical geography, which is one of the Earth sciences. Climatology includes some aspects of oceanography and biogeochemistry.\nThe main methods employed by climatologists are the analysis of observations and modelling of the physical processes that determine climate. Short term weather forecasting can be interpreted in terms of knowledge of longer-term phenomena of climate, for instance climatic cycles such as the El Ni\u00f1o\u2013Southern Oscillation (ENSO), the Madden\u2013Julian oscillation (MJO), the North Atlantic oscillation (NAO), the Arctic oscillation (AO), the Pacific decadal oscillation (PDO), and the Interdecadal Pacific Oscillation (IPO). Climate models are used for a variety of purposes from studying the dynamics of the weather and climate system to predictions of future climate. \n\n"
    },
    {
      "id": "669675",
      "title": "Cluster analysis",
      "url": "https://en.wikipedia.org/wiki/Cluster_analysis",
      "summary": "Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some specific sense defined by the analyst) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\nCluster analysis refers to a family of algorithms and tasks rather than one specific algorithm. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.\nBesides the term clustering, there is a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek \u03b2\u03cc\u03c4\u03c1\u03c5\u03c2 \"grape\"), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.\nCluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology."
    },
    {
      "id": "1500869",
      "title": "Coefficient of determination",
      "url": "https://en.wikipedia.org/wiki/Coefficient_of_determination",
      "summary": "In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).\nIt is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.There are several definitions of R2 that are only sometimes equivalent. One class of such cases includes that of simple linear regression where r2 is used instead of R2. When only an intercept is included, then r2 is simply the square of the sample correlation coefficient (i.e., r) between the observed outcomes and the observed predictor values. If additional regressors are included, R2 is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination normally ranges from 0 to 1.\nThere are cases where R2 can yield negative values. This can arise when the predictions that are being compared to the corresponding outcomes have not been derived from a model-fitting procedure using those data. Even if a model-fitting procedure has been used, R2 may still be negative, for example when linear regression is conducted without including an intercept, or when a non-linear function is used to fit the data. In cases where negative values arise, the mean of the data provides a better fit to the outcomes than do the fitted function values, according to this particular criterion.\nThe coefficient of determination can be more (intuitively) informative than MAE, MAPE, MSE, and RMSE in regression analysis evaluation, as the former can be expressed as a percentage, whereas the latter measures have arbitrary ranges. It also proved more robust for poor fits compared to SMAPE on the test datasets in the article.When evaluating the goodness-of-fit of simulated (Ypred) vs. measured (Yobs) values, it is not appropriate to base this on the R2 of the linear regression (i.e., Yobs= m\u00b7Ypred + b). The R2 quantifies the degree of any linear correlation between Yobs and Ypred, while for the goodness-of-fit evaluation only one specific linear correlation should be taken into consideration: Yobs = 1\u00b7Ypred + 0 (i.e., the 1:1 line).\n\n"
    },
    {
      "id": "42581062",
      "title": "Cognitive computing",
      "url": "https://en.wikipedia.org/wiki/Cognitive_computing",
      "summary": "Cognitive computing refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing.  These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), human\u2013computer interaction, dialog and narrative generation, among other technologies."
    },
    {
      "id": "5739",
      "title": "Compiler",
      "url": "https://en.wikipedia.org/wiki/Compiler",
      "summary": "In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.:\u200ap1\u200aThere are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs.  A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimised compiler for a language.\nRelated software include decompilers, programs that translate from low-level languages to higher level ones; programs that translate between high-level languages, usually called source-to-source compilers or transpilers; language rewriters, usually programs that translate the form of expressions without a change of language; and compiler-compilers, compilers that produce compilers (or parts of them), often in a generic and reusable way so as to be able to produce many differing compilers.\nA compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and machine specific code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations.:\u200ap2\u200a The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In theory, a programming language can have both a compiler and an interpreter. In practice, programming languages tend to be associated with just one (a compiler or an interpreter)."
    },
    {
      "id": "155414",
      "title": "Computability theory",
      "url": "https://en.wikipedia.org/wiki/Computability_theory",
      "summary": "Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, computability theory overlaps with proof theory and effective descriptive set theory.\nBasic questions addressed by computability theory include:\n\nWhat does it mean for a function on the natural numbers to be computable?\nHow can noncomputable functions be classified into a hierarchy based on their level of noncomputability?Although there is considerable overlap in terms of knowledge and methods, mathematical computability theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.\n\n"
    },
    {
      "id": "48520204",
      "title": "Computational anatomy",
      "url": "https://en.wikipedia.org/wiki/Computational_anatomy",
      "summary": "Computational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability. It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.\nThe field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, machine learning, computational mechanics, computational science, biological imaging, neuroscience, physics, probability, and statistics; it also has strong connections with fluid mechanics and geometric mechanics. Additionally, it complements newer, interdisciplinary fields like bioinformatics and neuroinformatics in the sense that its interpretation uses metadata derived from the original sensor imaging modalities (of which magnetic resonance imaging is one example). It focuses on the anatomical structures being imaged, rather than the medical imaging devices. It is similar in spirit to the history of computational linguistics, a discipline that focuses on the linguistic structures rather than the sensor acting as the transmission and communication media.\nIn computational anatomy, the diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow in \n  \n    \n      \n        \n          \n            \n              R\n            \n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle {\\mathbb {R} }^{3}}\n  . The flows between coordinates in computational anatomy are constrained to be geodesic flows satisfying the principle of least action for the Kinetic energy of the flow. The kinetic energy is defined through a Sobolev smoothness norm with strictly more than two generalized, square-integrable derivatives for each component of the flow velocity, which guarantees that the flows in \n  \n    \n      \n        \n          \n            R\n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{3}}\n   are diffeomorphisms. \nIt also implies that the diffeomorphic shape momentum taken pointwise satisfying the Euler-Lagrange equation for geodesics is determined by its neighbors through spatial derivatives on the velocity field.  This separates the discipline from the case of incompressible fluids for which momentum is a pointwise function of velocity. Computational anatomy intersects the study of Riemannian manifolds and nonlinear global analysis, where groups of diffeomorphisms are the central focus. Emerging high-dimensional theories of shape are central to many studies in computational anatomy, as are questions emerging from the fledgling field of shape statistics.\nThe metric structures in computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology diffeomorphometry, the metric space study of coordinate systems via diffeomorphisms.\n\n"
    },
    {
      "id": "149353",
      "title": "Computational biology",
      "url": "https://en.wikipedia.org/wiki/Computational_biology",
      "summary": "Computational biology refers to the use of data analysis, mathematical modeling and computational simulations to understand biological systems and relationships. An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics. It differs from biological computing, a subfield of computer science and engineering which uses bioengineering to build computers.\n\n"
    },
    {
      "id": "6019",
      "title": "Computational chemistry",
      "url": "https://en.wikipedia.org/wiki/Computational_chemistry",
      "summary": "Computational chemistry is a branch of chemistry that uses computer simulations to assist in solving chemical problems. It uses methods of theoretical chemistry incorporated into computer programs to calculate the structures and properties of molecules, groups of molecules, and solids.  The importance of this subject stems from the fact that, with the exception of some relatively recent findings related to the hydrogen molecular ion (dihydrogen cation), achieving an accurate quantum mechanical depiction of chemical systems analytically, or in a closed form, is not feasible. The complexity inherent in the many-body problem exacerbates the challenge of providing detailed descriptions of quantum mechanical systems. While computational results normally complement information obtained by chemical experiments, it can occasionally predict unobserved chemical phenomena."
    },
    {
      "id": "7543",
      "title": "Computational complexity theory",
      "url": "https://en.wikipedia.org/wiki/Computational_complexity_theory",
      "summary": "In theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\nA problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically."
    },
    {
      "id": "1420447",
      "title": "Computational economics",
      "url": "https://en.wikipedia.org/wiki/Computational_economics",
      "summary": "Computational economics is an interdisciplinary research discipline that involves combines methods in computational science and economics to solve complex economic problems.  This subject encompasses computational modeling of economic systems. Some of these areas are unique, while others established areas of economics by allowing robust data analytics and solutions of problems that would be arduous to research without computers and associated numerical methods.Computational methods have been applied in various fields of economics research, including but not limiting to:   \nEconometrics: Non-parametric approaches, semi-parametric approaches, and machine learning.\nDynamic systems modeling: Optimization, dynamic stochastic general equilibrium modeling, and agent-based modeling.\n\n"
    },
    {
      "id": "7948184",
      "title": "Computational engineering",
      "url": "https://en.wikipedia.org/wiki/Computational_engineering",
      "summary": "Computational Engineering is an emerging discipline that deals with the development and application of computational models for engineering, known as Computational Engineering Models or CEM. Computational engineering uses computers to solve engineering design problems important to a variety of industries. At this time, various different approaches are summarized under the term Computational Engineering, including using computational geometry and virtual design for engineering tasks, often coupled with a simulation-driven approach In Computational Engineering, algorithms solve mathematical and logical models that describe engineering challenges, sometimes coupled with some aspect of AI, specifically Reinforcement Learning.In Computational Engineering the engineer encodes their knowledge using logical structuring. The result is an algorithm, the Computational Engineering Model, that can produce many different variants of engineering designs, based on varied input requirements. The results can then be analyzed through additional mathematical models to create algorithmic feedback loops.Simulations of physical behaviors relevant to the field, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (as well as natural phenomena (computational science). It is therefore related to Computational Science and Engineering, which has been described as the \"third mode of discovery\" (next to theory and experimentation).In Computational Engineering, computer simulation provides the capability to create feedback that would be inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive.\nComputational Engineering should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in Computational Engineering (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with Computational Engineering methods (as an application area).\nIt is typically offered as a masters or doctorate program.\n\n"
    },
    {
      "id": "176927",
      "title": "Computational geometry",
      "url": "https://en.wikipedia.org/wiki/Computational_geometry",
      "summary": "Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.\nComputational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.\nThe main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.\nOther important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), and computer vision (3D reconstruction).\nThe main branches of computational geometry are:\n\nCombinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term \"computational geometry\" in this sense by 1975.\nNumerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term \"computational geometry\" in this meaning has been in use since 1971.Although most algorithms of computational geometry have been developed (and are being developed) for electronic computers, some algorithms were developed for unconventional computers (e.g. optical computers )\n\n"
    },
    {
      "id": "387537",
      "title": "Computational learning theory",
      "url": "https://en.wikipedia.org/wiki/Computational_learning_theory",
      "summary": "In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.\n\n"
    },
    {
      "id": "5561",
      "title": "Computational linguistics",
      "url": "https://en.wikipedia.org/wiki/Computational_linguistics",
      "summary": "Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\n\n"
    },
    {
      "id": "10433833",
      "title": "Computational mathematics",
      "url": "https://en.wikipedia.org/wiki/Computational_mathematics",
      "summary": "Computational mathematics is an area of mathematics devoted to the interaction between mathematics and computer computation.A large part of computational mathematics consists roughly of using mathematics for allowing and improving computer computation in areas of science and engineering where mathematics are useful. This involves in particular algorithm design, computational complexity, numerical methods and computer algebra.\nComputational mathematics refers also to the use of computers for mathematics itself. This includes mathematical experimentation for establishing conjectures (particularly in number theory), the use of computers for proving theorems (for example the four color theorem), and the design and use of proof assistants."
    },
    {
      "id": "40077102",
      "title": "Computational social science",
      "url": "https://en.wikipedia.org/wiki/Computational_social_science",
      "summary": "Computational social science is the academic sub-discipline concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena.  Fields include computational economics, computational sociology, cliodynamics, culturomics, nonprofit studies, and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.\n\n"
    },
    {
      "id": "15832717",
      "title": "Computational statistics",
      "url": "https://en.wikipedia.org/wiki/Computational_statistics",
      "summary": "Computational statistics, or statistical computing, is the bond between statistics and computer science, and refers to the statistical methods that are enabled by using computational methods. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.As in traditional statistics the goal is to transform raw data into knowledge, but the focus lies on computer intensive statistical methods, such as cases with very large sample size and non-homogeneous data sets.The terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as \"the application of computer science to statistics\",\nand 'computational statistics' as \"aiming at the design of algorithm for implementing\nstatistical methods on computers, including the ones unthinkable before the computer\nage (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems\" [sic].The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.\n\n"
    },
    {
      "id": "411964",
      "title": "Computer accessibility",
      "url": "https://en.wikipedia.org/wiki/Computer_accessibility",
      "summary": "Computer accessibility refers to the accessibility of a computer system to all people, regardless of disability type or severity of impairment. The term accessibility is most often used in reference to specialized hardware or software, or a combination of both, designed to enable the use of a computer by a person with a disability or impairment.\nAccessibility features are meant to make the use of technology less challenging for those with disabilities. Common accessibility features include text-to-speech, closed-captioning, and keyboard shortcuts. More specific technologies that need additional hardware are referred to as assistive technology.There are many disabilities or impairments that can be a barrier to effective computer use. Some of these impairments, which can be acquired from disease, trauma, or congenital disorders, include:\n\nCognitive impairments (head injury, autism, developmental disabilities) and learning disabilities, (such as dyslexia, dyscalculia, or ADHD).\nVisual impairment, such as low-vision, complete or partial blindness, and color blindness.\nHearing-related disabilities (deafness), including deafness, being hard of hearing, and hyperacusis.\nMotor or dexterity impairment such as paralysis, cerebral palsy, dyspraxia, carpal tunnel syndrome, and repetitive strain injury.A topic closely linked to computer accessibility is web accessibility. Similar to computer accessibility, web accessibility is the practice of making the use of the World Wide Web easier to use for individuals with disabilities.\nAccessibility is often abbreviated as the numeronym a11y, where the number 11 refers to the number of letters omitted.  This parallels the abbreviations of internationalization and localization as i18n and l10n, respectively.  Moreover, a11y is also listed on the USPTO Supplemental Register under Accessibility Now, Inc."
    },
    {
      "id": "6777",
      "title": "Computer animation",
      "url": "https://en.wikipedia.org/wiki/Computer_animation",
      "summary": "Computer animation is the process used for digitally generating animations. The more general term computer-generated imagery (CGI) encompasses both static scenes (still images) and dynamic images (moving images), while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics. The animation's target is sometimes the computer itself, while other times it is film.\nComputer animation is essentially a digital successor to stop motion techniques, but using models and traditional animation techniques using frame-by-frame animation illustrations. Also computer-generated animations allow a single graphic artist to produce such content without using actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new similar image but advanced slightly in time (usually at a rate of 24, 25, or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.\nFor 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.For 3D animations, all frames must be rendered after the modeling is complete. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real-time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use the software on the end user's computer to render in real-time as an alternative to streaming or pre-loaded high bandwidth animations."
    },
    {
      "id": "25652303",
      "title": "Computer architecture",
      "url": "https://en.wikipedia.org/wiki/Computer_architecture",
      "summary": "In computer science and computer engineering, computer architecture is a description of the structure of a computer system made from component parts. It can sometimes be a high-level description that ignores details of the implementation. At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation."
    },
    {
      "id": "1336512",
      "title": "PC game",
      "url": "https://en.wikipedia.org/wiki/PC_game",
      "summary": "A personal computer game, also known as computer game or abbreviated PC game, is an electronic game played on a personal computer (PC) and form of video game. They are defined by the open platform nature of PC systems.\nMainframe and minicomputer games are a precursor to personal computer games. Home computer games became popular following the video game crash of 1983. In the 1990s, PC games lost mass market traction to console games on the fifth generation such as the Sega Saturn, Nintendo 64 and PlayStation. They are enjoying a resurgence in popularity since the mid-2000s through digital distribution on online service providers. Personal computers as well as general computer software are considered synonymous with IBM PC compatible systems; while mobile devices \u2013 smartphones and tablets, such as those running on Android or iOS platforms \u2013 are also PCs in the general sense as opposed to console or arcade machine. Historically, it also included games on systems from Apple Computer and Commodore International. Microsoft Windows utilizing Direct3D become the most popular operating system for PC games in the 2000s. Games utilizing 3D graphics generally require a form of graphics processing unit, and PC games have been a major influencing factor for the development and marketing of graphics cards. Emulators are able to play games developed for other platforms. The demoscene originated from computer game cracking.\nThe uncoordinated nature of the PC game market make precisely assessing its size difficult. PC remains the most important gaming platform with 60% of developers being most interested in developing a game for the platform and 66% of developers currently developing a game for PC. In 2018, the global PC games market was valued at about $27.7 billion. According to research data provided by Statista in 2020 there were an estimated 1.75 billion PC gamers worldwide, up from 1.5 billion PC gaming users in the previous year. Newzoo reports that the PC gaming sector is the third-largest category across all platforms as of 2016, with the console sector second-largest, and mobile gaming sector biggest. 2.2 billion video gamers generate US$101.1 billion in revenue, excluding hardware costs. \"Digital game revenues will account for $94.4 billion or 87% of the global gaming market. The APAC region was estimated to generate $46.6 billion in 2016, or 47% of total global video game revenues (note, not only \"PC\" games). China alone accounts for half of APAC's revenues (at $24.4 billion), cementing its place as the largest video game market in the world, ahead of the US's anticipated market size of $23.5 billion."
    },
    {
      "id": "18567210",
      "title": "Computer graphics",
      "url": "https://en.wikipedia.org/wiki/Computer_graphics",
      "summary": "Computer graphics deals with generating images and art with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research.Some topics in computer graphics include user interface design, sprite graphics, rendering, ray tracing, geometry processing, computer animation, vector graphics, 3D modeling, shaders, GPU design, implicit surfaces, visualization, scientific computing, image processing, computational photography, scientific visualization, computational geometry and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, physics, and perception.\n\nComputer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, and video games, in general."
    },
    {
      "id": "21808348",
      "title": "Computer hardware",
      "url": "https://en.wikipedia.org/wiki/Computer_hardware",
      "summary": "Computer hardware includes the physical parts of a computer, such as the case, central processing unit (CPU), random access memory (RAM), monitor, mouse, keyboard, computer data storage, graphics card, sound card, speakers and motherboard.By contrast, software is the set of instructions that can be stored and run by hardware. Hardware is so-termed because it is hard or rigid with respect to changes, whereas software is soft because it is easy to change.\nHardware is typically directed by the software to execute any command or instruction. A combination of hardware and software forms a usable computing system, although other systems exist with only hardware.\n\n"
    },
    {
      "id": "4122592",
      "title": "Computer network",
      "url": "https://en.wikipedia.org/wiki/Computer_network",
      "summary": "A computer network is a set of computers sharing resources located on or provided by network nodes. Computers use common communication protocols over digital interconnections to communicate with each other. These interconnections are made up of telecommunication network technologies based on physically wired, optical, and wireless radio-frequency methods that may be arranged in a variety of network topologies.\nThe nodes of a computer network can include personal computers, servers, networking hardware, or other specialized or general-purpose hosts. They are identified by network addresses and may have hostnames. Hostnames serve as memorable labels for the nodes and are rarely changed after initial assignment. Network addresses serve for locating and identifying the nodes by communication protocols such as the Internet Protocol.\nComputer networks may be classified by many criteria, including the transmission medium used to carry signals, bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanisms, and organizational intent.Computer networks support many applications and services, such as access to the World Wide Web, digital video and audio, shared use of application and storage servers, printers and fax machines, and use of email and instant messaging applications.\n\n"
    },
    {
      "id": "404048",
      "title": "Computing Machinery and Intelligence",
      "url": "https://en.wikipedia.org/wiki/Computing_Machinery_and_Intelligence",
      "summary": "\"Computing Machinery and Intelligence\" is a seminal paper written by Alan Turing on the topic of artificial intelligence. The paper, published in 1950 in Mind, was the first to introduce his concept of what is now known as the Turing test to the general public.\nTuring's paper considers the question \"Can machines think?\" Turing says that since the words \"think\" and \"machine\" cannot be clearly defined we should \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.\" To do this, he must first find a simple and unambiguous idea to replace the word \"think\", second he must explain exactly which \"machines\" he is considering, and finally, armed with these tools, he formulates a new question, related to the first, that he believes he can answer in the affirmative.\n\n"
    },
    {
      "id": "81196",
      "title": "Computing platform",
      "url": "https://en.wikipedia.org/wiki/Computing_platform",
      "summary": "A computing platform, digital platform, or software platform is an environment in which software is executed. It may be the hardware or the operating system (OS), a web browser and associated application programming interfaces, or other underlying software, as long as the program code is executed using the services provided by the platform. Computing platforms have different abstraction levels, including a computer architecture, an OS, or runtime libraries. A computing platform is the stage on which computer programs can run.\nA platform can be seen both as a constraint on the software development process, in that different platforms provide different functionality and restrictions; and as an assistant to the development process, in that they provide low-level functionality ready-made.  For example, an OS may be a platform that abstracts the underlying differences in hardware and provides a generic command for saving files or accessing the network."
    },
    {
      "id": "2581605",
      "title": "Concurrent computing",
      "url": "https://en.wikipedia.org/wiki/Concurrent_computing",
      "summary": "Concurrent computing is a form of computing in which several computations are executed concurrently\u2014during overlapping time periods\u2014instead of sequentially\u2014with one completing before the next starts.\nThis is a property of a system\u2014whether a program, computer, or a network\u2014where there is a separate execution point or \"thread of control\" for each process. A concurrent system is one where a computation can advance without waiting for all other computations to complete.Concurrent computing is a form of modular programming. In its paradigm an overall computation is factored into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.\n\n"
    },
    {
      "id": "801135",
      "title": "Conditional independence",
      "url": "https://en.wikipedia.org/wiki/Conditional_independence",
      "summary": "In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is the hypothesis, and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   are observations, conditional independence can be stated as an equality:\n\n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        ,\n        C\n        )\n        =\n        P\n        (\n        A\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\mid B,C)=P(A\\mid C)}\n  where \n  \n    \n      \n        P\n        (\n        A\n        \u2223\n        B\n        ,\n        C\n        )\n      \n    \n    {\\displaystyle P(A\\mid B,C)}\n   is the probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given both \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  . Since the probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   is the same as the probability of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   given both \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , this equality expresses that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   contributes nothing to the certainty of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  . In this case, \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   are said to be conditionally independent given \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  , written symbolically as: \n  \n    \n      \n        (\n        A\n        \u22a5\n        \n        \n        \n        \u22a5\n        B\n        \u2223\n        C\n        )\n      \n    \n    {\\displaystyle (A\\perp \\!\\!\\!\\perp B\\mid C)}\n  . In the language of causal equality notation, two functions \n  \n    \n      \n        f\n        (\n        y\n        )\n      \n    \n    {\\displaystyle f(y)}\n   and \n  \n    \n      \n        g\n        (\n        y\n        )\n      \n    \n    {\\displaystyle g(y)}\n   which both depend on a common variable \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   are described as conditionally independent using the notation \n  \n    \n      \n        f\n        \n          (\n          y\n          )\n        \n         \n        \n          \n            =\n            \n              \u21b6\u21b7\n            \n          \n        \n         \n        g\n        \n          (\n          y\n          )\n        \n      \n    \n    {\\displaystyle f\\left(y\\right)~{\\overset {\\curvearrowleft \\curvearrowright }{=}}~g\\left(y\\right)}\n  , which is equivalent to the notation \n  \n    \n      \n        P\n        (\n        f\n        \u2223\n        g\n        ,\n        y\n        )\n        =\n        P\n        (\n        f\n        \u2223\n        y\n        )\n      \n    \n    {\\displaystyle P(f\\mid g,y)=P(f\\mid y)}\n  .\nThe concept of conditional independence is essential to graph-based theories of statistical inference, as it establishes a mathematical relation between a collection of conditional statements and a graphoid.\n\n"
    },
    {
      "id": "4118276",
      "title": "Conditional random field",
      "url": "https://en.wikipedia.org/wiki/Conditional_random_field",
      "summary": "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, \"linear chain\" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\nOther examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, part-of-speech tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision."
    },
    {
      "id": "5856528",
      "title": "Special Interest Group on Knowledge Discovery and Data Mining",
      "url": "https://en.wikipedia.org/wiki/Special_Interest_Group_on_Knowledge_Discovery_and_Data_Mining",
      "summary": "SIGKDD, representing the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining, hosts an influential annual conference.\n\n"
    },
    {
      "id": "1175156",
      "title": "Conference on Neural Information Processing Systems",
      "url": "https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems",
      "summary": "The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December. The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts.\n\n"
    },
    {
      "id": "1642843",
      "title": "Continuous production",
      "url": "https://en.wikipedia.org/wiki/Continuous_production",
      "summary": "Continuous production is a flow production method used to manufacture, produce, or process materials without interruption.  Continuous production is called a continuous process or a continuous flow process because the materials, either dry bulk or fluids that are being processed are continuously in motion, undergoing chemical reactions or subject to mechanical or heat treatment.  Continuous processing is contrasted with batch production.\nContinuous usually means operating 24 hours per day, seven days per week with infrequent maintenance shutdowns, such as semi-annual or annual.  Some chemical plants can operate for more than one to two years without a shutdown.  Blast furnaces can run from four to ten years without stopping.\n\n"
    },
    {
      "id": "7039",
      "title": "Control theory",
      "url": "https://en.wikipedia.org/wiki/Control_theory",
      "summary": "Control theory is a field of control engineering and applied mathematics that deals with the control of dynamical systems in engineered processes and machines. The objective is to develop a model or algorithm governing the application of system inputs to drive the system to a desired state, while minimizing any delay, overshoot, or steady-state error and ensuring a level of control stability; often with the aim to achieve a degree of optimality.\nTo do this, a controller with the requisite corrective behavior is required. This controller monitors the controlled process variable (PV), and compares it with the reference or set point (SP). The difference between actual and desired value of the process variable, called the error signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are  controllability and observability.  Control theory is used in control system engineering to design automation  that have revolutionized manufacturing, aircraft, communications and other industries, and created new fields such as robotics.  \nExtensive use is usually made of a diagrammatic style known as the block diagram. In it the transfer function, also known as the system function or network function, is a mathematical model of the relation between the input and output based on the differential equations describing the system.\nControl theory dates from the 19th century, when the theoretical basis for the operation of governors was first described by James Clerk Maxwell.  Control theory was further advanced by Edward Routh in 1874, Charles Sturm and in 1895, Adolf Hurwitz, who all contributed to the establishment of control stability criteria; and from 1922 onwards, the development of PID control theory by Nicolas Minorsky.\nAlthough a major application of mathematical control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs - thus control theory also has applications in life sciences, computer engineering, sociology and operations research."
    },
    {
      "id": "22284121",
      "title": "Variable (computer science)",
      "url": "https://en.wikipedia.org/wiki/Variable_(computer_science)",
      "summary": "In computer programming, a variable is an abstract storage location paired with an associated symbolic name, which contains some known or unknown quantity of data or object referred to as a value; or in simpler terms, a variable is a named container for a particular set of bits or type of data (like integer, float, string etc...). A variable can eventually be associated with or identified by a memory address. The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.Variables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.\nA variable's storage location may be referenced by several different identifiers, a situation known as aliasing. Assigning a value to the variable using one of the identifiers will change the value that can be accessed through the other identifiers.\nCompilers have to replace variables' symbolic names with the actual locations of the data. While a variable's name, type, and location often remain fixed, the data stored in the location may be changed during program execution.\n\n"
    },
    {
      "id": "7519",
      "title": "Convolution",
      "url": "https://en.wikipedia.org/wiki/Convolution",
      "summary": "In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (\n  \n    \n      \n        f\n        \u2217\n        g\n      \n    \n    {\\displaystyle f*g}\n  ). The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. The integral is evaluated for all values of shift, producing the convolution function. The choice of which function is reflected and shifted before the integral does not change the integral result (see commutativity). Graphically, it expresses how the 'shape' of one function is modified by the other. \nSome features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, convolution (\n  \n    \n      \n        f\n        \u2217\n        g\n      \n    \n    {\\displaystyle f*g}\n  ) differs from cross-correlation (\n  \n    \n      \n        f\n        \u22c6\n        g\n      \n    \n    {\\displaystyle f\\star g}\n  ) only in that either f(x) or g(x) is reflected about the y-axis in convolution; thus it is a cross-correlation of g(\u2212x) and f(x), or f(\u2212x) and g(x). For complex-valued functions, the cross-correlation operator is the adjoint of the convolution operator.\nConvolution has applications that include probability, statistics, acoustics, spectroscopy, signal processing and image processing, geophysics, engineering, physics, computer vision and differential equations.The convolution can be defined for functions on Euclidean space and other groups (as algebraic structures). For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by periodic convolution. (See row 18 at DTFT \u00a7 Properties.) A discrete convolution can be defined for functions on the set of integers.\nGeneralizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.Computing the inverse of the convolution operation is known as deconvolution."
    },
    {
      "id": "40409788",
      "title": "Convolutional neural network",
      "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
      "summary": "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,  only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\nThey have applications in: \n\nimage and video recognition,\nrecommender systems,\n\nimage classification,\n\nimage segmentation,\n\nmedical image analysis,\n\nnatural language processing,\n\nbrain\u2013computer interfaces, and\n\nfinancial time series.CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increases the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.\n\n"
    },
    {
      "id": "33737411",
      "title": "Corinna Cortes",
      "url": "https://en.wikipedia.org/wiki/Corinna_Cortes",
      "summary": "Corinna Cortes (born 31 March, 1961) is a Danish computer scientist known for her contributions to machine learning. She is a Vice President at Google Research in New York City. Cortes is an ACM Fellow and a recipient of the Paris Kanellakis Award for her work on theoretical foundations of support vector machines."
    },
    {
      "id": "53887",
      "title": "Text corpus",
      "url": "https://en.wikipedia.org/wiki/Text_corpus",
      "summary": "In linguistics and natural language processing, a corpus (pl.: corpora) or text corpus is a dataset, consisting of  natively digital and older, digitalized, language resources, either annotated or unannotated.\nAnnotated, they have been used in corpus linguistics for statistical hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\nIn search technology, a corpus is the collection of documents which is being searched.\n\n"
    },
    {
      "id": "9793263",
      "title": "Covariance function",
      "url": "https://en.wikipedia.org/wiki/Covariance_function",
      "summary": "In probability theory and statistics, the covariance function describes how much two random variables change together (their covariance) with varying spatial or temporal separation. For a random field or stochastic process Z(x) on a domain D, a covariance function C(x, y) gives the covariance of the values of the random field at the two locations x and y:\n\n  \n    \n      \n        C\n        (\n        x\n        ,\n        y\n        )\n        :=\n        cov\n        \u2061\n        (\n        Z\n        (\n        x\n        )\n        ,\n        Z\n        (\n        y\n        )\n        )\n        =\n        \n          E\n        \n        \n          [\n          \n            {\n            Z\n            (\n            x\n            )\n            \u2212\n            \n              E\n            \n            [\n            Z\n            (\n            x\n            )\n            ]\n            }\n            \u22c5\n            {\n            Z\n            (\n            y\n            )\n            \u2212\n            \n              E\n            \n            [\n            Z\n            (\n            y\n            )\n            ]\n            }\n          \n          ]\n        \n        .\n        \n      \n    \n    {\\displaystyle C(x,y):=\\operatorname {cov} (Z(x),Z(y))=\\mathbb {E} \\left[\\{Z(x)-\\mathbb {E} [Z(x)]\\}\\cdot \\{Z(y)-\\mathbb {E} [Z(y)]\\}\\right].\\,}\n  The same C(x, y) is called the autocovariance function in two instances: in time series (to denote exactly the same concept except that x and y refer to locations in time rather than in space), and in multivariate random fields (to refer to the covariance of a variable with itself, as opposed to the cross covariance between two different variables at different locations, Cov(Z(x1), Y(x2))).\n\n"
    },
    {
      "id": "15905419",
      "title": "Credit card fraud",
      "url": "https://en.wikipedia.org/wiki/Credit_card_fraud",
      "summary": "Credit card fraud is an inclusive term for fraud committed using a payment card, such as a credit card or debit card. The purpose may be to obtain goods or services or to make payment to another account, which is controlled by a criminal. The Payment Card Industry Data Security Standard (PCI DSS) is the data security standard created to help financial institutions process card payments securely and reduce card fraud.Credit card fraud can be authorised, where the genuine customer themselves processes payment to another account which is controlled by a criminal, or unauthorised, where the account holder does not provide authorisation for the payment to proceed and the transaction is carried out by a third party. In 2018, unauthorised financial fraud losses across payment cards and remote banking totalled \u00a3844.8 million in the United Kingdom. Whereas banks and card companies prevented \u00a31.66 billion in unauthorised fraud in 2018. That is the equivalent to \u00a32 in every \u00a33 of attempted fraud being stopped.Credit card fraud can occur when unauthorized users gain access to an individual's credit card information in order to make purchases, other transactions, or open new accounts. A few examples of credit card fraud include account takeover fraud, new account fraud, cloned cards, and cards-not-present schemes. This unauthorized access occurs through phishing, skimming, and information sharing by a user, oftentimes unknowingly. However, this type of fraud can be detected through means of artificial intelligence and machine learning as well as prevented by issuers, institutions, and individual cardholders. According to a 2021 annual report, about 50% of all Americans have experienced a fraudulent charge on their credit or debit cards, and more than one in three credit or debit card holders have experienced fraud multiple times. This amounts to 127 million people in the US that have been victims of credit card theft at least once.\nRegulators, card providers and banks take considerable time and effort to collaborate with investigators worldwide with the goal of ensuring fraudsters are not successful. Cardholders' money is usually protected from scammers with regulations that make the card provider and bank accountable. The technology and security measures behind credit cards are continuously advancing, adding barriers for fraudsters attempting to steal money.\n\n"
    },
    {
      "id": "554546",
      "title": "Crossover (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)",
      "summary": "In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and is analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions may be mutated before being added to the population.\nDifferent algorithms in evolutionary computation may use different data structures to store genetic information, and each genetic representation can be recombined with different crossover operators. Typical data structures that can be recombined with crossover are bit arrays, vectors of real numbers, or trees.\nThe list of operators presented below is by no means complete and serves mainly as an exemplary illustration of this dyadic genetic operator type. More operators and more details can be found in the literature.\n\n"
    },
    {
      "id": "5292585",
      "title": "Crowdsourcing",
      "url": "https://en.wikipedia.org/wiki/Crowdsourcing",
      "summary": "Crowdsourcing involves a large group of dispersed participants contributing or producing goods or services\u2014including ideas, votes, micro-tasks, and finances\u2014for payment or as volunteers. Contemporary crowdsourcing often involves digital platforms to attract and divide work between participants to achieve a cumulative result. Crowdsourcing is not limited to online activity, however, and there are various historical examples of crowdsourcing. The word crowdsourcing is a portmanteau of \"crowd\" and \"outsourcing\". In contrast to outsourcing, crowdsourcing usually involves less specific and more public groups of participants.Advantages of using crowdsourcing include lowered costs, improved speed, improved quality, increased flexibility, and/or increased scalability of the work, as well as promoting diversity. Crowdsourcing methods include competitions, virtual labor markets, open online collaboration and data donation. Some forms of crowdsourcing, such as in \"idea competitions\" or \"innovation contests\" provide ways for organizations to learn beyond the \"base of minds\" provided by their employees (e.g. LEGO Ideas). Commercial platforms, such as Amazon Mechanical Turk, match microtasks submitted by requesters to workers who perform them. Crowdsourcing is also used by nonprofit organizations to develop common goods, such as Wikipedia."
    },
    {
      "id": "18934432",
      "title": "Cryptography",
      "url": "https://en.wikipedia.org/wiki/Cryptography",
      "summary": "Cryptography, or cryptology (from Ancient Greek: \u03ba\u03c1\u03c5\u03c0\u03c4\u03cc\u03c2, romanized: krypt\u00f3s \"hidden, secret\"; and \u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd graphein, \"to write\", or -\u03bb\u03bf\u03b3\u03af\u03b1 -logia, \"study\", respectively), is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, information security, electrical engineering, digital signal processing, physics, and others. Core concepts related to  information security (data confidentiality, data integrity, authentication, and non-repudiation) are also central to cryptography. Practical applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.\nCryptography prior to the modern age was effectively synonymous with encryption, converting readable information (plaintext) to unintelligible nonsense text (ciphertext), which can only be read by reversing the process (decryption). The sender of an encrypted (coded) message shares the decryption (decoding) technique only with the intended recipients to preclude access from adversaries. The cryptography literature often uses the names \"Alice\" (or \"A\") for the sender, \"Bob\" (or \"B\") for the intended recipient, and \"Eve\" (or \"E\") for the eavesdropping adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, cryptography methods have become increasingly complex and their applications more varied.\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed \"computationally secure\". Theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology require these designs to be continually reevaluated and, if necessary, adapted. Information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, are much more difficult to use in practice than the best theoretically breakable but computationally secure schemes.\nThe growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement disputes with regard to digital media."
    },
    {
      "id": "3369375",
      "title": "Cyberwarfare",
      "url": "https://en.wikipedia.org/wiki/Cyberwarfare",
      "summary": "Cyberwarfare is the use of cyber attacks against an enemy state, causing comparable harm to actual warfare and/or disrupting vital computer systems. Some intended outcomes could be espionage, sabotage, propaganda, manipulation or economic warfare.\nThere is significant debate among experts regarding the definition of cyberwarfare, and even if such a thing exists. One view is that the term is a misnomer since no cyber attacks to date could be described as a war. An alternative view is that it is a suitable label for cyber attacks which cause physical damage to people and objects in the real world.Many countries including the United States, United Kingdom, Russia, China, Israel, Iran, and North Korea have active cyber capabilities for offensive and defensive operations. As states explore the use of cyber operations and combine capabilities, the likelihood of physical confrontation and violence playing out as a result of, or part of, a cyber operation is increased. However, meeting the scale and protracted nature of war is unlikely, thus ambiguity remains.The first instance of kinetic military action used in response to a cyber-attack resulting in the loss of human life was observed on 5 May 2019, when the Israel Defense Forces targeted and destroyed a building associated with an ongoing cyber-attack."
    },
    {
      "id": "331535",
      "title": "Nucleic acid sequence",
      "url": "https://en.wikipedia.org/wiki/Nucleic_acid_sequence",
      "summary": "A nucleic acid sequence is a succession of bases within the nucleotides forming alleles within a DNA (using GACT) or RNA (GACU) molecule. This succession is denoted by a series of a set of five different letters that indicate the order of the nucleotides. By convention, sequences are usually presented from the 5' end to the 3' end. For DNA, with its double helix, there are two possible directions for the notated sequence; of these two, the sense strand is used. Because nucleic acids are normally linear (unbranched) polymers, specifying the sequence is equivalent to defining the covalent structure of the entire molecule. For this reason, the nucleic acid sequence is also termed the primary structure.\nThe sequence represents genetic information. Biological deoxyribonucleic acid represents the information which directs the functions of an organism.\nNucleic acids also have a secondary structure and tertiary structure. Primary structure is sometimes mistakenly referred to as \"primary sequence\". However there is no parallel concept of secondary or tertiary sequence.\n\n"
    },
    {
      "id": "1124646",
      "title": "Dartmouth workshop",
      "url": "https://en.wikipedia.org/wiki/Dartmouth_workshop",
      "summary": "The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered to be the founding event of artificial intelligence as a field.\nThe project lasted approximately six to eight weeks and was essentially an extended brainstorming session. Eleven mathematicians and scientists originally planned to attend; not all of them attended, but more than ten others came for short times.\n\n"
    },
    {
      "id": "2679315",
      "title": "David J. C. MacKay",
      "url": "https://en.wikipedia.org/wiki/David_J._C._MacKay",
      "summary": "Sir David John Cameron MacKay  (22 April 1967 \u2013 14 April 2016) was a British physicist, mathematician, and academic. He was the Regius Professor of Engineering in the Department of Engineering at the University of Cambridge and from 2009 to 2014 was Chief Scientific Advisor to the UK Department of Energy and Climate Change (DECC). MacKay wrote the book Sustainable Energy \u2013 Without the Hot Air.\n\n"
    },
    {
      "id": "2823113",
      "title": "David Rumelhart",
      "url": "https://en.wikipedia.org/wiki/David_Rumelhart",
      "summary": "David Everett Rumelhart (June 12, 1942 \u2013 March 13, 2011) was an American psychologist who made many contributions to the formal analysis of human cognition, working primarily within the frameworks of mathematical psychology, symbolic artificial intelligence, and parallel distributed processing. He also admired formal linguistic approaches to cognition, and explored the possibility of formulating a formal grammar to capture the structure of stories.\n\n"
    },
    {
      "id": "50568835",
      "title": "David Silver (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/David_Silver_(computer_scientist)",
      "summary": "David Silver  (born 1976) is a principal research scientist at Google DeepMind and a professor at University College London. He has led research on reinforcement learning with AlphaGo, AlphaZero and co-lead on AlphaStar."
    },
    {
      "id": "2714979",
      "title": "Decision boundary",
      "url": "https://en.wikipedia.org/wiki/Decision_boundary",
      "summary": "In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.\nA decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable.\nDecision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.\nDecision boundaries can be approximations of optimal stopping boundaries.  The decision boundary is the set of points of that hyperplane that pass through zero.  For example, the angle between a vector and points in a set must be zero for points that are on or close to the decision boundary. Decision boundary instability can be incorporated with generalization error as a standard for selecting the most accurate and stable classifier. \n\n"
    },
    {
      "id": "469578",
      "title": "Decision support system",
      "url": "https://en.wikipedia.org/wiki/Decision_support_system",
      "summary": "A decision support system (DSS) is an information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance\u2014i.e., unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both.\nWhile academics have perceived DSS as a tool to support decision making processes, DSS users see DSS as a tool to facilitate organizational processes. Some authors have extended the definition of DSS to include any system that might support decision making and some DSS include a decision-making software component; Sprague (1980) defines a properly termed DSS as follows:\n\nDSS tends to be aimed at the less well structured, underspecified problem that upper level managers typically face;\nDSS attempts to combine the use of models or analytic techniques with traditional data access and retrieval functions;\nDSS specifically focuses on features which make them easy to use by non-computer-proficient people in an interactive mode; and\nDSS emphasizes flexibility and adaptability to accommodate changes in the environment and the decision making approach of the user.DSSs include knowledge-based systems. A properly designed DSS is an interactive software-based system intended to help decision makers compile useful information from a combination of raw data, documents, personal knowledge, and/or business models to identify and solve problems and make decisions.\nTypical information that a decision support application might gather and present includes:\n\ninventories of information assets (including legacy and relational data sources, cubes, data warehouses, and data marts),\ncomparative sales figures between one period and the next,\nprojected revenue figures based on product sales assumptions.\n\n"
    },
    {
      "id": "577003",
      "title": "Decision tree learning",
      "url": "https://en.wikipedia.org/wiki/Decision_tree_learning",
      "summary": "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\nTree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences.Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making)."
    },
    {
      "id": "47332350",
      "title": "DeepDream",
      "url": "https://en.wikipedia.org/wiki/DeepDream",
      "summary": "DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately overprocessed images.Google's program popularized the term (deep) \"dreaming\" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.\n\n"
    },
    {
      "id": "64396232",
      "title": "DeepSpeed",
      "url": "https://en.wikipedia.org/wiki/DeepSpeed",
      "summary": "DeepSpeed is an open source deep learning optimization library for PyTorch. The library is designed to reduce computing power and memory use and to train large distributed models with better parallelism on existing computer hardware. DeepSpeed is optimized for low latency, high throughput training. It includes the Zero Redundancy Optimizer (ZeRO) for training models with 1 trillion or more parameters. Features include mixed precision training, single-GPU, multi-GPU, and multi-node training as well as custom model parallelism. The DeepSpeed source code is licensed under MIT License and available on GitHub.The team claimed to achieve up to a 6.2x throughput improvement, 2.8x faster convergence, and 4.6x less communication.\n\n"
    },
    {
      "id": "69432561",
      "title": "Deep learning speech synthesis",
      "url": "https://en.wikipedia.org/wiki/Deep_learning_speech_synthesis",
      "summary": "Deep learning speech synthesis  refers to the application of deep learning models to generate natural-sounding human speech from written text (text-to-speech) or spectrum (vocoder). Deep neural networks (DNN) are trained using a large amount of recorded speech and, in the case of a text-to-speech system, the associated labels and/or input text.\n\n"
    },
    {
      "id": "43169442",
      "title": "Deeplearning4j",
      "url": "https://en.wikipedia.org/wiki/Deeplearning4j",
      "summary": "Eclipse Deeplearning4j is a programming library written in Java for the Java virtual machine (JVM). It is a framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.Deeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group headquartered in San Francisco. It is supported commercially by the startup Skymind, which bundles DL4J, TensorFlow, Keras and other deep learning libraries in an enterprise distribution called the Skymind Intelligence Layer. Deeplearning4j was contributed to the Eclipse Foundation in October 2017.\n\n"
    },
    {
      "id": "3259263",
      "title": "Demis Hassabis",
      "url": "https://en.wikipedia.org/wiki/Demis_Hassabis",
      "summary": "Demis Hassabis  (born 27 July 1976) is a British computer scientist, artificial intelligence researcher and entrepreneur. In his early career he was a video game AI programmer and designer, and an expert board games player. He is the chief executive officer and co-founder of DeepMind and Isomorphic Labs, and a UK Government AI Advisor. He is a Fellow of the Royal Society, and has won many prestigious awards for his work on AlphaFold including the Breakthrough Prize, the Canada Gairdner International Award, and the Lasker Award. In 2017 he was appointed a CBE and listed in the Time 100 most influential people list."
    },
    {
      "id": "226660",
      "title": "Dempster\u2013Shafer theory",
      "url": "https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory",
      "summary": "The theory of belief functions, also referred to as evidence theory or Dempster\u2013Shafer theory (DST), is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. First introduced by Arthur P. Dempster in the context of statistical inference, the theory was later developed by Glenn Shafer into a general framework for modeling epistemic uncertainty\u2014a mathematical theory of evidence. The theory allows one to combine evidence from different sources and arrive at a degree of belief (represented by a mathematical object called belief function) that takes into account all the available evidence.\nIn a narrow sense, the term Dempster\u2013Shafer theory refers to the original conception of the theory by Dempster and Shafer. However, it is more common to use the term in the wider sense of the same general approach, as adapted to specific kinds of situations. In particular, many authors have proposed different rules for combining evidence, often with a view to handling conflicts in evidence better. The early contributions have also been the starting points of many important developments, including the transferable belief model and the theory of hints.\n\n"
    },
    {
      "id": "554671",
      "title": "Density estimation",
      "url": "https://en.wikipedia.org/wiki/Density_estimation",
      "summary": "In statistics, probability density estimation or simply density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function.  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization. The most basic form of density estimation is a rescaled histogram.\n\n"
    },
    {
      "id": "661384",
      "title": "Dependability",
      "url": "https://en.wikipedia.org/wiki/Dependability",
      "summary": "In systems engineering, dependability is a measure of a system's availability, reliability, maintainability, and in some cases, other characteristics such as durability, safety and security.  In real-time computing, dependability is the ability to provide services that can be trusted within a time-period. The service guarantees must hold even when the system is subject to attacks or natural failures. \nThe International Electrotechnical Commission (IEC), via its Technical Committee TC 56 develops and maintains international standards that provide systematic methods and tools for dependability assessment and management of equipment, services, and systems throughout their life cycles. The IFIP Working Group 10.4 on \"Dependable Computing and Fault Tolerance\" plays a role in synthesizing the technical community's progress in the field and organizes two workshops each year to disseminate the results. \nDependability can be broken down into three elements:\n\nAttributes - a way to assess the dependability of a system\nThreats - an understanding of the things that can affect the dependability of a system\nMeans - ways to increase a system's dependability"
    },
    {
      "id": "48813654",
      "title": "Sparse dictionary learning",
      "url": "https://en.wikipedia.org/wiki/Sparse_dictionary_learning",
      "summary": "Sparse dictionary learning (also known as sparse coding or SDL) is a representation learning method which aims at finding a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.\nOne of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery. In compressed sensing, a high-dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit, CoSaMP or fast non-iterative algorithms can be used to recover the signal.\nOne of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as Fourier or wavelet transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting."
    },
    {
      "id": "330206",
      "title": "Differentiable function",
      "url": "https://en.wikipedia.org/wiki/Differentiable_function",
      "summary": "In mathematics, a differentiable function of one real variable is a function whose derivative exists at each point in its domain. In other words, the graph of a differentiable function has a non-vertical tangent line at each interior point in its domain. A differentiable function is smooth (the function is locally well approximated as a linear function at each interior point) and does not contain any break, angle, or cusp.\nIf x0 is an interior point in the domain of a function f, then f is said to be differentiable at x0 if the derivative \n  \n    \n      \n        \n          f\n          \u2032\n        \n        (\n        \n          x\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle f'(x_{0})}\n   exists. In other words, the graph of f has a non-vertical tangent line at the point (x0, f(x0)). f is said to be differentiable on U if it is differentiable at every point of U. f is said to be continuously differentiable if its derivative is also a continuous function over the domain of the function \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n  . Generally speaking, f is said to be of class \n  \n    \n      \n        \n          C\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle C^{k}}\n   if its first \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   derivatives \n  \n    \n      \n        \n          f\n          \n            \u2032\n          \n        \n        (\n        x\n        )\n        ,\n        \n          f\n          \n            \u2032\n            \u2032\n          \n        \n        (\n        x\n        )\n        ,\n        \u2026\n        ,\n        \n          f\n          \n            (\n            k\n            )\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\textstyle f^{\\prime }(x),f^{\\prime \\prime }(x),\\ldots ,f^{(k)}(x)}\n   exist and are continuous over the domain of the function \n  \n    \n      \n        f\n      \n    \n    {\\textstyle f}\n  .\nFor a multivariable function, as shown here, the differentiability of it is something more than the existence of the partial derivatives of it."
    },
    {
      "id": "52036598",
      "title": "Differentiable neural computer",
      "url": "https://en.wikipedia.org/wiki/Differentiable_neural_computer",
      "summary": "In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (but not by definition) recurrent in its implementation. The model was published in 2016 by Alex Graves et al. of DeepMind.\n\n"
    },
    {
      "id": "59939845",
      "title": "Differentiable programming",
      "url": "https://en.wikipedia.org/wiki/Differentiable_programming",
      "summary": "Differentiable programming is a programming paradigm in which a numeric computer program can be differentiated throughout via automatic differentiation. This allows for gradient-based optimization of parameters in the program, often via gradient descent, as well as other learning approaches that are based on higher order derivative information. Differentiable programming has found use in a wide variety of areas, particularly scientific computing and artificial intelligence. One of the early proposals to adopt such a framework in a systematic fashion to improve upon learning algorithms was made by the Advanced Concepts Team at the European Space Agency in early 2016.\n\n"
    },
    {
      "id": "71912239",
      "title": "Diffusion model",
      "url": "https://en.wikipedia.org/wiki/Diffusion_model",
      "summary": "In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of three major components: the forward process, the reverse process, and the sampling procedure. The goal of diffusion models is to learn a diffusion process that generates the probability distribution of a given dataset. They learn the latent structure of a dataset by modeling the way in which data points diffuse through their latent space.In the case of computer vision, diffusion models can be applied to a variety of tasks, including image denoising, inpainting, super-resolution, and image generation. This typically involves training a neural network to sequentially denoise images blurred with Gaussian noise. The model is trained to reverse the process of adding noise to an image. After training to convergence, it can be used for image generation by starting with an image composed of random noise for the network to iteratively denoise. Announced on 13 April 2022, OpenAI's text-to-image model DALL-E 2 is an example that uses diffusion models for both the model's prior (which produces an image embedding given a text caption) and the decoder that generates the final image.  Diffusion models have recently found applications in natural language processing (NLP), particularly in areas like text generation and summarization.Diffusion models are typically formulated as markov chains and trained using variational inference. Examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations."
    },
    {
      "id": "1667059",
      "title": "Diffusion process",
      "url": "https://en.wikipedia.org/wiki/Diffusion_process",
      "summary": "In probability theory and statistics, diffusion processes are a class of continuous-time Markov process with almost surely continuous sample paths. Diffusion process is stochastic in nature and hence is used to model many real-life stochastic systems. Brownian motion, reflected Brownian motion and Ornstein\u2013Uhlenbeck processes are examples of diffusion processes. It is used heavily in statistical physics, statistical analysis, information theory, data science, neural networks, finance and marketing.\nA sample path of a diffusion process models the trajectory of a particle embedded in a flowing fluid and subjected to random displacements due to collisions with other particles, which is called Brownian motion.  The position of the particle is then random; its probability density function as a function of space and time is  governed by a convection\u2013diffusion equation.\n\n"
    },
    {
      "id": "148113",
      "title": "Digital art",
      "url": "https://en.wikipedia.org/wiki/Digital_art",
      "summary": "Digital art refers to any artistic work or practice that uses digital technology as part of the creative or presentation process. It can also refer to computational art that uses and engages with digital media.Since the 1960s, various names have been used to describe digital art, including computer art, electronic art, multimedia art and new media art.\n\n"
    },
    {
      "id": "18934863",
      "title": "Digital library",
      "url": "https://en.wikipedia.org/wiki/Digital_library",
      "summary": "A digital library, also called an online library, an internet library, a digital repository,  a library without walls, or a digital collection, is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations. The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability."
    },
    {
      "id": "9933471",
      "title": "Digital marketing",
      "url": "https://en.wikipedia.org/wiki/Digital_marketing",
      "summary": "Digital marketing is the component of marketing that uses the Internet and online-based digital technologies such as desktop computers, mobile phones, and other digital media and platforms to promote products and services. Its development during the 1990s and 2000s changed the way brands and businesses use technology for marketing. As digital platforms became increasingly incorporated into marketing plans and everyday life, and as people increasingly used digital devices instead of visiting physical shops, digital marketing campaigns have become prevalent, employing combinations of search engine optimization (SEO), search engine marketing (SEM), content marketing, influencer marketing, content automation, campaign marketing, data-driven marketing, e-commerce marketing, social media marketing, social media optimization, e-mail direct marketing, display advertising, e-books, and optical disks and games have become commonplace. Digital marketing extends to non-Internet channels that provide digital media, such as television, mobile phones (SMS and MMS), callbacks, and on-hold mobile ringtones. The extension to non-Internet channels differentiates digital marketing from online marketing.\n\n"
    },
    {
      "id": "53231",
      "title": "Discovery (observation)",
      "url": "https://en.wikipedia.org/wiki/Discovery_(observation)",
      "summary": "Discovery is the act of detecting something new, or something previously unrecognized as meaningful. Concerning sciences and academic disciplines, discovery is the observation of new phenomena, new actions, or new events and providing new reasoning to explain the knowledge gathered through such observations with previously acquired knowledge from abstract thought and everyday experiences. A discovery may sometimes be based on earlier discoveries, collaborations, or ideas. Some discoveries represent a radical breakthrough in knowledge or technology.\nNew discoveries are acquired through various senses and are usually assimilated, merging with pre-existing knowledge and actions. Questioning is a major form of human thought and interpersonal communication, and plays a key role in discovery. Discoveries are often made due to questions. Some discoveries lead to the invention of objects, processes, or techniques. A discovery may sometimes be based on earlier discoveries, collaborations or ideas, and the process of discovery requires at least the awareness that an existing concept or method can be modified or transformed. However, some discoveries also represent a radical breakthrough in knowledge."
    },
    {
      "id": "8492",
      "title": "Discrete mathematics",
      "url": "https://en.wikipedia.org/wiki/Discrete_mathematics",
      "summary": "Discrete mathematics is the study of mathematical structures that can be considered \"discrete\" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than \"continuous\" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic. By contrast, discrete mathematics excludes topics in \"continuous mathematics\" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics\".The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.\nResearch in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in \"discrete\" steps and store data in \"discrete\" bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems.\nAlthough the main objects of study in discrete mathematics are discrete objects, analytic methods from \"continuous\" mathematics are often employed as well.\nIn university curricula, discrete mathematics appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well. Some high-school-level discrete mathematics textbooks have appeared as well. At this level, discrete mathematics is sometimes seen as a preparatory course, like precalculus in this respect.The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.\n\n"
    },
    {
      "id": "237629",
      "title": "Distributed artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Distributed_artificial_intelligence",
      "summary": "Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems. \nMulti-agent systems and distributed problem solving are the two main DAI approaches. There are numerous applications and tools."
    },
    {
      "id": "71608",
      "title": "Document management system",
      "url": "https://en.wikipedia.org/wiki/Document_management_system",
      "summary": "A document management system (DMS) is usually a computerized system used to store, share, track and manage files or documents. Some systems include history tracking where a log of the various versions created and modified by different users is recorded. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.\n\n"
    },
    {
      "id": "519239",
      "title": "Domain-specific language",
      "url": "https://en.wikipedia.org/wiki/Domain-specific_language",
      "summary": "A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term \"domain-specific language\" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.\nThe line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing-complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.\n\n"
    },
    {
      "id": "323121",
      "title": "Donald O. Hebb",
      "url": "https://en.wikipedia.org/wiki/Donald_O._Hebb",
      "summary": "Donald Olding Hebb  (July 22, 1904 \u2013 August 20, 1985) was a Canadian psychologist who was influential in the area of neuropsychology, where he sought to understand how the function of neurons contributed to psychological processes such as learning. He is best known for his theory of Hebbian learning, which he introduced in his classic 1949 work The Organization of Behavior. He has been described as the father of neuropsychology and neural networks. A Review of General Psychology survey, published in 2002, ranked Hebb as the 19th most cited psychologist of the 20th century. His views on learning described behavior and thought in terms of brain function, explaining cognitive processes in terms of connections between neuron assemblies."
    },
    {
      "id": "1242713",
      "title": "Dynamic Bayesian network",
      "url": "https://en.wikipedia.org/wiki/Dynamic_Bayesian_network",
      "summary": "A dynamic Bayesian network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. \n\n"
    },
    {
      "id": "125297",
      "title": "Dynamic programming",
      "url": "https://en.wikipedia.org/wiki/Dynamic_programming",
      "summary": "Dynamic programming is both a mathematical optimization method and an algorithmic paradigm. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.\nIn both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.\nIf sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation."
    },
    {
      "id": "39208",
      "title": "Commerce",
      "url": "https://en.wikipedia.org/wiki/Commerce",
      "summary": "Commerce is the large-scale organized system of activities, functions, procedures and institutions that directly or indirectly contribute to the smooth, unhindered distribution and transfer of goods and services on a substantial scale and at the right time, place, quantity, quality and price through various channels from the original producers to the final consumers within local, regional, national or international economies. The diversity in the distribution of natural resources, differences of human needs and wants, and division of labour along with comparative advantage are the principal factors that give rise to commercial exchanges.Commerce consists of trade and aids to trade along the entire supply chain. Trade is the exchange of goods (including raw materials, intermediate and finished goods) and services between buyers and sellers in return for a price at traditional (or online) marketplaces. It is categorized into domestic trade, including retail and wholesale as well as local, regional and inter-regional transactions and foreign trade, encompassing import, export and entrep\u00f4t/re-export trades. Trade also involves the exchange of currencies, commodities  and securities in specialized exchange markets. Aids to trade or auxiliary commercial activities facilitate trade and include commercial intermediaries, banking and financial services, transportation, packaging, warehousing, communication, advertising and insurance. Their purpose is to remove hindrances related to direct personal contact, payments, savings, funding, separation of place and time, product protection and preservation, knowledge and risk. \nThe broader framework of commerce incorporates additional elements and factors such as laws and regulations (including intellectual property rights and antitrust laws), policies, tariffs and trade barriers, consumers and consumer trends, producers and production strategies, supply chains and their management, financial transactions (including those in financial markets), market dynamics (including supply and demand), technological innovation, competition and entrepreneurship, trade agreements, multinational corporations and small and medium-sized enterprisess (SMEs), and macroeconomic factors (like economic stability).\nCommerce drives economic growth, development and prosperity, promotes regional and international interdependence, fosters cultural exchange, creates jobs, improves people's standard of living by giving them access to a wider variety of goods and services, and encourages innovation and competition for better products. On the other hand, commerce can worsen economic inequality by concentrating wealth (and power) into the hands of a small number of individuals, and by prioritizing short-term profit over long-term sustainability and ethical, social, and environmental considerations, leading to environmental degradation, labor exploitation and disregard for consumer safety. Unregulated, it can lead to excessive consumption (generating undesirable waste) and unsustainable exploitation of nature (causing resource depletion). Harnessing commerce's benefits for the society while mitigating its drawbacks remains vital for policymakers, businesses and other stakeholders. \nCommerce traces its origins to ancient localized barter systems, leading to the establishment of periodic marketplaces, and culminating in the development of currencies for efficient trade. In medieval times, trade routes (like the Silk Road) with pivotal commercial hubs (like Venice) connected regions and continents, enabling long-distance trade and cultural exchange. From the 15th to the early 20th century, European colonial powers dominated global commerce on an unprecedented scale. In the 19th century, modern banking and stock exchanges along with the industrial revolution fundamentally reshaped commerce. In the post-colonial 20th century, free market principles gained ground,  multinational corporations and consumer economies thrived in U.S.-led capitalist countries and free trade agreements (like GATT and WTO) emerged, whereas communist economies encountered trade restrictions, limiting consumer choice. Notably, developing countries saw their share in world trade rise from a quarter to a third by the century's end. 21st century commerce is increasingly technology-driven (see e-commerce), globalized, intricately regulated, ethically responsible and sustainability-focused, with multilateral economic integrations (like the European Union and BRICS) leading to its reconfiguration.\n\n"
    },
    {
      "id": "19834151",
      "title": "ECML PKDD",
      "url": "https://en.wikipedia.org/wiki/ECML_PKDD",
      "summary": "ECML PKDD, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, is one of the leading academic conferences on machine learning and knowledge discovery, held in Europe every year.\n\n"
    },
    {
      "id": "8887731",
      "title": "Echo state network",
      "url": "https://en.wikipedia.org/wiki/Echo_state_network",
      "summary": "An echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behavior is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.\nAlternatively, one may consider a nonparametric Bayesian formulation of the output layer, under which: (i) a prior distribution is imposed over the output weights; and (ii) the output weights are marginalized out in the context of prediction generation, given the training data. This idea has been demonstrated in by using Gaussian priors, whereby a Gaussian process model with ESN-driven kernel function is obtained. Such a solution was shown to outperform ESNs with trainable (finite) sets of weights in several benchmarks.\nSome publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; (ii) Matlab code: an efficient matlab for an echo state network; (iii) ReservoirComputing.jl: an efficient Julia-based implementation of various types of echo state networks; and (iv) pyESN: simple echo state networks in Python.\n\n"
    },
    {
      "id": "3670866",
      "title": "Edge device",
      "url": "https://en.wikipedia.org/wiki/Edge_device",
      "summary": "In computer networking, an edge device is a device that provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.\n\n"
    },
    {
      "id": "1944675",
      "title": "Educational technology",
      "url": "https://en.wikipedia.org/wiki/Educational_technology",
      "summary": "Educational technology (commonly abbreviated as edutech, or edtech) is the combined use of computer hardware, software, and educational theory and practice to facilitate learning. When referred to with its abbreviation, \"EdTech,\" it often refers to the industry of companies that create educational technology. In EdTech Inc.: Selling, Automating and Globalizing Higher Education in the Digital Age, Tanner Mirrlees and Shahid Alvi (2019) argue \"EdTech is no exception to industry ownership and market rules\" and \"define the EdTech industries as all the privately owned companies currently involved in the financing, production and distribution of commercial hardware, software, cultural goods, services and platforms for the educational market with the goal of turning a profit. Many of these companies are US-based and rapidly expanding into educational markets across North America, and increasingly growing all over the world.\"In addition to the practical educational experience, educational technology is based on theoretical knowledge from various disciplines such as communication, education, psychology, sociology, artificial intelligence, and computer science. It encompasses several domains including learning theory, computer-based training, online learning, and m-learning where mobile technologies are used.\n\n"
    },
    {
      "id": "891719",
      "title": "Ehud Shapiro",
      "url": "https://en.wikipedia.org/wiki/Ehud_Shapiro",
      "summary": "Ehud Shapiro (Hebrew: \u05d0\u05d4\u05d5\u05d3 \u05e9\u05e4\u05d9\u05e8\u05d0; born 1955) is an Israeli scientist, artist, and entrepreneur, who is Professor of Computer Science and Biology at the Weizmann Institute of Science. With international reputation, he made fundamental contributions to many scientific disciplines, laying in each a long-term research agenda by asking a novel basic question and offering a first step towards answering it, including how to computerize the process of scientific discovery, by providing an algorithmic interpretation to Karl Popper's methodology of conjectures and refutations; how to automate program debugging, by algorithms for fault localization; how to unify parallel, distributed, and systems programming with a high-level logic-based programming language; how to use the metaverse as a foundation for social networking; how to devise molecular computers that can function as smart programmable drugs;  how to uncover the human cell lineage tree, via single-cell genomics; how to support digital democracy, by devising an alternative architecture to the digital realm.Shapiro was also an internet pioneer, entrepreneur, and a pioneer and proponent of digital democracy.Shapiro is the founder of the Ba Rock Band and  a founder of the Israeli political party \"Democratit\". He is a winner of two ERC (European Research Council) Advanced Grants.\n\n"
    },
    {
      "id": "64305316",
      "title": "Electrochemical RAM",
      "url": "https://en.wikipedia.org/wiki/Electrochemical_RAM",
      "summary": "Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration. An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from that of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technological challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials, Stanford University built an organic proton-based cell, and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL). In 2022, researchers at Massachusetts Institute of Technology built an inorganic, CMOS-compatible protonic technology that achieved near-ideal modulation characteristics using nanosecond fast pulses \n\n"
    },
    {
      "id": "216881",
      "title": "Electronic design automation",
      "url": "https://en.wikipedia.org/wiki/Electronic_design_automation",
      "summary": "Electronic design automation (EDA), also referred to as electronic computer-aided design (ECAD), is a category of software tools for designing electronic systems such as integrated circuits and printed circuit boards. The tools work together in a design flow that chip designers use to design and analyze entire semiconductor chips. Since a modern semiconductor chip can have billions of components, EDA tools are essential for their design; this article in particular describes EDA specifically with respect to integrated circuits (ICs)."
    },
    {
      "id": "190831",
      "title": "Electronic publishing",
      "url": "https://en.wikipedia.org/wiki/Electronic_publishing",
      "summary": "Electronic publishing (also referred to as publishing, digital publishing, or online publishing) includes the digital publication of e-books, digital magazines, and the development of digital libraries and catalogues. It also includes the editing of books, journals, and magazines to be posted on a screen (computer, e-reader, tablet, or smartphone)."
    },
    {
      "id": "371301",
      "title": "Electronic voting",
      "url": "https://en.wikipedia.org/wiki/Electronic_voting",
      "summary": "Electronic voting (also known as e-voting) is voting that uses electronic means to either aid or take care of casting and counting ballots.\nDepending on the particular implementation, e-voting may use standalone electronic voting machines (also called EVM) or computers connected to the Internet (online voting).  It may encompass a range of Internet services, from basic transmission of tabulated results to full-function online voting through common connectable household devices. The degree of automation may be limited to marking a paper ballot, or may be a comprehensive system of vote input, vote recording, data encryption and transmission to servers, and consolidation and tabulation of election results.\nA worthy e-voting system must perform most of these tasks while complying with a set of standards established by regulatory bodies, and must also be capable to deal successfully with strong requirements associated with security, accuracy, integrity, swiftness, privacy, auditability, accessibility, cost-effectiveness, scalability and ecological sustainability.\nElectronic voting technology can include punched cards, optical scan voting systems and specialized voting kiosks (including self-contained direct-recording electronic voting systems, or DRE). It can also involve transmission of ballots and votes via telephones, private computer networks, or the Internet.\nIn general, two main types of e-voting can be identified:\n\ne-voting which is physically supervised by representatives of governmental or independent electoral authorities (e.g. electronic voting machines located at polling stations);\nremote e-voting via the Internet (also called i-voting) where the voter submits his or her vote electronically to the election authorities, from any location.Electronic voting systems are used everywhere in many countries across the whole world, incl. Argentina, Australia, Bangladesh, Belgium, Brazil, Canada, France, Germany, India, Italy, Japan, Kazakhstan, South Korea, Malaysia, the Netherlands, Norway,  the Philippines, Spain, Switzerland, Thailand, the United Kingdom and the United States.\n\n"
    },
    {
      "id": "73176768",
      "title": "EleutherAI",
      "url": "https://en.wikipedia.org/wiki/EleutherAI",
      "summary": "EleutherAI () is a grass-roots non-profit artificial intelligence (AI) research group. The group, considered an open-source version of OpenAI, was formed in a Discord server in July 2020 to organize a replication of GPT-3. In early 2023, it formally incorporated as the EleutherAI Foundation, a non-profit research institute.\n\n"
    },
    {
      "id": "1336001",
      "title": "Email filtering",
      "url": "https://en.wikipedia.org/wiki/Email_filtering",
      "summary": "Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of messages at an SMTP server, possibly applying anti-spam techniques. Filtering can be applied to incoming emails as well as to outgoing ones.\nDepending on the calling environment, email filtering software can reject an item at the initial SMTP connection stage or pass it through unchanged for delivery to the user's mailbox.  It is also possible to redirect the message for delivery elsewhere, quarantine it for further checking, modify it or 'tag' it in any other way.\n\n"
    },
    {
      "id": "46630",
      "title": "Embedded system",
      "url": "https://en.wikipedia.org/wiki/Embedded_system",
      "summary": "An embedded system is a computer system\u2014a combination of a computer processor, computer memory, and input/output peripheral devices\u2014that has a dedicated function within a larger mechanical or electronic system. It is embedded as part of a complete device often including electrical or electronic hardware and mechanical parts. \nBecause an embedded system typically controls physical operations of the machine that it is embedded within, it often has real-time computing constraints.  Embedded systems control many devices in common use. In 2009, it was estimated that ninety-eight percent of all microprocessors manufactured were used in embedded systems.Modern embedded systems are often based on microcontrollers (i.e. microprocessors with integrated memory and peripheral interfaces), but ordinary microprocessors (using external chips for memory and peripheral interface circuits) are also common, especially in more complex systems. In either case, the processor(s) used may be types ranging from general purpose to those specialized in a certain class of computations, or even custom designed for the application at hand. A common standard class of dedicated processors is the digital signal processor (DSP).\nSince the embedded system is dedicated to specific tasks, design engineers can optimize it to reduce the size and cost of the product and increase its reliability and performance. Some embedded systems are mass-produced, benefiting from economies of scale.\nEmbedded systems range in size from portable personal devices such as digital watches and MP3 players to bigger machines like home appliances, industrial assembly lines, robots, transport vehicles, traffic light controllers, and medical imaging systems. Often they constitute subsystems of other machines like avionics in aircraft and astrionics in spacecraft. Large installations like factories, pipelines and electrical grids rely on multiple embedded systems networked together. Generalized through software customization, embedded systems such as programmable logic controllers frequently comprise their functional units.\nEmbedded systems range from those low in complexity, with a single microcontroller chip, to very high with multiple units, peripherals and networks, which may reside in equipment racks or across large geographical areas connected via long-distance communications lines.\n\n"
    },
    {
      "id": "1455062",
      "title": "Empirical risk minimization",
      "url": "https://en.wikipedia.org/wiki/Empirical_risk_minimization",
      "summary": "Empirical risk minimization is a principle in statistical learning theory which defines a family of learning algorithms based on evaluating performance over a known and fixed dataset. The core idea is based on an application of the law of large numbers; more specifically, we cannot know exactly how well a predictive algorithm will work in practice (i.e. the true \"risk\") because we don't know the true distribution of the data, but we can instead estimate and optimize the performance of the algorithm on a known set of training data. The performance over the known set of training data is referred to as the empirical risk.\n\n"
    },
    {
      "id": "22212276",
      "title": "Ensemble learning",
      "url": "https://en.wikipedia.org/wiki/Ensemble_learning",
      "summary": "In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\nUnlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives."
    },
    {
      "id": "37531624",
      "title": "Logical consequence",
      "url": "https://en.wikipedia.org/wiki/Logical_consequence",
      "summary": "Logical consequence (also entailment) is a fundamental concept in logic which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises? and What does it mean for a conclusion to be a consequence of premises? All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.Logical consequence is necessary and formal, by way of examples that explain with formal proof and models of interpretation. A sentence is said to be a logical consequence of a set of sentences, for a given language, if and only if, using only logic (i.e., without regard to any personal interpretations of the sentences) the sentence must be true if every sentence in the set is true.Logicians make precise accounts of logical consequence regarding a given language \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  , either by constructing a deductive system for \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n   or by formal intended semantics for language \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  . The Polish logician Alfred Tarski identified three features of an adequate characterization of entailment: (1) The logical consequence relation relies on the logical form of the sentences: (2) The relation is a priori, i.e., it can be determined with or without regard to empirical evidence (sense experience); and (3) The logical consequence relation has a modal component.\n\n"
    },
    {
      "id": "1010494",
      "title": "Enterprise information system",
      "url": "https://en.wikipedia.org/wiki/Enterprise_information_system",
      "summary": "An Enterprise Information System (EIS) is any kind of information system which improves the functions of enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.The word enterprise can have various connotations. Frequently the term is used only to refer to very large organizations such as multi-national companies or public-sector organizations. However, the term may be used to mean virtually anything, by virtue of it having become a corporate-speak buzzword."
    },
    {
      "id": "2302514",
      "title": "Enterprise software",
      "url": "https://en.wikipedia.org/wiki/Enterprise_software",
      "summary": "Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than its individual users. Enterprise software is an integral part of a computer-based information system, handling a number of business operations, for example to enhance business and management reporting tasks, or support production operations and back office functions. Enterprise systems must process information at a relatively high speed.Services provided by enterprise software are typically business-oriented tools. As companies and other organizations have similar departments and systems, enterprise software is often available as a suite of customizable programs. Function-specific enterprise software uses include database management, customer relationship management, supply chain management and business process management.\n\n"
    },
    {
      "id": "461509",
      "title": "Errors and residuals",
      "url": "https://en.wikipedia.org/wiki/Errors_and_residuals",
      "summary": "In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its \"true value\" (not necessarily observable). The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.\nIn econometrics, \"errors\" are also called disturbances.\n\n"
    },
    {
      "id": "13659583",
      "title": "Ethics of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
      "summary": "The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems. It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics.\n\n"
    },
    {
      "id": "190837",
      "title": "Evolutionary algorithm",
      "url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
      "summary": "In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\nEvolutionary algorithms can be seen as a kind of Monte-Carlo method.\n\n"
    },
    {
      "id": "46583121",
      "title": "Existential risk from artificial general intelligence",
      "url": "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence",
      "summary": "Existential risk from artificial general intelligence is the idea that substantial progress in artificial general intelligence (AGI) could result in human extinction or an irreversible global catastrophe.One argument goes as follows: human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass humanity in general intelligence and become superintelligent, then it could become difficult or impossible to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.The plausibility of existential catastrophe due to AI is widely debated, and hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge, and whether practical scenarios for AI takeovers exist. Concerns about superintelligence have been voiced by leading computer scientists and tech CEOs such as Geoffrey Hinton, Yoshua Bengio, Alan Turing, Elon Musk, and OpenAI CEO Sam Altman. In 2022, a survey of AI researchers with a 17% response rate found that the majority of respondents believed there is a 10 percent or greater chance that our inability to control AI will cause an existential catastrophe. In 2023, hundreds of AI experts and other notable figures signed a statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\" Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak and United Nations Secretary-General Ant\u00f3nio Guterres called for an increased focus on global AI regulation.\nTwo sources of concern stem from the problems of AI control and alignment: controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would resist attempts to disable it or change its goals, as that would prevent it from accomplishing its present goals. It would be extremely difficult to align a superintelligence with the full breadth of significant human values and constraints. In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.A third source of concern is that a sudden \"intelligence explosion\" might take an unprepared human race by surprise. Such scenarios consider the possibility that an AI that is more intelligent than its creators might be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers and society at large to control. Empirically, examples like AlphaZero teaching itself to play Go show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such systems do not involve altering their fundamental architecture."
    },
    {
      "id": "470752",
      "title": "Expectation\u2013maximization algorithm",
      "url": "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm",
      "summary": "In statistics, an expectation\u2013maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step. It can be used, for example, to estimate a mixture of gaussians, or to solve the multiple linear regression problem.\n\n"
    },
    {
      "id": "10136",
      "title": "Expert system",
      "url": "https://en.wikipedia.org/wiki/Expert_system",
      "summary": "In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if\u2013then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software. \nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\n\n"
    },
    {
      "id": "54575571",
      "title": "Explainable artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence",
      "summary": "Explainable AI (XAI), often overlapping with Interpretable AI, or Explainable Machine Learning (XML), either refers to an AI system over which it is possible for humans to retain intellectual oversight, or to the methods to achieve this. The main focus is usually on the reasoning behind the decisions or predictions made by the AI which are made more understandable and transparent. XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.XAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.Machine learning (ML) algorithms used in AI can be categorized as white-box or black-box. White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and can hardly be understood even by domain experts. XAI algorithms follow the three principles of transparency, interpretability, and explainability. A model is transparent \u201cif the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\u201d Interpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans. Explainability is a concept that is recognized as important, but a consensus definition is not available. One possibility is \u201cthe collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)\u201d. If algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts.Sometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions. Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image and text prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms. Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset.AI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command \"maximize the accuracy of assessing how positive film reviews are in the test dataset.\" The AI may learn useful general rules from the test set, such as \"reviews containing the word \"horrible\" are likely to be negative.\" However, it may also learn inappropriate rules, such as \"reviews containing 'Daniel Day-Lewis' are usually positive\"; such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be \"cheating\" or \"unfair.\" A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real-world data outside the test set."
    },
    {
      "id": "602401",
      "title": "Facial recognition system",
      "url": "https://en.wikipedia.org/wiki/Facial_recognition_system",
      "summary": "A facial recognition system is a technology potentially capable of matching a human face from a digital image or a video frame against a database of faces. Such a system is typically employed to authenticate users through ID verification services, and works by pinpointing and measuring facial features from a given image.Development began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics. Because computerized facial recognition involves the measurement of a human's physiological characteristics, facial recognition systems are categorized as biometrics. Although the accuracy of facial recognition systems as a biometric technology is lower than iris recognition, fingerprint image acquisition, palm recognition or voice recognition, it is widely adopted due to its contactless process. Facial recognition systems have been deployed in advanced human\u2013computer interaction, video surveillance, law enforcement, passenger screening, decisions on employment and housing and automatic indexing of images.Facial recognition systems are employed throughout the world today by governments and private companies. Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness. The use of facial recognition systems has also raised controversy, with claims that the systems violate citizens' privacy, commonly make incorrect identifications, encourage gender norms and racial profiling, and do not protect important biometric data. The appearance of synthetic media such as deepfakes has also raised concerns about its security. These claims have led to the ban of facial recognition systems in several cities in the United States. Growing societal concerns led social networking company Meta Platforms to shut down its Facebook facial recognition system in 2021, deleting the face scan data of more than one billion users. The change represented one of the largest shifts in facial recognition usage in the technology's history."
    },
    {
      "id": "253492",
      "title": "Factor analysis",
      "url": "https://en.wikipedia.org/wiki/Factor_analysis",
      "summary": "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus \"error\" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.Simply put, the factor loading of a variable quantifies the extent to which the variable is related to a given factor.A common rationale behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in psychometrics, personality psychology, biology, marketing, product management, operations research, finance, and machine learning. It may help to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality."
    },
    {
      "id": "62683332",
      "title": "Fairness (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Fairness_(machine_learning)",
      "summary": "Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. For example gender, ethnicity, sexual orientation or disability. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers."
    },
    {
      "id": "43286898",
      "title": "False positives and false negatives",
      "url": "https://en.wikipedia.org/wiki/False_positives_and_false_negatives",
      "summary": "A false positive is an error in binary classification in which a test result incorrectly indicates the presence of a condition (such as a disease when the disease is not present), while a false negative is the opposite error, where the test result incorrectly indicates the absence of a condition when it is actually present. These are the two kinds of errors in a binary test, in contrast to the two kinds of correct result (a true positive and a true negative). They are also known in medicine as a false positive (or false negative) diagnosis, and in statistical classification as a false positive (or false negative) error.In statistical hypothesis testing, the analogous concepts are known as type I and type II errors, where a positive result corresponds to rejecting the null hypothesis, and a negative result corresponds to not rejecting the null hypothesis. The terms are often used interchangeably, but there are differences in detail and interpretation due to the differences between medical testing and statistical hypothesis testing.\n\n"
    },
    {
      "id": "1299404",
      "title": "Feature (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Feature_(machine_learning)",
      "summary": "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\n\n"
    },
    {
      "id": "38870173",
      "title": "Feature learning",
      "url": "https://en.wikipedia.org/wiki/Feature_learning",
      "summary": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform  a specific task.\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data have not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\nFeature learning can be either supervised, unsupervised or self-supervised.\n\nIn supervised feature learning, features are learned using labeled input data. Labeled data includes input-label pairs where the input is given to the model and it must produce the ground truth label as the correct answer. This can be leveraged to generate feature representations with the model which result in high label prediction accuracy. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.\nIn unsupervised feature learning, features are learned with unlabeled input data by analyzing the relationship between points in the dataset.  Examples include dictionary learning, independent component analysis, matrix factorization and various forms of clustering.\nIn self-supervised feature learning, features are learned using unlabeled data like unsupervised learning, however input-label pairs are constructed from each data point, which enables learning the structure of the data through supervised methods such as gradient descent. Classical examples include word embeddings and autoencoders. SSL has since been applied to many modalities through the use of deep neural network architectures such as CNNs and transformers."
    },
    {
      "id": "60992857",
      "title": "Federated learning",
      "url": "https://en.wikipedia.org/wiki/Federated_learning",
      "summary": "Federated learning (also known as collaborative learning) is a sub-field of machine learning focusing on settings in which multiple entities (often referred to as clients) collaboratively train a model while ensuring that their data remains decentralized. This stands in contrast to machine learning settings in which data is centrally stored. One of the primary defining characteristics of federated learning is data heterogeneity. Due to the decentralized nature of the clients' data, there is no no guarantee that data samples held by each client are independently and identically distributed.\nFederated learning is generally concerned with and motivated by issues such as data privacy, data minimization, and data access rights. Its applications involve a variety of research areas including defense, telecommunications, the Internet of Things, and pharmaceuticals.\n\n"
    },
    {
      "id": "1706332",
      "title": "Feedforward neural network",
      "url": "https://en.wikipedia.org/wiki/Feedforward_neural_network",
      "summary": "A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one direction\u2014forward\u2014from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops, in contrast to recurrent neural networks, which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the \"vanilla\" neural networks.\n\n"
    },
    {
      "id": "44359594",
      "title": "Fei-Fei Li",
      "url": "https://en.wikipedia.org/wiki/Fei-Fei_Li",
      "summary": "Fei-Fei Li (\u674e\u98de\u98de; born 1976) is a China-born American computer scientist, known for establishing ImageNet, the dataset that enabled rapid advances in computer vision in the 2010s. She is Sequoia Capital professor of computer science at Stanford University and former board director at Twitter. Li is a co-director of the Stanford Institute for Human-Centered Artificial Intelligence and a co-director of the Stanford Vision and Learning Lab. She served as the director of the Stanford Artificial Intelligence Laboratory from 2013 to 2018.In 2017, she co-founded AI4ALL, a nonprofit organization working to increase diversity and inclusion in the field of artificial intelligence. Her research expertise includes artificial intelligence, machine learning, deep learning, computer vision and cognitive neuroscience.Li was elected as a member of the National Academy of Engineering in 2020, of the National Academy of Medicine in 2020, and of the American Academy of Arts and Sciences in 2021."
    },
    {
      "id": "41916",
      "title": "Financial market",
      "url": "https://en.wikipedia.org/wiki/Financial_market",
      "summary": "A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.\nThe term \"market\" is sometimes used for what are more strictly exchanges, organizations that facilitate the trade in financial securities, e.g., a stock exchange or commodity exchange. This may be a physical location (such as the New York Stock Exchange (NYSE), London Stock Exchange (LSE), JSE Limited (JSE), Bombay Stock Exchange (BSE)) or an electronic system such as NASDAQ. Much trading of stocks takes place on an exchange; still, corporate actions (merger, spinoff) are outside an exchange, while any two companies or people, for whatever reason, may agree to sell the stock from the one to the other without using an exchange.\nTrading of currencies and bonds is largely on a bilateral basis, although some bonds trade on a stock exchange, and people are building electronic systems for these as well, to stock exchanges. There are also global initiatives such as the United Nations Sustainable Development Goal 10 which has a target to improve regulation and monitoring of global financial markets.\n\n"
    },
    {
      "id": "60929882",
      "title": "Flux (machine-learning framework)",
      "url": "https://en.wikipedia.org/wiki/Flux_(machine-learning_framework)",
      "summary": "Flux is an open-source machine-learning software library and ecosystem written in Julia. Its current stable release is v0.14.5 . It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design. For example, GPU support is implemented transparently by CuArrays.jl This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings, such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++. Flux joined NumFOCUS as an affiliated project in December of 2021.Flux's focus on interoperability has enabled, for example, support for Neural Differential Equations, by fusing Flux.jl and DifferentialEquations.jl into DiffEqFlux.jl.Flux supports recurrent and convolutional networks. It is also capable of differentiable programming through its source-to-source automatic differentiation package, Zygote.jl.Julia is a popular language in machine-learning and Flux.jl is its most highly regarded machine-learning repository. A demonstration compiling Julia code to run in Google's tensor processing unit (TPU) received praise from Google Brain AI lead Jeff Dean.Flux has been used as a framework to build neural networks that work with homomorphic encrypted data without ever decrypting it. This kind of application is envisioned to be central for privacy to future API using machine-learning models.Flux.jl is an intermediate representation for running high level programs on CUDA hardware. It was the predecessor to CUDAnative.jl which is also a GPU programming language.\n\n"
    },
    {
      "id": "74609356",
      "title": "Force control",
      "url": "https://en.wikipedia.org/wiki/Force_control",
      "summary": "Force control is the control of the force with which a machine or the manipulator of a robot acts on an object or its environment. By controlling the contact force, damage to the machine as well as to the objects to be processed and injuries when handling people can be prevented. In manufacturing tasks, it can compensate for errors and reduce wear by maintaining a uniform contact force. Force control achieves more consistent results than position control, which is also used in machine control. Force control can be used as an alternative to the usual motion control, but is usually used in a complementary way, in the form of hybrid control concepts. The acting force for control is usually measured via force transducers or estimated via the motor current.\nForce control has been the subject of research for almost three decades and is increasingly opening up further areas of application thanks to advances in sensor and actuator technology and new control concepts. Force control is particularly suitable for contact tasks that serve to mechanically process workpieces, but it is also used in telemedicine, service robot and the scanning of surfaces.\nFor force measurement, force sensors exist that can measure forces and torques in all three spatial directions. Alternatively, the forces can also be estimated without sensors, e.g. on the basis of the motor currents. Indirect force control by modeling the robot as a mechanical resistance (impedance) and direct force control in parallel or hybrid concepts are used as control concepts. Adaptive approaches, fuzzy controllers and machine learning for force control are currently the subject of research.\n\n"
    },
    {
      "id": "161883",
      "title": "Formal methods",
      "url": "https://en.wikipedia.org/wiki/Formal_methods",
      "summary": "In computer science, formal methods are mathematically rigorous techniques for the specification, development, analysis, and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.Formal methods employ a variety of theoretical computer science fundamentals, including logic calculi, formal languages, automata theory, control theory, program semantics, type systems, and type theory."
    },
    {
      "id": "351887",
      "title": "Friendly artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Friendly_artificial_intelligence",
      "summary": "Friendly artificial intelligence (also friendly AI or FAI) is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to fostering the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behavior and ensuring it is adequately constrained.\n\n"
    },
    {
      "id": "2422496",
      "title": "Fuzzy clustering",
      "url": "https://en.wikipedia.org/wiki/Fuzzy_clustering",
      "summary": "Fuzzy clustering (also referred to as soft clustering or soft k-means) is a form of clustering in which each data point can belong to more than one cluster.\nClustering or cluster analysis involves assigning data points to clusters such that items in the same cluster are as similar as possible, while items belonging to different clusters are as dissimilar as possible. Clusters are identified via similarity measures. These similarity measures include distance, connectivity, and intensity. Different similarity measures may be chosen based on the data or the application.\n\n"
    },
    {
      "id": "49180",
      "title": "Fuzzy logic",
      "url": "https://en.wikipedia.org/wiki/Fuzzy_logic",
      "summary": "Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\nThe term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by mathematician Lotfi Zadeh.  Fuzzy logic had, however,  been studied since the 1920s, as infinite-valued logic\u2014notably by \u0141ukasiewicz and Tarski.Fuzzy logic is based on the observation that people make decisions based on imprecise and non-numerical information. Fuzzy models or fuzzy sets are mathematical means of representing vagueness and imprecise information (hence the term fuzzy). These models have the capability of recognising, representing, manipulating, interpreting, and using data and information that are vague and lack certainty.Fuzzy logic has been applied to many fields, from control theory to artificial intelligence.\n\n"
    },
    {
      "id": "5218",
      "title": "Central processing unit",
      "url": "https://en.wikipedia.org/wiki/Central_processing_unit",
      "summary": "A central processing unit (CPU)\u2014also called a central processor or main processor\u2014is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic\u2013logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components. Modern CPUs devote a lot of semiconductor area to caches and instruction-level parallelism to increase performance and to CPU modes to support operating systems and virtualization. \nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to support CPU-level multithreading.An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC).\n\n"
    },
    {
      "id": "50569499",
      "title": "Gated recurrent unit",
      "url": "https://en.wikipedia.org/wiki/Gated_recurrent_unit",
      "summary": "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM. \nGRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs showed that gating is indeed helpful in general, and Bengio's team came to no concrete conclusion on which of the two gating units was better.\n\n"
    },
    {
      "id": "302944",
      "title": "Gaussian process",
      "url": "https://en.wikipedia.org/wiki/Gaussian_process",
      "summary": "In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\nThe concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.\nGaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly. Such quantities include the average value of the process over a range of times and the error in estimating the average using sample values at a small set of times. While exact models often scale poorly as the amount of data increases, multiple approximation methods have been developed which often retain good accuracy while drastically reducing computation time."
    },
    {
      "id": "50523514",
      "title": "Gboard",
      "url": "https://en.wikipedia.org/wiki/Gboard",
      "summary": "Gboard is a virtual keyboard app developed by Google for Android and iOS devices. It was first released on iOS in May 2016, followed by a release on Android in December 2016, debuting as a major update to the already-established Google Keyboard app on Android.\nGboard features Google Search, including web results (removed since April 2020) and predictive answers, easy searching and sharing of GIF and emoji content, a predictive typing engine suggesting the next word depending on context, and multilingual language support. Updates to the keyboard have enabled additional functionality, including GIF suggestions, options for a dark color theme or adding a personal image as the keyboard background, support for voice dictation, next-phrase prediction, and hand-drawn emoji recognition. At the time of its launch on iOS, the keyboard only offered support for the English language, with more languages being gradually added in the following months, whereas on Android, the keyboard supported more than 100 languages at the time of release.\nIn August 2018, Gboard passed 1 billion installs on the Google Play Store, making it one of the most popular Android apps. This is measured by the Google Play Store and includes downloads by users as well as pre-installed instances of the app."
    },
    {
      "id": "74637995",
      "title": "Gemini (language model)",
      "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
      "summary": "Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, and Gemini Nano, it was announced on December 6, 2023, positioned as a contender to OpenAI's GPT-4. It powers the generative artificial intelligence chatbot of the same name.\n\n"
    },
    {
      "id": "7408685",
      "title": "General game playing",
      "url": "https://en.wikipedia.org/wiki/General_game_playing",
      "summary": "General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully. For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.General video game playing  (GVGP) is the concept of GGP adjusted to the purpose of playing video games. For video games, game rules have to be either learnt over multiple iterations by artificial players like TD-Gammon, or are predefined manually in a domain-specific language and sent in advance to artificial players like in traditional GGP. Starting in 2013, significant progress was made following the deep reinforcement learning approach, including the development of programs that can learn to play Atari 2600 games as well as a program that can learn to play Nintendo Entertainment System games.The first commercial usage of general game playing technology was Zillions of Games in 1998. General game playing was also proposed for trading agents in supply chain management thereunder price negotiation in online auctions from 2003 on.\n\n"
    },
    {
      "id": "39156893",
      "title": "Generalization (learning)",
      "url": "https://en.wikipedia.org/wiki/Generalization_(learning)",
      "summary": "Generalization is the concept that humans, other animals, and artificial neural networks use past learning in present situations of learning if the conditions in the situations are regarded as similar. The learner uses generalized patterns, principles, and other similarities between past experiences and novel experiences to more efficiently navigate the world. For example, if a person has learned in the past that every time they eat an apple, their throat becomes itchy and swollen, they might assume they are allergic to all fruit. When this person is offered a banana to eat, they reject it upon assuming they are also allergic to it through generalizing that all fruits cause the same reaction. Although this generalization about being allergic to all fruit based on experiences with one fruit could be correct in some cases, it may not be correct in all. Both positive and negative effects have been shown in education through learned generalization and its contrasting notion of discrimination learning."
    },
    {
      "id": "12746",
      "title": "Generalization",
      "url": "https://en.wikipedia.org/wiki/Generalization",
      "summary": "A generalization is a form of abstraction whereby common properties of specific instances are formulated as general concepts or claims. Generalizations posit the existence of a domain or set of elements, as well as one or more common characteristics shared by those elements (thus creating a conceptual model). As such, they are the essential basis of all valid deductive inferences (particularly in logic, mathematics and science), where the process of verification is necessary to determine whether a generalization holds true for any given situation.\nGeneralization can also be used to refer to the process of identifying the parts of a whole, as belonging to the whole. The parts, which might be unrelated when left on their own, may be brought together as a group, hence belonging to the whole by establishing a common relation between them.\nHowever, the parts cannot be generalized into a whole\u2014until a common relation is established among all parts. This does not mean that the parts are unrelated, only that no common relation has been established yet for the generalization.\nThe concept of generalization has broad application in many connected disciplines, and might sometimes have a more specific meaning in a specialized context (e.g. generalization in psychology, generalization in learning).In general, given two related concepts A and B, A is a \"generalization\" of B (equiv., B is a special case of A) if and only if both of the following hold:\n\nEvery instance of concept B is also an instance of concept A.\nThere are instances of concept A which are not instances of concept B.For example, the concept animal is a generalization of the concept bird, since every bird is an animal, but not all animals are birds (dogs, for instance). For more, see Specialisation (biology)."
    },
    {
      "id": "747122",
      "title": "Generalized linear model",
      "url": "https://en.wikipedia.org/wiki/Generalized_linear_model",
      "summary": "In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\nGeneralized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression. They proposed an iteratively reweighted least squares method for maximum likelihood estimation (MLE) of the model parameters. MLE remains popular and is the default method on many statistical computing packages. Other approaches, including Bayesian regression and least squares fitting to variance stabilized responses, have been developed.\n\n"
    },
    {
      "id": "50073184",
      "title": "Generative adversarial network",
      "url": "https://en.wikipedia.org/wiki/Generative_adversarial_network",
      "summary": "A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.\nGiven a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning.The core idea of a GAN is based on the \"indirect\" training through the discriminator, another neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.\nGANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.\n\n"
    },
    {
      "id": "1222578",
      "title": "Generative model",
      "url": "https://en.wikipedia.org/wiki/Generative_model",
      "summary": "In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following Jebara (2004):\n\nA generative model is a statistical model of the joint probability distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n   on given observable variable X and target variable Y;\nA discriminative model is a model of the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        \u2223\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X=x)}\n   of the target Y, given an observation x; and\nClassifiers computed without using a probability model are also referred to loosely as \"discriminative\".The distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\nStandard examples of each, all of which are linear classifiers, are:\n\ngenerative classifiers:\nnaive Bayes classifier and\nlinear discriminant analysis\ndiscriminative model:\nlogistic regressionIn application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n   (discriminative model), and base classification on that; or one can estimate the joint distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n   (generative model), from that compute the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n  , and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.\n\n"
    },
    {
      "id": "40254",
      "title": "Genetic algorithm",
      "url": "https://en.wikipedia.org/wiki/Genetic_algorithm",
      "summary": "In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles, hyperparameter optimization, causal inference, etc."
    },
    {
      "id": "507174",
      "title": "Geoffrey Hinton",
      "url": "https://en.wikipedia.org/wiki/Geoffrey_Hinton",
      "summary": "Geoffrey Everest Hinton  (born 6 December 1947) is a British-Canadian  computer scientist and cognitive psychologist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing his departure from Google in May 2023, citing concerns about the risks of artificial intelligence (AI) technology. In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularised the backpropagation algorithm for training multi-layer neural networks, although they were not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community. The dramatic image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky and Ilya Sutskever for the ImageNet challenge 2012 was a breakthrough in the field of computer vision.Hinton received the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", together with Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the \"Godfathers of Deep Learning\", and have continued to give public talks together.In May 2023, Hinton announced his resignation from Google to be able to \"freely speak out about the risks of A.I.\" He has voiced concerns about deliberate misuse by malicious actors, technological unemployment, and existential risk from artificial general intelligence.\n\n"
    },
    {
      "id": "12398",
      "title": "Geographic information system",
      "url": "https://en.wikipedia.org/wiki/Geographic_information_system",
      "summary": "A geographic information system (GIS) consists of integrated computer hardware and software that store, manage, analyze, edit, output, and visualize geographic data. Much of this often happens within a spatial database, however, this is not essential to meet the definition of a GIS. In a broader sense, one may consider such a system also to include human users and support staff, procedures and workflows, the body of knowledge of relevant concepts and methods, and institutional organizations.\nThe uncounted plural, geographic information systems, also abbreviated GIS, is the most common term for the industry and profession concerned with these systems. It is roughly synonymous with geoinformatics. The academic discipline that studies these systems and their underlying geographic principles, may also be abbreviated as GIS, but the unambiguous GIScience is more common. GIScience is often considered a subdiscipline of geography within the branch of technical geography.\nGeographic information systems are utilized in multiple technologies, processes, techniques and methods. They are attached to various operations and numerous applications, that relate to: engineering, planning, management, transport/logistics, insurance, telecommunications, and business. For this reason, GIS and location intelligence applications are at the foundation of location-enabled services, which rely on geographic analysis and visualization.\nGIS provides the capability to relate previously unrelated information, through the use of location as the \"key index variable\". Locations and extents that are found in the Earth's spacetime are able to be recorded through the date and time of occurrence, along with x, y, and z coordinates; representing, longitude (x), latitude (y), and elevation (z). All Earth-based, spatial\u2013temporal, location and extent references should be relatable to one another, and ultimately, to a \"real\" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry and studies."
    },
    {
      "id": "60898046",
      "title": "Geolitica",
      "url": "https://en.wikipedia.org/wiki/Geolitica",
      "summary": "PredPol, Inc, now known as Geolitica, is a predictive policing company that attempts to predict property crimes using predictive analytics. PredPol is also the name of the software the company produces.\nPredPol began as a project of the Los Angeles Police Department (LAPD) and University of California, Los Angeles professor Jeff Brantingham. PredPol has produced a patented algorithm, which is based on a model used to predict earthquake aftershocks. \nAs of 2020, PredPol's algorithm is the most commonly used predictive policing algorithm in the U.S. Police departments that use PredPol are given printouts of jurisdiction maps that denote areas where crime has been predicted to occur throughout the day. The Los Angeles Times reported that officers are expected to patrol these areas during their shifts, as the system tracks their movements via the GPS in their patrol cars. Scholar Ruha Benjamin called PredPol a \"crime production algorithm,\" as police officers then more heavily patrol these predicted crime zones, expecting to see crime, which leads to a self-fulfilling prophecy.In an August 2023 earnings call, the CEO of SoundThinking announced that the company had begun the process of absorbing parts of Geolitica, including its engineering team, patents, and customers.  According to SoundThinking, Geolitica would cease operations at the end of 2023."
    },
    {
      "id": "50336055",
      "title": "Glossary of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence",
      "summary": "This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.\n\n"
    },
    {
      "id": "72798",
      "title": "Goofy",
      "url": "https://en.wikipedia.org/wiki/Goofy",
      "summary": "Goofy is a cartoon character created by The Walt Disney Company. He is a tall, anthropomorphic dog who typically wears a turtle neck and vest, with pants, shoes, white gloves, and a tall hat originally designed as a rumpled fedora. Goofy is a close friend of Mickey Mouse and Donald Duck, and is Max Goof's father. He is normally characterized as hopelessly clumsy and dim-witted, yet this interpretation is not always definitive; occasionally, Goofy is shown as intuitive and clever, albeit in his own unique, eccentric way.\nGoofy debuted in animated cartoons, starting in 1932 with Mickey's Revue as Dippy Dawg, who is older than Goofy would come to be. Later the same year, he was re-imagined as a younger character, now called Goofy, in the short The Whoopee Party. During the 1930s, he was used extensively as part of a comedy trio with Mickey and Donald. Starting in 1939, Goofy was given his own series of shorts that were popular in the 1940s and early 1950s. Two Goofy shorts were nominated for an Oscar: How to Play Football (1944) and Aquamania (1961). He also co-starred in a short series with Donald, including Polar Trappers (1938), where they first appeared without Mickey Mouse. Three more Goofy shorts were produced in the 1960s after which Goofy was only seen in television and Disney comics. He returned to theatrical animation in 1983 with Mickey's Christmas Carol. His most recent theatrical appearance was How to Hook Up Your Home Theater in 2007. Goofy has also been featured in television, most extensively in Goof Troop (1992), House of Mouse (2001\u20132003), Mickey Mouse Clubhouse (2006\u20132016), Mickey Mouse (2013\u20132019), Mickey and the Roadster Racers / Mickey Mouse Mixed-Up Adventures (2017\u20132021), and Mickey Mouse Funhouse (2021\u2013present).\nOriginally known as Dippy Dawg, the character is more commonly known simply as \"Goofy\", a name used in his short film series. In his 1950s cartoons, he usually played a character called George G. Geef. Sources from the Goof Troop continuity give the character's full name as G. G. \"Goofy\" Goof, likely in reference to the 1950s name. In many other sources, both animated and comics, the surname Goof continues to be used. In other 2000s-era comics, the character's full name has occasionally been given as Goofus D. Dawg."
    },
    {
      "id": "38651188",
      "title": "Google APIs",
      "url": "https://en.wikipedia.org/wiki/Google_APIs",
      "summary": "Google APIs are application programming interfaces (APIs) developed by Google which allow communication with Google Services and their integration to other services. Examples of these include Search, Gmail, Translate or Google Maps. Third-party apps can use these APIs to take advantage of or extend the functionality of the existing services.\nThe APIs provide functionality like analytics, machine learning as a service (the Prediction API) or access to user data (when permission to read the data is given). Another important example is an embedded Google map on a website, which can be achieved using the Static Maps API, Places API or Google Earth API.\n\n"
    },
    {
      "id": "42411494",
      "title": "Google Cloud Platform",
      "url": "https://en.wikipedia.org/wiki/Google_Cloud_Platform",
      "summary": "Google Cloud Platform (GCP), offered by Google, is a suite of cloud computing services that provides a series of modular cloud services including computing, data storage, data analytics, and machine learning, alongside a set of management tools. It runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, and Google Docs, according to Verma, et.al. Registration requires a credit card or bank account details.Google Cloud Platform provides infrastructure as a service, platform as a service, and serverless computing environments.\nIn April 2008, Google announced App Engine, a platform for developing and hosting web applications in Google-managed data centers, which was the first cloud computing service from the company. The service became generally available in November 2011. Since the announcement of App Engine, Google added multiple cloud services to the platform.\nGoogle Cloud Platform is a part of Google Cloud, which includes the Google Cloud Platform public cloud infrastructure, as well as Google Workspace (G Suite), enterprise versions of Android and ChromeOS, and application programming interfaces (APIs) for machine learning and enterprise mapping services.\n\n"
    },
    {
      "id": "41755648",
      "title": "Google DeepMind",
      "url": "https://en.wikipedia.org/wiki/Google_DeepMind",
      "summary": "DeepMind Technologies Limited, doing business as Google DeepMind, is a British-American artificial intelligence research laboratory which serves as a subsidiary of Google. Founded in the UK in 2010, it was acquired by Google in 2014, The company is based in London, with research centres in Canada, France, Germany and the United States.\nGoogle DeepMind has created neural network models that learn how to play video games in a fashion similar to that of humans, as well as Neural Turing machines (neural networks that can access external memory like a conventional Turing machine), resulting in a computer that loosely resembles short-term memory in the human brain.DeepMind made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, a world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning. In 2020, DeepMind made significant advances in the problem of protein folding with AlphaFold. In July 2022, it was announced that over 200 million predicted protein structures, representing virtually all known proteins, would be released on the AlphaFold database.DeepMind posted a blog post on 28 April 2022 on a single visual language model (VLM) named Flamingo that can accurately describe a picture of something with just a few training images. In July 2022, DeepMind announced the development of DeepNash, a model-free multi-agent reinforcement learning system capable of playing the board game Stratego at the level of a human expert. The company merged with Google AI's Google Brain division to become Google DeepMind in April 2023.\nIn November 2023, Google DeepMind announced an Open Source Graph Network for Materials Exploration (GNoME), the tool proposes millions of materials previously unknown to chemistry, including several hundred thousand stable crystalline structures, of which 736 had been experimentally produced by the Massachusetts Institute of Technology, at the time of the release.\n\n"
    },
    {
      "id": "71066378",
      "title": "Google JAX",
      "url": "https://en.wikipedia.org/wiki/Google_JAX",
      "summary": "Google JAX is a machine learning framework for transforming numerical functions. It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and TensorFlow's XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with various existing frameworks such as TensorFlow and PyTorch. The primary functions of JAX are:\ngrad: automatic differentiation\njit: compilation\nvmap: auto-vectorization\npmap: SPMD programming\n\n"
    },
    {
      "id": "2671720",
      "title": "Gordon Plotkin",
      "url": "https://en.wikipedia.org/wiki/Gordon_Plotkin",
      "summary": "Gordon David Plotkin,  (born 9 September 1946) is a theoretical computer scientist in the School of Informatics at the University of Edinburgh. Plotkin is probably best known for his introduction of structural operational semantics (SOS) and his work on denotational semantics. In particular, his notes on A Structural Approach to Operational Semantics were very influential. He has contributed to many other areas of computer science.\n\n"
    },
    {
      "id": "201489",
      "title": "Gradient descent",
      "url": "https://en.wikipedia.org/wiki/Gradient_descent",
      "summary": "Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for finding a local minimum of a differentiable multivariate function\nThe idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.\nIt is particularly useful in machine learning for minimizing the cost or loss function. Gradient descent should not be confused with local search algorithms, although both are iterative methods for optimization.\nGradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.A simple extension of gradient descent, stochastic gradient descent, serves as the most basic algorithm used for training most deep networks today.\n\n"
    },
    {
      "id": "4375576",
      "title": "Grammar induction",
      "url": "https://en.wikipedia.org/wiki/Grammar_induction",
      "summary": "Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\n\n"
    },
    {
      "id": "68162942",
      "title": "Graph neural network",
      "url": "https://en.wikipedia.org/wiki/Graph_neural_network",
      "summary": "A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.\nIn the more general subject of \"geometric deep learning\", certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer, in the context of computer vision, can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer, in natural language processing, can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text.\nThe key design element of GNNs is the use of pairwise message passing, such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception, several different GNN architectures have been proposed, which implement different flavors of message passing, started by recursive or convolutional constructive approaches. As of 2022, whether it is possible to define GNN architectures \"going beyond\" message passing, or if every GNN can be built on message passing over suitably defined graphs, is an open research question.Relevant application domains for GNNs include Natural Language Processing,  social networks, citation networks, molecular biology, chemistry, physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available, such as PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).\n\n"
    },
    {
      "id": "447298",
      "title": "Graphical model",
      "url": "https://en.wikipedia.org/wiki/Graphical_model",
      "summary": "A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics\u2014particularly Bayesian statistics\u2014and machine learning."
    },
    {
      "id": "390214",
      "title": "Graphics processing unit",
      "url": "https://en.wikipedia.org/wiki/Graphics_processing_unit",
      "summary": "A graphics processing unit (GPU) is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles). After their initial design, GPUs were found to be useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. Other non-graphical uses include the training of neural networks and cryptocurrency mining."
    },
    {
      "id": "72607666",
      "title": "Hallucination (artificial intelligence)",
      "url": "https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)",
      "summary": "In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation or delusion) is a response generated by an AI which contains false or misleading information presented as fact.For example, a hallucinating chatbot might, when asked to generate a financial report for a company, falsely state that the company's revenue was $13.6 billion (or some other number apparently \"plucked from thin air\").\nSuch phenomena are termed \"hallucinations\", in loose analogy with the phenomenon of hallucination in human psychology. However, one key difference is that human hallucination is usually associated with false percepts, but an AI hallucination is associated with the category of unjustified responses or beliefs. Some researchers believe the specific term \"AI hallucination\" unreasonably anthropomorphizes computers.AI hallucination gained prominence during the AI boom, alongside the rollout of widely used chatbots based on large language models (LLMs), such as ChatGPT. Users complained that such chatbots often seemed to pointlessly embed plausible-sounding random falsehoods within their generated content. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with some estimating chatbots hallucinate as much as 27% of the time and a study finding factual errors in 46% of generated responses.\n\n"
    },
    {
      "id": "203619",
      "title": "Handwriting recognition",
      "url": "https://en.wikipedia.org/wiki/Handwriting_recognition",
      "summary": "Handwriting recognition (HWR), also known as handwritten text recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices. The image of the written text may be sensed \"off line\" from a piece of paper by optical scanning (optical character recognition) or intelligent word recognition. Alternatively, the movements of the pen tip may be sensed \"on line\", for example by a pen-based computer screen surface, a generally easier task as there are more clues available. A handwriting recognition system handles formatting, performs correct segmentation into characters, and finds the most possible words.\n\n"
    },
    {
      "id": "607285",
      "title": "Haplotype",
      "url": "https://en.wikipedia.org/wiki/Haplotype",
      "summary": "A haplotype (haploid genotype) is a group of alleles in an organism that are inherited together from a single parent.Many organisms contain genetic material (DNA) which is inherited from two parents. Normally these organisms have their DNA organized in two sets of pairwise similar chromosomes. The offspring gets one chromosome in each pair from each parent. A set of pairs of chromosomes is called diploid and a set of only one half of each pair is called haploid. The haploid genotype (haplotype) is a genotype that considers the singular chromosomes rather than the pairs of chromosomes. It can be all the chromosomes from one of the parents or a minor part of a chromosome, for example a sequence of 9000 base pairs or a small set of alleles.\nSpecific contiguous parts of the chromosome are likely to be inherited together and not be split by chromosomal crossover, a phenomenon called genetic linkage. As a result, identifying these statistical associations and a few alleles of a specific haplotype sequence can facilitate identifying all other such polymorphic sites that are nearby on the chromosome (imputation). Such information is critical for investigating the genetics of common diseases; which in fact have been investigated in humans by the International HapMap Project.Other parts of the genome are almost always haploid and do not undergo crossover: for example, humans mitochondrial DNA is pass down through the maternal line and the Y chromosome is passed down the paternal line. In these cases, the entire sequence can be grouped into a simple evolutionary tree, with each branch founded by a unique-event polymorphism mutation (often, but not always, an single-nucleotide polymorphism (SNP)). Each clade under a branch, containing haplotypes with a single shared ancestor, is called a haplogroup.\n\n"
    },
    {
      "id": "2031045",
      "title": "Hardware acceleration",
      "url": "https://en.wikipedia.org/wiki/Hardware_acceleration",
      "summary": "Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both.\nTo perform computing tasks more efficiently, generally one can invest time and money in improving the software, improving the hardware, or both. There are various approaches with advantages and disadvantages in terms of decreased latency, increased throughput and reduced energy consumption. Typical advantages of focusing on software may include greater versatility, more rapid development, lower non-recurring engineering costs, heightened portability, and ease of updating features or patching bugs, at the cost of overhead to compute general operations. Advantages of focusing on hardware may include speedup, reduced power consumption, lower latency, increased parallelism and bandwidth, and better utilization of area and functional components available on an integrated circuit; at the cost of lower ability to update designs once etched onto silicon and higher costs of functional verification, times to market, and need for more parts. In the hierarchy of digital computing systems ranging from general-purpose processors to fully customized hardware, there is a tradeoff between flexibility and efficiency, with efficiency increasing by orders of magnitude when any given application is implemented higher up that hierarchy. This hierarchy includes general-purpose processors such as CPUs, more specialized processors such as programmable shaders in a GPU, fixed-function implemented on field-programmable gate arrays (FPGAs), and fixed-function implemented on application-specific integrated circuits (ASICs).Hardware acceleration is advantageous for performance, and practical when the functions are fixed so updates are not as needed as in software solutions. With the advent of reprogrammable logic devices such as FPGAs, the restriction of hardware acceleration to fully fixed algorithms has eased since 2010, allowing hardware acceleration to be applied to problem domains requiring modification to algorithms and processing control flow. The disadvantage however, is that in many open source projects, it requires proprietary libraries that not all vendors are keen to distribute or expose, making it difficult to integrate in such projects.\n\n"
    },
    {
      "id": "54213333",
      "title": "Hardware security",
      "url": "https://en.wikipedia.org/wiki/Hardware_security",
      "summary": "Hardware security is a discipline originated from the cryptographic engineering and involves hardware design, access control, secure multi-party computation, secure key storage, ensuring code authenticity, measures to ensure that the supply chain that built the product is secure among other things.A hardware security module (HSM) is a physical computing device that safeguards and manages digital keys for strong authentication and provides cryptoprocessing. These modules traditionally come in the form of a plug-in card or an external device that attaches directly to a computer or network server.\nSome providers in this discipline consider that the key difference between hardware security and software security is that hardware security is implemented using \"non-Turing-machine\" logic (raw combinatorial logic or simple state machines). One approach, referred to as \"hardsec\", uses FPGAs to implement non-Turing-machine security controls as a way of combining the security of hardware with the flexibility of software.Hardware backdoors are backdoors in hardware. Conceptionally related, a hardware Trojan (HT) is a malicious modification of electronic system, particularly in the context of integrated circuit.A physical unclonable function (PUF) is a physical entity that is embodied in a physical structure and is easy to evaluate but hard to predict. Further, an individual PUF device must be easy to make but practically impossible to duplicate, even given the exact manufacturing process that produced it. In this respect it is the hardware analog of a one-way function. The name \"physical unclonable function\" might be a little misleading as some PUFs are clonable, and most PUFs are noisy and therefore do not achieve the requirements for a function. Today, PUFs are usually implemented in integrated circuits and are typically used in applications with high security requirements.\nMany attacks on sensitive data and resources reported by organizations occur from within the organization itself.\n\n"
    },
    {
      "id": "18426501",
      "title": "Harvard University",
      "url": "https://en.wikipedia.org/wiki/Harvard_University",
      "summary": "Harvard University is a private Ivy League research university in Cambridge, Massachusetts. Founded in 1636 as Harvard College and named for its first benefactor, the Puritan clergyman John Harvard, it is the oldest institution of higher learning in the United States.\nIts influence, wealth, and rankings have made it one of the most prestigious universities in the world.Harvard's founding was authorized by the Massachusetts colonial legislature, \"dreading to leave an illiterate ministry to the churches\", though never formally affiliated with any denomination, in its early years Harvard College primarily trained Congregational clergy. Its curriculum and student body were gradually secularized during the 18th century. By the 19th century, Harvard emerged as the most prominent academic and cultural institution among the Boston elite. Following the American Civil War, under President Charles William Eliot's long tenure (1869\u20131909), the college developed multiple affiliated professional schools that transformed the college into a modern research university. In 1900, Harvard co-founded the Association of American Universities. James B. Conant led the university through the Great Depression and World War II, and liberalized admissions after the war.\nThe university is composed of ten academic faculties plus the Harvard Radcliffe Institute. The Faculty of Arts and Sciences offers study in a wide range of undergraduate and graduate academic disciplines, and other faculties offer only graduate degrees, including professional degrees. Harvard has three main campuses:\nthe 209-acre (85 ha) Cambridge campus centered on Harvard Yard; an adjoining campus immediately across Charles River in the Allston neighborhood of Boston; and the medical campus in Boston's Longwood Medical Area. Harvard's endowment is valued at $50.7 billion, making it the wealthiest academic institution in the world.  Endowment income enables the undergraduate college to admit students regardless of financial need and provide financial aid with no loans. According to the American Library Association, Harvard University has the fourth largest library by volumes held in the United States.\nHarvard alumni, faculty, and researchers have included 188 living billionaires, 8 U.S. presidents, numerous heads of state, founders of notable companies, Nobel laureates, Fields Medalists, members of Congress, MacArthur Fellows, Rhodes Scholars, Marshall Scholars, Turing Award Recipients, Pulitzer Prize winners, and Fulbright Scholars; by most metrics, Harvard ranks among the top globally in each of these categories. Additionally, students and alumni have won 10 Academy Awards and 110 Olympic medals (46 gold)."
    },
    {
      "id": "351581",
      "title": "Health informatics",
      "url": "https://en.wikipedia.org/wiki/Health_informatics",
      "summary": "Health informatics is the study and implementation of computer structures and algorithms to improve communication, understanding, and management of medical information. It can be viewed as branch of engineering and applied science.\nThe health domain provides an extremely wide variety of problems that can be tackled using computational techniques.Health informatics is a spectrum of multidisciplinary fields that includes study of the design, development and application of computational innovations to improve health care. The disciplines involved combines medicine fields with computing fields, in particular computer engineering, software engineering, information engineering, bioinformatics, bio-inspired computing, theoretical computer science, information systems, data science, information technology, autonomic computing, and behavior informatics. In academic institutions, medical informatics research focus on applications of artificial intelligence in healthcare and designing medical devices based on embedded systems. In some countries term informatics is also used in the context of applying library science to data management in hospitals. In this meaning health informatics aims at developing methods and technologies for the acquisition, processing, and study of patient data, An umbrella term of biomedical informatics has been proposed.There are many variations in the name of the field involved in applying information and communication technologies to healthcare, public health, and personal health, ranging from those focused on the molecular (e.g, genomic), organ system (e.g., imaging), individual (e.g., patient or consumer, care provider, and interaction between them), to population-level of application.  A spectrum of activity spans efforts ranging from theory and model development, to empirical research, to implementation and management, to widespread adoption.\n'Clinical informaticians' are qualified health and social care professionals and 'clinical informatics' is a subspecialty within several medical specialties.\n\n"
    },
    {
      "id": "404084",
      "title": "Hebbian theory",
      "url": "https://en.wikipedia.org/wiki/Hebbian_theory",
      "summary": "Hebbian theory is a neuropsychological theory claiming that an increase in synaptic efficacy arises from a presynaptic cell's repeated and persistent stimulation of a postsynaptic cell. It is an attempt to explain synaptic plasticity, the adaptation of brain neurons during the learning process. It was introduced by Donald Hebb in his 1949 book The Organization of Behavior. The theory is also called Hebb's rule, Hebb's postulate, and cell assembly theory. Hebb states it as follows:\n\nLet us assume that the persistence or repetition of a reverberatory activity (or \"trace\") tends to induce lasting cellular changes that add to its stability. ... When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A\u2019s efficiency, as one of the cells firing B, is increased.\nThe theory is often summarized as \"Cells that fire together wire together.\" However, Hebb emphasized that cell A needs to \"take part in firing\" cell B, and such causality can occur only if cell A fires just before, not at the same time as, cell B. This aspect of causation in Hebb's work foreshadowed what is now known about spike-timing-dependent plasticity, which requires temporal precedence.The theory attempts to explain associative or Hebbian learning, in which simultaneous activation of cells leads to pronounced increases in synaptic strength between those cells. It also provides a biological basis for errorless learning methods for education and memory rehabilitation. In the study of neural networks in cognitive function, it is often regarded as the neuronal basis of unsupervised learning.\n\n"
    },
    {
      "id": "63452",
      "title": "Heuristic",
      "url": "https://en.wikipedia.org/wiki/Heuristic",
      "summary": "A heuristic (; from Ancient Greek  \u03b5\u1f51\u03c1\u03af\u03c3\u03ba\u03c9 (heur\u00edsk\u014d) 'to find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.Examples that employ heuristics include using trial and error, a rule of thumb or an educated guess.\nHeuristics are the strategies derived from previous experiences with similar problems. These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues. When an individual applies a heuristic in practice, it generally performs as expected. However it can alternatively create systematic errors.\nThe most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification. Here are a few commonly used heuristics from George P\u00f3lya's 1945 book, How to Solve It:\nIf you are having difficulty understanding a problem, try drawing a picture.\nIf you can't find a solution, try assuming that you have a solution and seeing what you can derive from that (\"working backward\").\nIf the problem is abstract, try examining a concrete example.\nTry solving a more general problem first (the \"inventor's paradox\": the more ambitious plan may have more chances of success).\nIn psychology, heuristics are simple, efficient rules, either learned or inculcated by evolutionary processes. These psychological heuristics have been proposed to explain how people make decisions, come to judgements, and solve problems. These rules typically come into play when people face complex problems or incomplete information. Researchers employ various methods to test whether people use these rules. The rules have been shown to work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases."
    },
    {
      "id": "14220429",
      "title": "Heuristic (computer science)",
      "url": "https://en.wikipedia.org/wiki/Heuristic_(computer_science)",
      "summary": "In mathematical optimization and computer science, heuristic (from Greek \u03b5\u1f51\u03c1\u03af\u03c3\u03ba\u03c9 \"I find, discover\") is a technique designed for problem solving more quickly when classic methods are too slow for finding an exact or approximate solution, or when classic methods fail to find any exact solution in a search space.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.\nA heuristic function, also simply called a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.\n\n"
    },
    {
      "id": "98770",
      "title": "Hidden Markov model",
      "url": "https://en.wikipedia.org/wiki/Hidden_Markov_model",
      "summary": "A hidden Markov model (HMM) is a Markov model in which the observations are dependent on a latent (or \"hidden\") Markov process (referred to as \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  ). An HMM requires that there be an observable process \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   whose outcomes depend on the outcomes of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   in a known way. Since \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   cannot be observed directly, the goal is to learn about state of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   by observing \n  \n    \n      \n        Y\n        .\n      \n    \n    {\\displaystyle Y.}\n   By definition of being a Markov model, an HMM has an additional requirement that the outcome of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   at time \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t=t_{0}}\n   must be \"influenced\" exclusively by the outcome of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   at \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t=t_{0}}\n   and that the outcomes of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   at \n  \n    \n      \n        t\n        <\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t<t_{0}}\n   must be conditionally independent of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   at \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t=t_{0}}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   at time \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n        .\n      \n    \n    {\\displaystyle t=t_{0}.}\n   Estimation of the parameters in an HMM can be performed using maximum likelihood. For linear chain HMMs, the Baum\u2013Welch algorithm can be used to estimate the parameters.\nHidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition\u2014such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.\n\n"
    },
    {
      "id": "477573",
      "title": "Hierarchical clustering",
      "url": "https://en.wikipedia.org/wiki/Hierarchical_clustering",
      "summary": "In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two categories:\n\nAgglomerative: This is a \"bottom-up\" approach: Each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\nDivisive: This is a \"top-down\" approach: All observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\nHierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances. On the other hand, except for the special case of single-linkage distance, none of the algorithms (except exhaustive search in \n  \n    \n      \n        \n          \n            O\n          \n        \n        (\n        \n          2\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {O}}(2^{n})}\n  ) can be guaranteed to find the optimum solution.\n\n"
    },
    {
      "id": "2894560",
      "title": "History of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
      "summary": "The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\nAlan Turing was the first person to carry out substantial research in the field that he called Machine Intelligence. The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.\nInvestment and interest in AI boomed in the 2020s when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets."
    },
    {
      "id": "22904524",
      "title": "Human-in-the-loop",
      "url": "https://en.wikipedia.org/wiki/Human-in-the-loop",
      "summary": "Human-in-the-loop  or HITL is used in multiple contexts. It can be defined as a model requiring human interaction. HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL along with the related human-on-the-loop are also used in relation to lethal autonomous weapons. Further, HITL is used in the context of machine learning.\n\n"
    },
    {
      "id": "490620",
      "title": "Human brain",
      "url": "https://en.wikipedia.org/wiki/Human_brain",
      "summary": "The brain is the central organ of the human nervous system, and with the spinal cord makes up the central nervous system. The brain consists of the cerebrum, the brainstem and the cerebellum. It controls most of the activities of the body, processing, integrating, and coordinating the information it receives from the sense organs, and making decisions as to the instructions sent to the rest of the body. The brain is contained in, and protected by, the skull bones of the head.\nThe cerebrum, the largest part of the human brain, consists of two cerebral hemispheres. Each hemisphere has an inner core composed of white matter, and an outer surface \u2013 the cerebral cortex \u2013 composed of grey matter. The cortex has an outer layer, the neocortex, and an inner allocortex. The neocortex is made up of six neuronal layers, while the allocortex has three or four. Each hemisphere is conventionally divided into four lobes \u2013 the frontal, temporal, parietal, and occipital lobes. The frontal lobe is associated with executive functions including self-control, planning, reasoning, and abstract thought, while the occipital lobe is dedicated to vision. Within each lobe, cortical areas are associated with specific functions, such as the sensory, motor and association regions. Although the left and right hemispheres are broadly similar in shape and function, some functions are associated with one side, such as language in the left and visual-spatial ability in the right. The hemispheres are connected by commissural nerve tracts, the largest being the corpus callosum.\nThe cerebrum is connected by the brainstem to the spinal cord. The brainstem consists of the midbrain, the pons, and the medulla oblongata. The cerebellum is connected to the brainstem by three pairs of nerve tracts called cerebellar peduncles. Within the cerebrum is the ventricular system, consisting of four interconnected ventricles in which cerebrospinal fluid is produced and circulated. Underneath the cerebral cortex are several important structures, including the thalamus, the epithalamus, the pineal gland, the hypothalamus, the pituitary gland, and the subthalamus; the limbic structures, including the amygdalae and the hippocampi, the claustrum, the various nuclei of the basal ganglia, the basal forebrain structures, and the three circumventricular organs. Brain structures that are not on the midplane exist in pairs, so there are for example two hippocampi and two amygdalae. The cells of the brain include neurons and supportive glial cells. There are more than 86 billion neurons in the brain, and a more or less equal number of other cells. Brain activity is made possible by the interconnections of neurons and their release of neurotransmitters in response to nerve impulses. Neurons connect to form neural pathways, neural circuits, and elaborate network systems. The whole circuitry is driven by the process of neurotransmission.\nThe brain is protected by the skull, suspended in cerebrospinal fluid, and isolated from the bloodstream by the blood\u2013brain barrier. However, the brain is still susceptible to damage, disease, and infection. Damage can be caused by trauma, or a loss of blood supply known as a stroke. The brain is susceptible to degenerative disorders, such as Parkinson's disease, dementias including Alzheimer's disease, and multiple sclerosis. Psychiatric conditions, including schizophrenia and clinical depression, are thought to be associated with brain dysfunctions. The brain can also be the site of tumours, both benign and malignant; these mostly originate from other sites in the body.\nThe study of the anatomy of the brain is neuroanatomy, while the study of its function is neuroscience. Numerous techniques are used to study the brain. Specimens from other animals, which may be examined microscopically, have traditionally provided much information. Medical imaging technologies such as functional neuroimaging, and electroencephalography (EEG) recordings are important in studying the brain. The medical history of people with brain injury has provided insight into the function of each part of the brain. Neuroscience research has expanded considerably, and research is ongoing.\nIn culture, the philosophy of mind has for centuries attempted to address the question of the nature of consciousness and the mind\u2013body problem. The pseudoscience of phrenology attempted to localise personality attributes to regions of the cortex in the 19th century. In science fiction, brain transplants are imagined in tales such as the 1942 Donovan's Brain."
    },
    {
      "id": "4287389",
      "title": "Human image synthesis",
      "url": "https://en.wikipedia.org/wiki/Human_image_synthesis",
      "summary": "Human image synthesis is technology that can be applied to make believable and even photorealistic renditions of human-likenesses, moving or still. It has effectively existed since the early 2000s. Many films using computer generated imagery have featured synthetic images of human-like characters digitally composited onto the real or other simulated film material. Towards the end of the 2010s deep learning artificial intelligence has been applied to synthesize images and video that look like humans, without need for human assistance, once the training phase has been completed, whereas the old school 7D-route required massive amounts of human work\n.\n\n"
    },
    {
      "id": "2932246",
      "title": "Hybrid intelligent system",
      "url": "https://en.wikipedia.org/wiki/Hybrid_intelligent_system",
      "summary": "Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\n\nNeuro-symbolic systems\nNeuro-fuzzy systems\nHybrid connectionist-symbolic models\nFuzzy expert systems\nConnectionist expert systems\nEvolutionary neural networks\nGenetic fuzzy systems\nRough fuzzy hybridization\nReinforcement learning with fuzzy, neural, or evolutionary methods as well as symbolic reasoning methods.From the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman,  and Michael A. Arbib.\nAn example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning.\nIntelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy.\n\n"
    },
    {
      "id": "54361643",
      "title": "Hyperparameter optimization",
      "url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization",
      "summary": "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process.\nHyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.  The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance, and therefore choose the set of values for hyperparameters that maximize it."
    },
    {
      "id": "22584291",
      "title": "IBM Watson",
      "url": "https://en.wikipedia.org/wiki/IBM_Watson",
      "summary": "IBM Watson is a computer system capable of answering questions posed in natural language. It was developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci. Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.The computer system was initially developed to answer questions on the quiz show Jeopardy! and in 2011, the Watson computer system competed on Jeopardy! against champions Brad Rutter and Ken Jennings, winning the first-place prize of 1 million USD.In February 2013, IBM announced that Watson's first commercial application would be for utilization management decisions in lung cancer treatment at Memorial Sloan Kettering Cancer Center, New York City, in conjunction with WellPoint (now Elevance Health).. \n\n"
    },
    {
      "id": "55207134",
      "title": "IBM Watson Studio",
      "url": "https://en.wikipedia.org/wiki/IBM_Watson_Studio",
      "summary": "Watson Studio, formerly Data Science Experience or DSX, is IBM\u2019s software platform for data science. The platform consists of a workspace that includes multiple collaboration and open-source tools for use in data science.In Watson Studio, a data scientist can create a project with a group of collaborators, all having access to various analytics models and using various languages (R/Python/Scala). Watson Studio brings together staple open source tools including RStudio, Spark and Python in an integrated environment, along with additional tools such as a managed Spark service and data shaping facilities, in a secure and governed environment.Watson Studio provides access to data sets that are available through Watson Data Platform, on-premises or on the cloud. The platform also has a large community and embedded resources such as articles on the latest developments from the data science world and public data sets. The platform is available in on-premises, cloud, and desktop forms.\n\n"
    },
    {
      "id": "1951226",
      "title": "IEEE Spectrum",
      "url": "https://en.wikipedia.org/wiki/IEEE_Spectrum",
      "summary": "IEEE Spectrum is a magazine edited by the Institute of Electrical and Electronics Engineers.\nThe first issue of IEEE Spectrum was published in January 1964 as a successor to Electrical Engineering.\nIn 2010, IEEE Spectrum was the recipient of Utne Reader magazine's Utne Independent Press Award for Science/Technology Coverage. In 2012, IEEE Spectrum was selected as the winner of the National Magazine Awards \"General Excellence Among Thought Leader Magazines\" category.\n\n"
    },
    {
      "id": "12953325",
      "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "url": "https://en.wikipedia.org/wiki/IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence",
      "summary": "IEEE Transactions on Pattern Analysis and Machine Intelligence (sometimes abbreviated as IEEE PAMI or simply PAMI) is a monthly peer-reviewed scientific journal published by the IEEE Computer Society."
    },
    {
      "id": "29261811",
      "title": "I (newspaper)",
      "url": "https://en.wikipedia.org/wiki/I_(newspaper)",
      "summary": "The i is a British national newspaper published in London by Daily Mail and General Trust and distributed across the United Kingdom. It is aimed at \"readers and lapsed readers\" of all ages and commuters with limited time, and was originally launched in 2010 as a sister paper to The Independent. It was later acquired by Johnston Press in 2016 after The Independent shifted to a digital-only model. The i came under the control of JPIMedia a day after Johnston Press filed for administration on 16 November 2018. The paper and its website were bought by the Daily Mail and General Trust (DMGT) on 29 November 2019, for \u00a349.6 million. On 6 December 2019 the Competition and Markets Authority served an initial enforcement order on DMGT and DMG Media Limited, requiring the paper to be run separately pending investigation.Since its inception, the i has expanded its layout and coverage, adding special sections for notable events and revamping its weekend edition. The paper had an average daily circulation of 302,757 in March 2013, significantly more than The Independent, though that figure has since continued to decline, and had dropped to 233,869 by February 2019. The paper is classified as a 'quality' in the UK market but is published in the standard compact tabloid-size format.\n\n"
    },
    {
      "id": "51213339",
      "title": "Ian Goodfellow",
      "url": "https://en.wikipedia.org/wiki/Ian_Goodfellow",
      "summary": "Ian J. Goodfellow (born 1987) is an American computer scientist, engineer, and executive, most noted for his work on artificial neural networks and deep learning. He was previously employed as a research scientist at Google Brain and director of machine learning at Apple and has made several important contributions to the field of deep learning including the invention of the generative adversarial network (GAN). Goodfellow co-wrote, as the first author, the textbook Deep Learning (2016) and wrote the chapter on deep learning in the authoritative textbook of the field of artificial intelligence, Artificial Intelligence: A Modern Approach (used in more than 1,500 universities in 135 countries).\n\n"
    },
    {
      "id": "51097862",
      "title": "Ilya Sutskever",
      "url": "https://en.wikipedia.org/wiki/Ilya_Sutskever",
      "summary": "Ilya Sutskever  (; Hebrew: \u05d0\u05d9\u05dc\u05d9\u05d4 \u05e1\u05d5\u05e6\u05e7\u05d1\u05e8; Russian: \u0418\u043b\u044c\u044f\u0301  \u0421\u0443\u0446\u043a\u0435\u0301\u0432\u0435\u0440 [\u026a\u02c8l\u02b2ja  s\u028ats\u02c8k\u02b2ev\u02b2\u026ar] born 1985/86) is a computer scientist working in machine learning. He is a co-founder and former Chief Scientist at OpenAI.He has made several major contributions to the field of deep learning. In 2023, Sutskever was one of the members of the OpenAI board who fired CEO Sam Altman; Altman returned a week later, and Sutskever stepped down from the board. He is the co-inventor, with Alex Krizhevsky and Geoffrey Hinton, of AlexNet, a convolutional neural network. Sutskever is also one of the many co-authors of the AlphaGo paper."
    },
    {
      "id": "46469",
      "title": "Image compression",
      "url": "https://en.wikipedia.org/wiki/Image_compression",
      "summary": "Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.\n\n"
    },
    {
      "id": "476836",
      "title": "Noise reduction",
      "url": "https://en.wikipedia.org/wiki/Noise_reduction",
      "summary": "Noise reduction is the process of removing noise from a signal. Noise reduction techniques exist for audio and images. Noise reduction algorithms may distort the signal to some degree. Noise rejection is the ability of a circuit to isolate an undesired signal component from the desired signal component, as with common-mode rejection ratio.\nAll signal processing devices, both analog and digital, have traits that make them susceptible to noise. Noise can be random with an even frequency distribution (white noise), or frequency-dependent noise introduced by a device's mechanism or signal processing algorithms.\nIn electronic systems, a major type of noise is hiss created by random electron motion due to thermal agitation. These agitated electrons rapidly add and subtract from the output signal and thus create detectable noise.\nIn the case of photographic film and magnetic tape, noise (both visible and audible) is introduced due to the grain structure of the medium. In photographic film, the size of the grains in the film determines the film's sensitivity, more sensitive film having larger-sized grains. In magnetic tape, the larger the grains of the magnetic particles (usually ferric oxide or magnetite), the more prone the medium is to noise. To compensate for this, larger areas of film or magnetic tape may be used to lower the noise to an acceptable level.\n\n"
    },
    {
      "id": "3255074",
      "title": "Imprecise probability",
      "url": "https://en.wikipedia.org/wiki/Imprecise_probability",
      "summary": "Imprecise probability generalizes probability theory to allow for partial probability specifications, and is applicable when information is scarce, vague, or conflicting, in which case a unique probability distribution may be hard to identify. Thereby, the theory aims to represent the available knowledge more accurately.  Imprecision is useful for dealing with expert elicitation, because:\n\nPeople have a limited ability to determine their own subjective probabilities and might find that they can only provide an interval.\nAs an interval is compatible with a range of opinions, the analysis ought to be more convincing to a range of different people.\n\n"
    },
    {
      "id": "14617",
      "title": "Intel",
      "url": "https://en.wikipedia.org/wiki/Intel",
      "summary": "Intel Corporation is an American multinational corporation and technology company headquartered in Santa Clara, California. It is one of the world's largest semiconductor chip manufacturers by revenue. Intel supplies microprocessors for most manufacturers of computer systems, and is one of the developers of the x86 series of instruction sets found in most personal computers (PCs). Intel also manufactures chipsets, network interface controllers, flash memory, graphics processing units (GPUs), field-programmable gate arrays (FPGAs), and other devices related to communications and computing.\nIntel ranked No. 45 in the 2020 Fortune 500 list of the largest United States corporations by total revenue for nearly a decade, from 2007 to 2016 fiscal years.Intel (Integrated electronics) was founded on July 18, 1968, by semiconductor pioneers Gordon Moore (of Moore's law), Robert Noyce and Arthur Rock, and is associated with the executive leadership and vision of Andrew Grove. Intel was a key component of the rise of Silicon Valley as a high-tech center, as well as being an early developer of SRAM and DRAM memory chips, which represented the majority of its business until 1981. Although Intel created the world's first commercial microprocessor chip in 1971, it was not until the success of the PC in the early 1990s that this became its primary business.\nDuring the 1990s, Intel invested heavily in new microprocessor designs fostering the rapid growth of the computer industry. During this period, Intel became the dominant supplier of PC microprocessors and was known for aggressive and anti-competitive tactics in defense of its market position, particularly against AMD, as well as a struggle with Microsoft for control over the direction of the PC industry.The Open Source Technology Center at Intel hosts PowerTOP and LatencyTOP, and supports other open source projects such as Wayland, Mesa, Threading Building Blocks (TBB), and Xen.Intel is incorporated in Delaware under Delaware General Corporation Law.\n\n"
    },
    {
      "id": "598031",
      "title": "Independent component analysis",
      "url": "https://en.wikipedia.org/wiki/Independent_component_analysis",
      "summary": "In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the \"cocktail party problem\" of listening in on one person's speech in a noisy room."
    },
    {
      "id": "173926",
      "title": "Inductive bias",
      "url": "https://en.wikipedia.org/wiki/Inductive_bias",
      "summary": "The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.\nInductive bias is anything which makes the algorithm learn one pattern instead of another pattern (e.g. step-functions in decision trees instead of continuous function in a linear regression model).\nIn machine learning, one aims to construct algorithms that are able to learn to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training.  Without any additional assumptions, this problem cannot be solved since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase inductive bias.A classical example of an inductive bias is Occam's razor, assuming that the simplest consistent hypothesis about the target function is actually the best. Here consistent means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm.\nApproaches to a more formal definition of inductive bias are based on mathematical logic. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. However, this strict formalism fails in many practical cases, where the inductive bias can only be given as a rough description (e.g. in the case of artificial neural networks), or not at all.\n\n"
    },
    {
      "id": "54069",
      "title": "Inductive logic programming",
      "url": "https://en.wikipedia.org/wiki/Inductive_logic_programming",
      "summary": "Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses.  The term \"inductive\" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.\n\nSchema: positive examples + negative examples + background knowledge \u21d2 hypothesis.Inductive logic programming is particularly useful in bioinformatics and natural language processing.\n\n"
    },
    {
      "id": "41644056",
      "title": "Inductive programming",
      "url": "https://en.wikipedia.org/wiki/Inductive_programming",
      "summary": "Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\nDepending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations  such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.\n\n"
    },
    {
      "id": "393736",
      "title": "Inductive reasoning",
      "url": "https://en.wikipedia.org/wiki/Inductive_reasoning",
      "summary": "The term Inductive reasoning is used to refer to any method of reasoning in which broad generalizations or principles are derived from a body of observations. This article is concerned with the inductive reasoning other than deductive reasoning (such as mathematical induction), where the conclusion of a deductive argument is certain given the premises are correct; in contrast, the truth of the conclusion of an inductive argument is at best probable, based upon the evidence given."
    },
    {
      "id": "14539",
      "title": "Internet",
      "url": "https://en.wikipedia.org/wiki/Internet",
      "summary": "The Internet (or internet) is the global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries a vast range of information resources and services, such as the interlinked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and file sharing.\nThe origins of the Internet date back to research to enable time-sharing of computer resources and the development of packet switching in the 1960s. The set of rules (communication protocols) to enable internetworking on the Internet arose from research and development commissioned in the 1970s by the Defense Advanced Research Projects Agency (DARPA) of the United States Department of Defense in collaboration with universities and researchers across the United States and in the United Kingdom and France. The ARPANET initially served as a backbone for the interconnection of regional academic and military networks in the United States to enable resource sharing. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, encouraged worldwide participation in the development of new networking technologies and the merger of many networks using DARPA's Internet protocol suite. The linking of commercial networks and enterprises by the early 1990s, as well as the advent of the World Wide Web, marked the beginning of the transition to the modern Internet, and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network. Although the Internet was widely used by academia in the 1980s, subsequent commercialization is what incorporated its services and technologies into virtually every aspect of modern life.\nMost traditional communication media, including telephone, radio, television, paper mail, and newspapers, are reshaped, redefined, or even bypassed by the Internet, giving birth to new services such as email, Internet telephone, Internet television, online music, digital newspapers, and video streaming websites. Newspaper, book, and other print publishing have adapted to website technology or have been reshaped into blogging, web feeds, and online news aggregators. The Internet has enabled and accelerated new forms of personal interaction through instant messaging, Internet forums, and social networking services. Online shopping has grown exponentially for major retailers, small businesses, and entrepreneurs, as it enables firms to extend their \"brick and mortar\" presence to serve a larger market or even sell goods and services entirely online. Business-to-business and financial services on the Internet affect supply chains across entire industries.\nThe Internet has no single centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies. The overarching definitions of the two principal name spaces on the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise. In November 2006, the Internet was included on USA Today's list of the New Seven Wonders."
    },
    {
      "id": "1194259",
      "title": "Influence diagram",
      "url": "https://en.wikipedia.org/wiki/Influence_diagram",
      "summary": "An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation.  It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.\nID was first developed in the mid-1970s by decision analysts with an intuitive semantic that is easy to understand. It is now adopted widely and becoming an alternative to the decision tree which typically suffers from exponential growth in number of branches with each variable modeled.  ID is directly applicable in team decision analysis, since it allows incomplete sharing of information among team members to be modeled and solved explicitly.  Extensions of ID also find their use in game theory as an alternative representation of the game tree.\n\n"
    },
    {
      "id": "487312",
      "title": "Information geometry",
      "url": "https://en.wikipedia.org/wiki/Information_geometry",
      "summary": "Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics.   It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions."
    },
    {
      "id": "15036",
      "title": "Information security",
      "url": "https://en.wikipedia.org/wiki/Information_security",
      "summary": "Information security, sometimes shortened to InfoSec, is the practice of protecting information by mitigating information risks. It is part of information risk management. It typically involves preventing or reducing the probability of unauthorized or inappropriate access to data or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g., electronic or physical, tangible (e.g., paperwork), or intangible (e.g., knowledge). Information security's primary focus is the balanced protection of data confidentiality, integrity, and availability (also known as the \"CIA\" triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity. This is largely achieved through a structured risk management process that involves: \n\nIdentifying information and related assets, plus potential threats, vulnerabilities, and impacts;\nEvaluating the risks\nDeciding how to address or treat the risks, i.e., to avoid, mitigate, share, or accept them\nWhere risk mitigation is required, selecting or designing appropriate security controls and implementing them\nMonitoring the activities and making adjustments as necessary to address any issues, changes, or improvement opportunitiesTo standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on passwords, antivirus software, firewalls, encryption software, legal liability, security awareness and training, and so forth. This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred, and destroyed. However, the implementation of any standards and guidance within an entity may have limited effect if a culture of continual improvement is not adopted.\n\n"
    },
    {
      "id": "237495",
      "title": "Information system",
      "url": "https://en.wikipedia.org/wiki/Information_system",
      "summary": "An information systems (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information. From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology. Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.Bachelor of Business Information Systems  BBIS  is an Information Technology(IT) and management focused undergraduate program designed to better understand the needs of rapidly growing technology in business and IT sector.It is a bachelor degree that combines elements of business administration and computer science with majoring on information systems and technology.The purpose of this course is to equip students with the skills and knowledge needed to effectively manage and utilize information technology in a business and IT industry.\nA computer information system is a system that is composed of people and computers that processes or interprets information. The term is also sometimes used to simply refer to a computer system with software installed.\n\"Information systems\" is also an academic field study about systems with a specific reference to information and the complementary networks of computer hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.In many organizations, the department or unit responsible for information systems and data processing is known as \"information services\".Any specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end-use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.Alter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information.As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action.\nInformation systems are the primary focus of study for organizational informatics.\n\n"
    },
    {
      "id": "14773",
      "title": "Information theory",
      "url": "https://en.wikipedia.org/wiki/Information_theory",
      "summary": "Information theory is the mathematical study of the quantification, storage, and communication of information. The field was originally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.:\u200avii\u200a The field, in applied mathematics, is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.\nA key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy, less uncertainty) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security.\nApplications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference, cryptography, neurobiology, perception, linguistics, the evolution and function of molecular codes (bioinformatics), thermal physics, molecular dynamics, quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection, pattern recognition, anomaly detection and even art creation."
    },
    {
      "id": "30876141",
      "title": "Securitization",
      "url": "https://en.wikipedia.org/wiki/Securitization",
      "summary": "Securitization is the financial practice of pooling various types of contractual debt such as residential mortgages, commercial mortgages, auto loans or credit card debt obligations (or other non-debt assets which generate receivables) and selling their related cash flows to third party investors as securities, which may be described as bonds, pass-through securities, or collateralized debt obligations (CDOs). \nInvestors are repaid from the principal and interest cash flows collected from the underlying debt and redistributed through the capital structure of the new financing. \nSecurities backed by mortgage receivables are called mortgage-backed securities (MBS), while those backed by other types of receivables are asset-backed securities (ABS).\nThe granularity of pools of securitized assets can mitigate the credit risk of individual borrowers. Unlike general corporate debt, the credit quality of securitized debt is non-stationary due to changes in volatility that are time- and structure-dependent. If the transaction is properly structured and the pool performs as expected, the credit risk of all tranches of structured debt improves; if improperly structured, the affected tranches may experience dramatic credit deterioration and loss.Securitization has evolved from its beginnings in the late 18th century to an estimated outstanding of $10.24 trillion in the United States and $2.25 trillion in Europe as of the 2nd quarter of 2008. In 2007, ABS issuance amounted to $3.455 trillion in the US and $652 billion in Europe. WBS (Whole Business Securitization) arrangements first appeared in the United Kingdom in the 1990s, and became common in various Commonwealth legal systems where senior creditors of an insolvent business effectively gain the right to control the company.\n\n"
    },
    {
      "id": "15150",
      "title": "Integrated circuit",
      "url": "https://en.wikipedia.org/wiki/Integrated_circuit",
      "summary": "An integrated circuit, also known as a microchip or IC, is a small electronic device made up of multiple interconnected electronic components such as transistors, resistors, and capacitors. These components are etched onto a small piece of semiconductor material, usually silicon. Integrated circuits are used in a wide range of electronic devices, including computers, smartphones, and televisions, to perform various functions such as processing and storing information. They have greatly impacted the field of electronics by enabling device miniaturization and enhanced functionality.\nIntegrated circuits are orders of magnitude smaller, faster, and less expensive than those constructed of discrete components, allowing a large transistor count.\nThe IC's mass production capability, reliability, and building-block approach to integrated circuit design have ensured the rapid adoption of standardized ICs in place of designs using discrete transistors. ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones, and other home appliances are now essential parts of the structure of modern societies, made possible by the small size and low cost of ICs such as modern computer processors and microcontrollers.\nVery-large-scale integration was made practical by technological advancements in semiconductor device fabrication. Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more transistors on chips of the same size \u2013 a modern chip may have many billions of transistors in an area the size of a human fingernail. These advances, roughly following Moore's law, make the computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s.\nICs have three main advantages over circuits constructed out of discrete components: size, cost and performance. The size and cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and proximity. The main disadvantage of ICs is the high initial cost of designing them and the enormous capital cost of factory construction. This high initial cost means ICs are only commercially viable when high production volumes are anticipated.\n\n"
    },
    {
      "id": "15305",
      "title": "Integrated development environment",
      "url": "https://en.wikipedia.org/wiki/Integrated_development_environment",
      "summary": "An integrated development environment (IDE) is a software application that provides comprehensive facilities for software development. An IDE normally consists of at least a source-code editor, build automation tools, and a debugger. Some IDEs, such as IntelliJ IDEA, Eclipse and Lazarus contain the necessary compiler, interpreter or both; others, such as SharpDevelop and NetBeans, do not.\nThe boundary between an IDE and other parts of the broader software development environment is not well-defined; sometimes a version control system or various tools to simplify the construction of a graphical user interface (GUI) are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram for use in object-oriented software development."
    },
    {
      "id": "527943",
      "title": "Interaction design",
      "url": "https://en.wikipedia.org/wiki/Interaction_design",
      "summary": "Interaction design, often abbreviated as IxD, is \"the practice of designing interactive digital products, environments, systems, and services.\":\u200axxvii,\u200a30\u200a While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.:\u200axxvii,\u200a30\u200a Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field, as opposed to a science or engineering field.Interaction design borrows from a wide range of fields like psychology, human-computer interaction, information architecture, and user research to create designs that are tailored to the needs and preferences of users. This involves understanding the context in which the product will be used, identifying user goals and behaviors, and developing design solutions that are responsive to user needs and expectations.\nWhile disciplines such as software engineering have a heavy focus on designing for technical stakeholders, interaction design is focused on meeting the needs and optimizing the experience of users, within relevant technical or business constraints.:\u200axviii\u200a\n\n"
    },
    {
      "id": "39758073",
      "title": "International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics",
      "url": "https://en.wikipedia.org/wiki/International_Conference_on_Computational_Intelligence_Methods_for_Bioinformatics_and_Biostatistics",
      "summary": "The International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) is a yearly scientific conference focused on machine learning and computational intelligence applied to bioinformatics, biostatistics, and medical informatics.\n\n"
    },
    {
      "id": "27855504",
      "title": "International Conference on Intelligent Robots and Systems",
      "url": "https://en.wikipedia.org/wiki/International_Conference_on_Intelligent_Robots_and_Systems",
      "summary": "IROS, the IEEE/RSJ International Conference on Intelligent Robots and Systems, is an annual academic conference covering advances in robotics. It is one of the premier conferences of its field (alongside ICRA, International Conference on Robotics and Automation) with an 'A' rating from the Australian Ranking of ICT Conferences obtained in 2010 and an 'A1' rating from the Brazilian ministry of education in 2012.The acceptance rate can vary substantially (for example, it has been 32% in 2011 and 39% in 2012) and IROS typically receives more than 2000 paper submissions (for example, 790 out of 2459 submitted papers have been accepted for IROS 2011)."
    },
    {
      "id": "59466481",
      "title": "International Conference on Learning Representations",
      "url": "https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations",
      "summary": "The International Conference on Learning Representations (ICLR) is a machine learning conference typically held in late April or early May each year. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by Yann LeCun). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%).. In 2021, there were 2997 paper submissions, of which 860 were accepted (29%)..\n\n"
    },
    {
      "id": "18586449",
      "title": "International Conference on Machine Learning",
      "url": "https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning",
      "summary": "The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research. It is supported by the (IMLS). Precise dates vary year to year, but paper submissions are generally due at the end of January, and the conference is generally held the following July. The first ICML was held 1980 in Pittsburgh.\n\n"
    },
    {
      "id": "2614944",
      "title": "International Joint Conference on Artificial Intelligence",
      "url": "https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence",
      "summary": "The International Joint Conference on Artificial Intelligence (IJCAI) is the leading conference in the field of artificial intelligence. The conference series has been organized by the nonprofit IJCAI Organization since 1969, making it the oldest premier AI conference series in the world. It was held biennially in odd-numbered years from 1969 to 2015 and annually starting from 2016. More recently, IJCAI was held jointly every four years with ECAI since 2018 and PRICAI since 2020 to promote collaboration of AI researchers and practitioners. IJCAI covers a broad range of research areas in the field of AI. It is a large and highly selective conference, with only about 20% or less of the submitted papers accepted after peer review in the 5 years leading up to 2022. A lower acceptance rate usually means better quality papers and a higher reputation conference.\n\n"
    },
    {
      "id": "805626",
      "title": "Internet fraud",
      "url": "https://en.wikipedia.org/wiki/Internet_fraud",
      "summary": "Internet fraud is a type of cybercrime fraud or deception which makes use of the Internet and could involve hiding of information or providing incorrect information for the purpose of tricking victims out of money, property, and inheritance. Internet fraud is not considered a single, distinctive crime but covers a range of illegal and illicit actions that are committed in cyberspace. It is, however, differentiated from theft since, in this case, the victim voluntarily and knowingly provides the information, money or property to the perpetrator. It is also distinguished by the way it involves temporally and spatially separated offenders.According to the FBI's 2017 Internet Crime Report, the Internet Crime Complaint Center (IC3) received about 300,000 complaints. Victims lost over $1.4 billion in online fraud in 2017. According to a study conducted by the Center for Strategic and International Studies (CSIS) and McAfee, cybercrime costs the global economy as much as $600 billion, which translates into 0.8% of total global GDP. Online fraud appears in many forms. It ranges from email spam to online scams. Internet fraud can occur even if partly based on the use of Internet services and is mostly or completely based on the use of the Internet.\n\n"
    },
    {
      "id": "59868",
      "title": "Interpreter (computing)",
      "url": "https://en.wikipedia.org/wiki/Interpreter_(computing)",
      "summary": "In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:\n\nParse the source code and perform its behavior directly;\nTranslate source code into some efficient intermediate representation or object code and immediately execute that;\nExplicitly execute stored precompiled bytecode made by a compiler and matched with the interpreter Virtual Machine.Early versions of Lisp programming language and minicomputer and microcomputer BASIC dialects would be examples of the first type. Perl, Raku, Python, MATLAB, and Ruby are examples of the second, while UCSD Pascal is an example of the third type. Source programs are compiled ahead of time and stored as machine independent code, which is then linked at run-time and executed by an interpreter and/or compiler (for JIT systems). Some systems, such as Smalltalk and contemporary versions of BASIC and Java, may also combine two and three. Interpreters of various types have also been constructed for many languages traditionally associated with compilation, such as Algol, Fortran, Cobol, C and C++.\nWhile interpretation and compilation are the two main means by which programming languages are implemented, they are not mutually exclusive, as most interpreting systems also perform some translation work, just like compilers. The terms \"interpreted language\" or \"compiled language\" signify that the canonical implementation of that language is an interpreter or a compiler, respectively. A high-level language is ideally an abstraction independent of particular implementations.\n\n"
    },
    {
      "id": "113021",
      "title": "Intrusion detection system",
      "url": "https://en.wikipedia.org/wiki/Intrusion_detection_system",
      "summary": "An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any intrusion activity or violation is typically either reported to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.IDS types range in scope from single computers to large networks. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS). A system that monitors important operating system files is an example of an HIDS, while a system that analyzes incoming network traffic is an example of an NIDS. It is also possible to classify IDS by detection approach. The most well-known variants are signature-based detection (recognizing bad patterns, such as malware) and anomaly-based detection (detecting deviations from a model of \"good\" traffic, which often relies on machine learning). Another common variant is reputation-based detection (recognizing the potential threat according to the reputation scores). Some IDS products have the ability to respond to detected intrusions. Systems with response capabilities are typically referred to as an intrusion prevention system. Intrusion detection systems can also serve specific purposes by augmenting them with custom tools, such as using a honeypot to attract and characterize malicious traffic.\n\n"
    },
    {
      "id": "309754",
      "title": "Investigative journalism",
      "url": "https://en.wikipedia.org/wiki/Investigative_journalism",
      "summary": "Investigative journalism is a form of journalism in which reporters deeply investigate a single topic of interest, such as serious crimes, racial injustice, political corruption, or corporate wrongdoing. An investigative journalist may spend months or years researching and preparing a report. Practitioners sometimes use the terms \"watchdog reporting\" or \"accountability reporting\".\nMost investigative journalism has traditionally been conducted by newspapers, wire services, and freelance journalists. With the decline in income through advertising, many traditional news services have struggled to fund investigative journalism, due to it being very time-consuming and expensive. Journalistic investigations are increasingly carried out by news organizations working together, even internationally (as in the case of the Panama Papers and Paradise Papers), or by nonprofit outlets such as ProPublica, which rely on the support of the public and benefactors to fund their work.\n\n"
    },
    {
      "id": "61890679",
      "title": "Isolation forest",
      "url": "https://en.wikipedia.org/wiki/Isolation_forest",
      "summary": "Isolation Forest is an algorithm for data anomaly detection initially developed by Fei Tony Liu in 2008. Isolation Forest detects anomalies using binary trees. The algorithm has a linear time complexity and a low memory requirement, which works well with high-volume data.\nIn essence, the algorithm relies upon the characteristics of anomalies, i.e., being few and different, in order to detect anomalies. No density estimation is performed in the algorithm. The algorithm is different from decision tree algorithms in that only the path-length measure or approximation is being used to generate the anomaly score, no leaf node statistics on class distribution or target value is needed.\nIsolation Forest is fast because it splits the data space randomly, using randomly selected attribute and randomly selected split point.  The anomaly score is invertedly associated with the path-length as anomalies need fewer splits to be isolated, due to the fact that they are few and different. \n\n"
    },
    {
      "id": "40671069",
      "title": "Jerome H. Friedman",
      "url": "https://en.wikipedia.org/wiki/Jerome_H._Friedman",
      "summary": "Jerome Harold Friedman (born December 29, 1939) is an American statistician, consultant and Professor of Statistics at Stanford University, known for his contributions in the field of statistics and data mining.\n\n"
    },
    {
      "id": "649572",
      "title": "John Hopfield",
      "url": "https://en.wikipedia.org/wiki/John_Hopfield",
      "summary": "John Joseph Hopfield (born July 15, 1933) is an American scientist most widely known for his invention of an associative neural network in 1982. It is now more commonly known as the Hopfield network.\n\n"
    },
    {
      "id": "5721283",
      "title": "Journal of Machine Learning Research",
      "url": "https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research",
      "summary": "The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling. The current editors-in-chief are Francis Bach (Inria) and David Blei (Columbia University).\n\n"
    },
    {
      "id": "405484",
      "title": "J\u00fcrgen Schmidhuber",
      "url": "https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber",
      "summary": "J\u00fcrgen Schmidhuber (born 17 January 1963) is a German computer scientist noted for his work in the field of artificial intelligence, specifically artificial neural networks. He is a scientific director of the Dalle Molle Institute for Artificial Intelligence Research in Switzerland. He is also director of the Artificial Intelligence Initiative and professor of the Computer Science program in the Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) division at the King Abdullah University of Science and Technology (KAUST) in Saudi Arabia.He is best known for his foundational and highly-cited work on long short-term memory (LSTM), a type of neural network architecture which went on to become the dominant technique for various natural language processing tasks in research and commercial applications in the 2010s. He also introduced principles of meta-learning, generative adversarial networks and linear transformers, all of which are widespread in modern AI.\n\n"
    },
    {
      "id": "39799215",
      "title": "K-SVD",
      "url": "https://en.wikipedia.org/wiki/K-SVD",
      "summary": "In applied mathematics, k-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. k-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. It is structurally related to the expectation maximization (EM) algorithm. k-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.\n\n"
    },
    {
      "id": "1860407",
      "title": "K-means clustering",
      "url": "https://en.wikipedia.org/wiki/K-means_clustering",
      "summary": "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.\nThe problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.\nThe unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\n\n"
    },
    {
      "id": "1775388",
      "title": "K-nearest neighbors algorithm",
      "url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
      "summary": "In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\nA peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.\n\n"
    },
    {
      "id": "12185719",
      "title": "KXEN Inc.",
      "url": "https://en.wikipedia.org/wiki/KXEN_Inc.",
      "summary": "KXEN was an American software company which existed from 1998 to 2013 when it was acquired by SAP AG.\n\n"
    },
    {
      "id": "957422",
      "title": "Keep",
      "url": "https://en.wikipedia.org/wiki/Keep",
      "summary": "A keep (from the Middle English kype) is a type of fortified tower built within castles during the Middle Ages by European nobility. Scholars have debated the scope of the word keep, but usually consider it to refer to large towers in castles that were fortified residences, used as a refuge of last resort should the rest of the castle fall to an adversary. The first keeps were made of timber and formed a key part of the motte-and-bailey castles that emerged in Normandy and Anjou during the 10th century; the design spread to England, and south to Italy and Sicily. As a result of the Norman invasion of 1066, use spread into Wales during the second half of the 11th century and into Ireland in the 1170s. The Anglo-Normans and French rulers began to build stone keeps during the 10th and 11th centuries, including Norman keeps, with a square or rectangular design, and circular shell keeps. Stone keeps carried considerable political as well as military importance and could take a decade or more to build.\nDuring the 12th century, new designs began to be introduced \u2013 in France, quatrefoil-shaped keeps were introduced, while in England polygonal towers were built. By the end of the century, French and English keep designs began to diverge: Philip II of France built a sequence of circular keeps as part of his bid to stamp his royal authority on his new territories, while in England castles were built without keeps. In Spain, keeps were increasingly incorporated into both Christian and Islamic castles, although in Germany tall fighting towers called bergfriede were preferred to keeps in the western fashion. In the second half of the 14th century, there was a resurgence in the building of keeps. In France, the keep at Vincennes began a fashion for tall, heavily machicolated designs, a trend adopted in Spain most prominently through the Valladolid school of Spanish castle design. Meanwhile, tower keeps in England became popular amongst the most wealthy nobles: these large keeps, each uniquely designed, formed part of the grandest castles built during the period.\nIn the 15th century, the protective function of keeps was compromised by improved artillery. For example, in 1464 during the Wars of the Roses, the keep of Bamburgh Castle on the Northumberland coast, previously considered to be impregnable, was defeated with bombards. By the 16th century, keeps were slowly falling out of fashion as fortifications and residences. Many were destroyed in civil wars between the 17th and 18th centuries or incorporated into gardens as an alternative to follies. During the 19th century, keeps became fashionable once again, and in England and France, a number were restored or redesigned by Gothic architects. Despite further damage to many French and Spanish keeps during the wars of the 20th century, keeps now form an important part of the tourist and heritage industry in Europe."
    },
    {
      "id": "3424576",
      "title": "Kernel method",
      "url": "https://en.wikipedia.org/wiki/Kernel_method",
      "summary": "In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). These methods involve using linear classifiers to solve nonlinear problems. The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over all pairs of data points computed using inner products. The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to the Representer theorem. Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing. \nKernel methods  owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space.  This operation is often computationally cheaper than the explicit computation of the coordinates.  This approach is called the \"kernel trick\". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.\nAlgorithms capable of operating with kernels include the kernel perceptron, support-vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others.\nMost kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity)."
    },
    {
      "id": "8771825",
      "title": "Kernel regression",
      "url": "https://en.wikipedia.org/wiki/Kernel_regression",
      "summary": "In statistics, kernel regression is a non-parametric technique to estimate the conditional expectation of a random variable. The objective is to find a non-linear relation between a pair of random variables X and Y.\nIn any nonparametric regression, the conditional expectation of a variable \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   relative to a variable \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   may be written:\n\n  \n    \n      \n        E\n        \u2061\n        (\n        Y\n        \u2223\n        X\n        )\n        =\n        m\n        (\n        X\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (Y\\mid X)=m(X)}\n  where \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   is an unknown function.\n\n"
    },
    {
      "id": "42253",
      "title": "Data mining",
      "url": "https://en.wikipedia.org/wiki/Data_mining",
      "summary": "Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. Often the more general terms (large scale) data analysis and analytics\u2014or, when referring to actual methods, artificial intelligence and machine learning\u2014are more appropriate.\nThe actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps.\nThe difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations."
    },
    {
      "id": "67944487",
      "title": "Knowledge graph embedding",
      "url": "https://en.wikipedia.org/wiki/Knowledge_graph_embedding",
      "summary": "In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning, is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.\n\n"
    },
    {
      "id": "16920",
      "title": "Knowledge representation and reasoning",
      "url": "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
      "summary": "Knowledge representation and reasoning (KRR, KR&R, KR\u00b2) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems, and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning.\nExamples of knowledge representation formalisms include semantic nets, frames,  rules,  logic programs and ontologies. Examples of automated reasoning engines include inference engines, theorem provers,  model generators and classifiers.\n\n"
    },
    {
      "id": "61071091",
      "title": "Kubeflow",
      "url": "https://en.wikipedia.org/wiki/Kubeflow",
      "summary": "Kubeflow is an open-source platform for machine learning and MLOps on Kubernetes introduced by Google. The different stages in a typical machine learning lifecycle are represented with different software components in Kubeflow, including model development (Kubeflow Notebooks), model training (Kubeflow Pipelines, Kubeflow Training Operator), model serving (KServe), and automated machine learning (Katib).\nEach component of Kubeflow can be deployed separately, and it is not a requirement to deploy every component."
    },
    {
      "id": "34913689",
      "title": "LIONsolver",
      "url": "https://en.wikipedia.org/wiki/LIONsolver",
      "summary": "LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling and reactive business intelligence approach. A non-profit version is also available as LIONoso.\nLIONsolver is used to build models, visualize them, and improve business and engineering processes. \nIt is a tool for decision making based on data and quantitative model and it can be connected to most databases and external programs. \nThe software is fully integrated with the Grapheur business intelligence  and intended for more advanced users.\n\n"
    },
    {
      "id": "7914867",
      "title": "Lama",
      "url": "https://en.wikipedia.org/wiki/Lama",
      "summary": "Lama (Tibetan: \u0f56\u0fb3\u0f0b\u0f58\u0f0b, Wylie: bla-ma; \"boss\") is a title for a teacher of the Dharma in Tibetan Buddhism. The name is similar to the Sanskrit term guru, meaning \"heavy one\", endowed with qualities the student will eventually embody. The Tibetan word \"lama\" means \"highest principle\", and less literally \"highest mother\" or \"highest father\" to show close relationship between teacher and student.Historically, the term was used for venerated spiritual masters or heads of monasteries. Today the title can be used as an honorific title conferred on a monk, nun or a lay person (especially in the Nyingma, Kagyu and Sakya schools) advanced tantric practitioner to designate a level of spiritual attainment and authority to teach, or may be part of a title such as Dalai Lama or Panchen Lama applied to a lineage of reincarnate lamas (Tulkus).\nPerhaps due to misunderstandings by early western scholars attempting to understand Tibetan Buddhism, the term lama has historically been erroneously applied to Tibetan monks in general. Similarly, Tibetan Buddhism was referred to as \"Lamaism\" by early western scholars and travelers who perhaps did not understand that what they were witnessing was a form of Buddhism; they may also have been unaware of the distinction between Tibetan Buddhism and B\u00f6n. The term Lamaism is now considered by some to be derogatory.In the Vajrayana path of Tibetan Buddhism, the lama is often the tantric spiritual guide, the guru to the aspiring Buddhist yogi or yogini. As such, the lama will then appear as one of the Three Roots (a variant of the Three Jewels), alongside the yidam and protector (who may be a dakini, dharmapala or other Buddhist deity figure). The mind of the lama is considered Buddha \u2013 one's highest potential, the lama's speech is dharma, and the lama's body is one's guide and companion on the way to enlightenment, meaning the lama is the perfect embodiment of the sangha. Another expression of lama can be expressed through the 3 Kayas.\n\n"
    },
    {
      "id": "17851",
      "title": "Lambda",
      "url": "https://en.wikipedia.org/wiki/Lambda",
      "summary": "Lambda (; uppercase \u039b, lowercase \u03bb; Greek: \u03bb\u03ac\u03bc(\u03b2)\u03b4\u03b1, l\u00e1m(b)da) is the eleventh letter of the Greek alphabet, representing the voiced alveolar lateral approximant IPA: [l]. In the system of Greek numerals, lambda has a value of 30. Lambda is derived from the Phoenician Lamed. Lambda gave rise to the Latin L and the Cyrillic El (\u041b). The ancient grammarians and dramatists give evidence to the pronunciation as [la\u02d0bda\u02d0] (\u03bb\u03ac\u03b2\u03b4\u03b1) in Classical Greek times. In Modern Greek, the name of the letter, \u039b\u03ac\u03bc\u03b4\u03b1, is pronounced [\u02c8lam.\u00f0a].\nIn early Greek alphabets, the shape and orientation of lambda varied. Most variants consisted of two straight strokes, one longer than the other, connected at their ends. The angle might be in the upper-left, lower-left (\"Western\" alphabets) or top (\"Eastern\" alphabets). Other variants had a vertical line with a horizontal or sloped stroke running to the right. With the general adoption of the Ionic alphabet, Greek settled on an angle at the top; the Romans put the angle at the lower-left.\nThe HTML 4 character entity references for the Greek capital and small letter lambda are &#923; and &#955; respectively. The Unicode code points for lambda are U+039B and U+03BB.\n\n"
    },
    {
      "id": "1911810",
      "title": "Language model",
      "url": "https://en.wikipedia.org/wiki/Language_model",
      "summary": "A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed \u2018Shannon-style\u2019 experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.Language models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, handwriting recognition, grammar induction, and information retrieval.Large language models, currently their most advanced form, are a combination of larger datasets (frequently using scraped words from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model."
    },
    {
      "id": "73248112",
      "title": "Large language model",
      "url": "https://en.wikipedia.org/wiki/Large_language_model",
      "summary": "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs are artificial neural networks, the largest and most capable of which are built with a transformer-based architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's PaLM and Gemini (used in Bard), Meta's LLaMA family of open-source models, and Anthropic's Claude models.\n\n"
    },
    {
      "id": "30806",
      "title": "Tree (data structure)",
      "url": "https://en.wikipedia.org/wiki/Tree_(data_structure)",
      "summary": "In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent, except for the root node, which has no parent (i.e., the root node as the top-most node in the tree hierarchy). These constraints mean there are no cycles or \"loops\" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes (parent and children nodes of a node under consideration if they exists) in a single straight line (called edge or link between two adjacent nodes).\nBinary trees are a commonly used type, which constrain the number of children for each parent to at most two. When the order of the children is specified, this data structure corresponds to an ordered tree in graph theory. A value or pointer to other data may be associated with every node in the tree, or sometimes only with the leaf nodes, which have no children nodes.\nThe abstract data type (ADT) can be represented in a number of ways, including a list of parents with pointers to children, a list of children with pointers to parents, or a list of nodes and a separate list of parent-child relations (a specific type of adjacency list). Representations might also be more complicated, for example using indexes or ancestor lists for performance.\nTrees as used in computing are similar to but can be different from mathematical constructs of trees in graph theory, trees in set theory, and trees in descriptive set theory.\n\n"
    },
    {
      "id": "854461",
      "title": "Learning classifier system",
      "url": "https://en.wikipedia.org/wiki/Learning_classifier_system",
      "summary": "Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning).  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling, classification, data mining, regression, function approximation, or game strategy).  This approach allows complex solution spaces to be broken up into smaller, simpler parts.\nThe founding concepts behind learning classifier systems came from attempts to model complex adaptive systems, using rule-based agents to form an artificial cognitive system (i.e. artificial intelligence).\n\n"
    },
    {
      "id": "25050663",
      "title": "Learning to rank",
      "url": "https://en.wikipedia.org/wiki/Learning_to_rank",
      "summary": "Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data may, for example, consist of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data."
    },
    {
      "id": "4909283",
      "title": "Leo Breiman",
      "url": "https://en.wikipedia.org/wiki/Leo_Breiman",
      "summary": "Leo Breiman (January 27, 1928 \u2013 July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards, and was a member of the United States National Academy of Sciences.\nBreiman's work helped to bridge the gap between statistics and computer science, particularly in the field of machine learning. His most important contributions were his work on classification and regression trees and ensembles of trees fit to bootstrap samples. Bootstrap aggregation was given the name bagging by Breiman. Another of Breiman's ensemble approaches is the random forest.\n\n"
    },
    {
      "id": "106421",
      "title": "Library (computing)",
      "url": "https://en.wikipedia.org/wiki/Library_(computing)",
      "summary": "In computer science, a library is a collection of resources that is leveraged during software development to implement a computer program.\nResources may include configuration data, documentation, help data, message templates, source code or pre-compiled functions and classes, values or type specifications. \nIn IBM's OS/360 and its successors this is called a partitioned data set.\nA library of functions has a well-defined interface by which the functions are invoked. For instance, a program could use a library to indirectly make system calls instead of making those system calls directly in the program. In addition, the functions are exposed by the library for reuse by multiple, independent programs. \nA program invokes the library functions via a well-defined mechanism. For example, in C, a library function is invoked by using C's normal function call. The linker generates code to call a function via the library mechanism if the function is available from a library instead of from the program itself.Library functions are available to be used by multiple, unrelated programs, whereas a function defined in a program can only be used by that program. This distinction can gain a hierarchical notion when a program grows large. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. \nA distinguishing feature of a library that it can be used by multiple, independent programs, and the programmer only needs to know the interface -- not the internal details of the library.\nThe value of a library lies in the reuse of standardized program elements. When a program invokes a library, it gains the behavior implemented inside the library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion and ease the distribution of the code.\nThe functions of a library can be connected to the invoking program at different program lifecycle phases. If the code of the library is accessed during the build of the invoking program, then the library is called a static library. An alternative is to build the program executable to be separate from the library file. The library functions are connected after the executable is started, either at load-time or runtime. In this case, the library is called a dynamic library.\nMost compiled languages have a standard library, although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have organized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries."
    },
    {
      "id": "64785522",
      "title": "LightGBM",
      "url": "https://en.wikipedia.org/wiki/LightGBM",
      "summary": "LightGBM, short for light gradient-boosting machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability."
    },
    {
      "id": "98974",
      "title": "Linear classifier",
      "url": "https://en.wikipedia.org/wiki/Linear_classifier",
      "summary": "In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.       5\u201312\u201323\n\n"
    },
    {
      "id": "1470657",
      "title": "Linear discriminant analysis",
      "url": "https://en.wikipedia.org/wiki/Linear_discriminant_analysis",
      "summary": "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\nLDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.\nLDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.\nLDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type."
    },
    {
      "id": "48758386",
      "title": "Linear regression",
      "url": "https://en.wikipedia.org/wiki/Linear_regression",
      "summary": "In statistics, linear regression is a statistical model which estimates the linear relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. If the explanatory variables are measured with error then errors-in-variables models are required, also known as measurement error models.\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is error i.e variance reduction in prediction or forecasting, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Use of the Mean Squared Error(MSE) as the cost on a dataset that has many large outliers, can result in a model that fits the outliers more than the true data due to the higher importance assigned by MSE to large errors. So, cost functions that are robust to outliers should be used if the dataset has many large outliers.  Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous."
    },
    {
      "id": "2142",
      "title": "List of artificial intelligence projects",
      "url": "https://en.wikipedia.org/wiki/List_of_artificial_intelligence_projects",
      "summary": "The following is a list of current and past, non-classified notable artificial intelligence projects.\n\n"
    },
    {
      "id": "73733460",
      "title": "List of datasets in computer vision and image processing",
      "url": "https://en.wikipedia.org/wiki/List_of_datasets_in_computer_vision_and_image_processing",
      "summary": "This is a list of datasets for machine learning research. It is part of the list of datasets for machine-learning research. These datasets consist primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification."
    },
    {
      "id": "454351",
      "title": "List of important publications in computer science",
      "url": "https://en.wikipedia.org/wiki/List_of_important_publications_in_computer_science",
      "summary": "This is a list of important publications in computer science, organized by field. Some reasons why a particular publication might be regarded as important:\n\nTopic creator \u2013 A publication that created a new topic\nBreakthrough \u2013 A publication that changed scientific knowledge significantly\nInfluence \u2013 A publication which has significantly influenced the world or has had a massive impact on the teaching of computer science."
    },
    {
      "id": "27321681",
      "title": "Local outlier factor",
      "url": "https://en.wikipedia.org/wiki/Local_outlier_factor",
      "summary": "In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and J\u00f6rg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.LOF shares some concepts with DBSCAN and OPTICS such as the concepts of \"core distance\" and \"reachability distance\", which are used for local density estimation."
    },
    {
      "id": "1151991",
      "title": "Logic in computer science",
      "url": "https://en.wikipedia.org/wiki/Logic_in_computer_science",
      "summary": "Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:\n\nTheoretical foundations and analysis\nUse of computer technology to aid logicians\nUse of concepts from logic for computer applications\n\n"
    },
    {
      "id": "17927",
      "title": "Logic programming",
      "url": "https://en.wikipedia.org/wiki/Logic_programming",
      "summary": "Logic programming is a programming, database and knowledge representation paradigm based on formal logic. A logic program is a set of sentences in logical form, representing knowledge about some problem domain. Computation is performed by applying logical reasoning to that knowledge, to solve problems in the domain.  Major logic programming language families include Prolog, Answer Set Programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\n\nA :- B1, ..., Bn.and are read as declarative sentences in logical form:\n\nA if B1 and ... and Bn.A is called the head of the rule, B1, ..., Bn is called the body, and the Bi are called  literals or conditions. When n = 0, the rule is called a fact and is written in the simplified form:\n\nA.Queries (or goals) have the same syntax as the bodies of rules and are commonly written in the form:\n\n?- B1, ..., Bn.In the simplest case of Horn clauses (or \"definite\" clauses), all of the A, B1, ..., Bn are atomic formulae of the form p(t1 ,..., tm), where p is a predicate symbol naming a relation, like \"motherhood\", and the ti are terms naming objects (or individuals). Terms include both constant symbols, like \"charles\", and variables, such as X, which start with an upper case letter.\nConsider, for example, the following Horn clause program:\n\nGiven a query, the program produces answers.\nFor instance for a query  ?- parent_child(X, william), the single answer is\n\nVarious queries can be asked.  For instance\nthe program can be queried both to generate grandparents and to generate grandchildren. It can even be used to generate all pairs of grandchildren and grandparents, or simply to check if a given pair is such a pair:\n\nAlthough Horn clause logic programs are  Turing complete, for most practical applications, Horn clause programs need to be extended to \"normal\" logic programs with negative conditions. For example, the definition of sibling uses a negative condition, where the   predicate = is defined by the clause  X = X: \n\nLogic programming languages that include negative conditions have the knowledge representation capabilities of a non-monotonic logic.\nIn ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures. From this point of view,  clause A :- B1,...,Bn is understood as:\n\nto solve A, solve B1, and ... and solve Bn.Negative conditions in the bodies of clauses also have a procedural interpretation, known as negation as failure: A negative literal  not B is deemed to hold if and only if the positive literal  B fails to hold.\nMuch of the research in the field of logic programming has been concerned with trying to develop a logical semantics for negation as failure and with developing other semantics and other implementations for negation. These developments have been important, in turn, for supporting the development of formal methods for logic-based program verification and program transformation."
    },
    {
      "id": "18152",
      "title": "Logical conjunction",
      "url": "https://en.wikipedia.org/wiki/Logical_conjunction",
      "summary": "In logic, mathematics and linguistics, and (\n  \n    \n      \n        \u2227\n      \n    \n    {\\displaystyle \\wedge }\n  ) is the truth-functional operator of conjunction or logical conjunction. The logical connective of this operator is typically represented as \n  \n    \n      \n        \u2227\n      \n    \n    {\\displaystyle \\wedge }\n   or \n  \n    \n      \n        &\n      \n    \n    {\\displaystyle \\&}\n   or \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n   (prefix) or \n  \n    \n      \n        \u00d7\n      \n    \n    {\\displaystyle \\times }\n   or \n  \n    \n      \n        \u22c5\n      \n    \n    {\\displaystyle \\cdot }\n   in which \n  \n    \n      \n        \u2227\n      \n    \n    {\\displaystyle \\wedge }\n   is the most modern and widely used.\nThe and of a set of operands is true if and only if all of its operands are true, i.e., \n  \n    \n      \n        A\n        \u2227\n        B\n      \n    \n    {\\displaystyle A\\land B}\n   is true if and only if \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is true and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n   is true.\nAn operand of a conjunction is a conjunct.\nBeyond logic, the term \"conjunction\" also refers to similar concepts in other fields:\n\nIn natural language, the denotation of expressions such as English \"and\";\nIn programming languages, the short-circuit and control structure;\nIn set theory, intersection.\nIn lattice theory, logical conjunction (greatest lower bound).\n\n"
    },
    {
      "id": "226631",
      "title": "Logistic regression",
      "url": "https://en.wikipedia.org/wiki/Logistic_regression",
      "summary": "In statistics, the logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See \u00a7 Background and \u00a7 Definition for formal mathematics, and \u00a7 Example for a worked example.\nBinary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see \u00a7 Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See \u00a7 Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.\nAnalogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see \u00a7 Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the \"simplest\" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see \u00a7 Maximum entropy.\nThe parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see \u00a7 Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see \u00a7 Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined \"logit\"; see \u00a7 History."
    },
    {
      "id": "10711453",
      "title": "Long short-term memory",
      "url": "https://en.wikipedia.org/wiki/Long_short-term_memory",
      "summary": "Long short-term memory (LSTM) network is a recurrent neural network (RNN), aimed to deal with the vanishing gradient problem present in traditional RNNs. Its relative insensitivity to gap length is its advantage over other RNNs, hidden Markov models and other sequence learning methods. It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\". It is applicable to classification, processing and predicting data based on time series, such as in handwriting, speech recognition,  machine translation, speech activity detection, robot control, video games, and healthcare.A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.\n\n"
    },
    {
      "id": "442137",
      "title": "Loss function",
      "url": "https://en.wikipedia.org/wiki/Loss_function",
      "summary": "In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function)  is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\nIn statistics, typically a  loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.  In the context of economics, for example, this is usually economic cost or regret.  In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cram\u00e9r in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss."
    },
    {
      "id": "44578554",
      "title": "Loss functions for classification",
      "url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification",
      "summary": "In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).  Given \n  \n    \n      \n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {X}}}\n   as the space of all possible inputs (usually \n  \n    \n      \n        \n          \n            X\n          \n        \n        \u2282\n        \n          \n            R\n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {X}}\\subset \\mathbb {R} ^{d}}\n  ), and \n  \n    \n      \n        \n          \n            Y\n          \n        \n        =\n        {\n        \u2212\n        1\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle {\\mathcal {Y}}=\\{-1,1\\}}\n   as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function \n  \n    \n      \n        f\n        :\n        \n          \n            X\n          \n        \n        \u2192\n        \n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle f:{\\mathcal {X}}\\to {\\mathcal {Y}}}\n   which best predicts a label \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   for a given input \n  \n    \n      \n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {x}}}\n  .  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same \n  \n    \n      \n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n      \n    \n    {\\displaystyle {\\vec {x}}}\n   to generate different \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  .  As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as\n\n  \n    \n      \n        I\n        [\n        f\n        ]\n        =\n        \n          \n            \u222b\n            \n              \n                \n                  X\n                \n              \n              \u00d7\n              \n                \n                  Y\n                \n              \n            \n          \n          V\n          (\n          f\n          (\n          \n            \n              \n                x\n                \u2192\n              \n            \n          \n          )\n          ,\n          y\n          )\n          \n          p\n          (\n          \n            \n              \n                x\n                \u2192\n              \n            \n          \n          ,\n          y\n          )\n          \n          d\n          \n            \n              \n                x\n                \u2192\n              \n            \n          \n          \n          d\n          y\n        \n      \n    \n    {\\displaystyle I[f]=\\displaystyle \\int _{{\\mathcal {X}}\\times {\\mathcal {Y}}}V(f({\\vec {x}}),y)\\,p({\\vec {x}},y)\\,d{\\vec {x}}\\,dy}\n  where \n  \n    \n      \n        V\n        (\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle V(f({\\vec {x}}),y)}\n   is a given loss function, and  \n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        ,\n        y\n        )\n      \n    \n    {\\displaystyle p({\\vec {x}},y)}\n   is the probability density function of the process that generated the data, which can equivalently be written as\n\n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        ,\n        y\n        )\n        =\n        p\n        (\n        y\n        \u2223\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        p\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle p({\\vec {x}},y)=p(y\\mid {\\vec {x}})p({\\vec {x}}).}\n  Within classification, several commonly used loss functions are written solely in terms of the product of the true label \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and the predicted label \n  \n    \n      \n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle f({\\vec {x}})}\n  . Therefore, they can be defined as functions of only one variable \n  \n    \n      \n        \u03c5\n        =\n        y\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle \\upsilon =yf({\\vec {x}})}\n  , so that \n  \n    \n      \n        V\n        (\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        ,\n        y\n        )\n        =\n        \u03d5\n        (\n        y\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        )\n        =\n        \u03d5\n        (\n        \u03c5\n        )\n      \n    \n    {\\displaystyle V(f({\\vec {x}}),y)=\\phi (yf({\\vec {x}}))=\\phi (\\upsilon )}\n   with a suitably chosen function \n  \n    \n      \n        \u03d5\n        :\n        \n          R\n        \n        \u2192\n        \n          R\n        \n      \n    \n    {\\displaystyle \\phi :\\mathbb {R} \\to \\mathbb {R} }\n  . These are called margin-based loss functions. Choosing a margin-based loss function amounts to choosing \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n  . Selection of a loss function within this framework impacts the optimal \n  \n    \n      \n        \n          f\n          \n            \u03d5\n          \n          \n            \u2217\n          \n        \n      \n    \n    {\\displaystyle f_{\\phi }^{*}}\n   which minimizes the expected risk, see empirical risk minimization. \nIn the case of binary classification, it is possible to simplify the calculation of expected risk from the integral specified above.  Specifically,\n\n  \n    \n      \n        \n          \n            \n              \n                I\n                [\n                f\n                ]\n              \n              \n                \n                =\n                \n                  \u222b\n                  \n                    \n                      \n                        X\n                      \n                    \n                    \u00d7\n                    \n                      \n                        Y\n                      \n                    \n                  \n                \n                V\n                (\n                f\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                ,\n                y\n                )\n                \n                p\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                ,\n                y\n                )\n                \n                d\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                \n                d\n                y\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u222b\n                  \n                    \n                      X\n                    \n                  \n                \n                \n                  \u222b\n                  \n                    \n                      Y\n                    \n                  \n                \n                \u03d5\n                (\n                y\n                f\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                )\n                \n                p\n                (\n                y\n                \u2223\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                \n                p\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                \n                d\n                y\n                \n                d\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u222b\n                  \n                    \n                      X\n                    \n                  \n                \n                [\n                \u03d5\n                (\n                f\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                )\n                \n                p\n                (\n                1\n                \u2223\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                +\n                \u03d5\n                (\n                \u2212\n                f\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                )\n                \n                p\n                (\n                \u2212\n                1\n                \u2223\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                ]\n                \n                p\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                \n                d\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  \u222b\n                  \n                    \n                      X\n                    \n                  \n                \n                [\n                \u03d5\n                (\n                f\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                )\n                \n                p\n                (\n                1\n                \u2223\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                +\n                \u03d5\n                (\n                \u2212\n                f\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                )\n                \n                (\n                1\n                \u2212\n                p\n                (\n                1\n                \u2223\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                )\n                ]\n                \n                p\n                (\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n                )\n                \n                d\n                \n                  \n                    \n                      x\n                      \u2192\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}I[f]&=\\int _{{\\mathcal {X}}\\times {\\mathcal {Y}}}V(f({\\vec {x}}),y)\\,p({\\vec {x}},y)\\,d{\\vec {x}}\\,dy\\\\[6pt]&=\\int _{\\mathcal {X}}\\int _{\\mathcal {Y}}\\phi (yf({\\vec {x}}))\\,p(y\\mid {\\vec {x}})\\,p({\\vec {x}})\\,dy\\,d{\\vec {x}}\\\\[6pt]&=\\int _{\\mathcal {X}}[\\phi (f({\\vec {x}}))\\,p(1\\mid {\\vec {x}})+\\phi (-f({\\vec {x}}))\\,p(-1\\mid {\\vec {x}})]\\,p({\\vec {x}})\\,d{\\vec {x}}\\\\[6pt]&=\\int _{\\mathcal {X}}[\\phi (f({\\vec {x}}))\\,p(1\\mid {\\vec {x}})+\\phi (-f({\\vec {x}}))\\,(1-p(1\\mid {\\vec {x}}))]\\,p({\\vec {x}})\\,d{\\vec {x}}\\end{aligned}}}\n  The second equality follows from the properties described above.  The third equality follows from the fact that 1 and \u22121 are the only possible values for \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  , and the fourth because \n  \n    \n      \n        p\n        (\n        \u2212\n        1\n        \u2223\n        x\n        )\n        =\n        1\n        \u2212\n        p\n        (\n        1\n        \u2223\n        x\n        )\n      \n    \n    {\\displaystyle p(-1\\mid x)=1-p(1\\mid x)}\n  . The term within brackets \n  \n    \n      \n        [\n        \u03d5\n        (\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        )\n        p\n        (\n        1\n        \u2223\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        +\n        \u03d5\n        (\n        \u2212\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        )\n        (\n        1\n        \u2212\n        p\n        (\n        1\n        \u2223\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        )\n        ]\n      \n    \n    {\\displaystyle [\\phi (f({\\vec {x}}))p(1\\mid {\\vec {x}})+\\phi (-f({\\vec {x}}))(1-p(1\\mid {\\vec {x}}))]}\n   is known as the conditional risk.\nOne can solve for the minimizer of \n  \n    \n      \n        I\n        [\n        f\n        ]\n      \n    \n    {\\displaystyle I[f]}\n   by taking the functional derivative of the last equality with respect to \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n   and setting the derivative equal to 0.  This will result in the following equation\n\n  \n    \n      \n        \n          \n            \n              \u2202\n              \u03d5\n              (\n              f\n              )\n            \n            \n              \u2202\n              f\n            \n          \n        \n        \u03b7\n        +\n        \n          \n            \n              \u2202\n              \u03d5\n              (\n              \u2212\n              f\n              )\n            \n            \n              \u2202\n              f\n            \n          \n        \n        (\n        1\n        \u2212\n        \u03b7\n        )\n        =\n        0\n        \n        \n        \n        \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle {\\frac {\\partial \\phi (f)}{\\partial f}}\\eta +{\\frac {\\partial \\phi (-f)}{\\partial f}}(1-\\eta )=0\\;\\;\\;\\;\\;(1)}\n  which is also equivalent to setting the derivative of the conditional risk equal to zero.\nGiven the binary nature of classification, a natural selection for a loss function (assuming equal cost for false positives and false negatives) would be the 0-1 loss function (0\u20131 indicator function), which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by\n\n  \n    \n      \n        V\n        (\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        ,\n        y\n        )\n        =\n        H\n        (\n        \u2212\n        y\n        f\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle V(f({\\vec {x}}),y)=H(-yf({\\vec {x}}))}\n  where \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   indicates the Heaviside step function.\nHowever, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem.  As a result, it is better to substitute loss function surrogates which are tractable for commonly used learning algorithms, as they have convenient properties such as being convex and smooth.  In addition to  their computational tractability,  one can show that the solutions to the learning problem using these loss surrogates allow for the recovery of the actual solution to the original classification problem.  Some of these surrogates are described below.\nIn practice, the probability distribution \n  \n    \n      \n        p\n        (\n        \n          \n            \n              x\n              \u2192\n            \n          \n        \n        ,\n        y\n        )\n      \n    \n    {\\displaystyle p({\\vec {x}},y)}\n   is unknown.  Consequently, utilizing a training set of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   independently and identically distributed sample points\n\n  \n    \n      \n        S\n        =\n        {\n        (\n        \n          \n            \n              \n                x\n                \u2192\n              \n            \n          \n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            1\n          \n        \n        )\n        ,\n        \u2026\n        ,\n        (\n        \n          \n            \n              \n                x\n                \u2192\n              \n            \n          \n          \n            n\n          \n        \n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle S=\\{({\\vec {x}}_{1},y_{1}),\\dots ,({\\vec {x}}_{n},y_{n})\\}}\n  drawn from the data sample space, one seeks to minimize empirical risk\n\n  \n    \n      \n        \n          I\n          \n            S\n          \n        \n        [\n        f\n        ]\n        =\n        \n          \n            1\n            n\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        V\n        (\n        f\n        (\n        \n          \n            \n              \n                x\n                \u2192\n              \n            \n          \n          \n            i\n          \n        \n        )\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle I_{S}[f]={\\frac {1}{n}}\\sum _{i=1}^{n}V(f({\\vec {x}}_{i}),y_{i})}\n  as a proxy for expected risk. (See statistical learning theory for a more detailed description.)\n\n"
    },
    {
      "id": "20412",
      "title": "MATLAB",
      "url": "https://en.wikipedia.org/wiki/MATLAB",
      "summary": "MATLAB (an abbreviation of \"MATrix LABoratory\") is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages.\nAlthough MATLAB is intended primarily for numeric computing, an optional toolbox uses the MuPAD symbolic engine allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.\nAs of 2020, MATLAB has more than four million users worldwide. They come from various backgrounds of engineering, science, and economics. As of 2017, more than 5000 global colleges and universities use MATLAB to support instruction and research.\n\n"
    },
    {
      "id": "434274",
      "title": "MIT Computer Science and Artificial Intelligence Laboratory",
      "url": "https://en.wikipedia.org/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory",
      "summary": "Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research institute at the Massachusetts Institute of Technology (MIT) formed by the 2003 merger of the Laboratory for Computer Science (LCS) and the Artificial Intelligence Laboratory (AI Lab). Housed within the Ray and Maria Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership. It is part of the Schwarzman College of Computing but is also overseen by the MIT Vice President of Research.\n\n"
    },
    {
      "id": "57388949",
      "title": "ML.NET",
      "url": "https://en.wikipedia.org/wiki/ML.NET",
      "summary": "ML.NET is a free software machine learning library for the C# and F# programming languages. It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks. Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.\n\n"
    },
    {
      "id": "39227709",
      "title": "Massive Online Analysis",
      "url": "https://en.wikipedia.org/wiki/Massive_Online_Analysis",
      "summary": "Massive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.\n\n"
    },
    {
      "id": "52513310",
      "title": "Apache MXNet",
      "url": "https://en.wikipedia.org/wiki/Apache_MXNet",
      "summary": "Apache MXNet is an open-source deep learning software framework that trains and deploys deep neural networks. It is scalable, allows fast model training, and supports a flexible programming model and multiple programming languages (including C++, Python, Java, Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language). The MXNet library is portable and can scale to multiple GPUs and machines. It was co-developed by Carlos Guestrin at the University of Washington (along with GraphLab).As of September 2023, it is no longer actively developed.\n\n"
    },
    {
      "id": "32237314",
      "title": "Machine ethics",
      "url": "https://en.wikipedia.org/wiki/Machine_ethics",
      "summary": "Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents. Machine ethics differs from other ethical fields related to engineering and technology.  Machine ethics should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with the grander social effects of technology.\n\n"
    },
    {
      "id": "53802271",
      "title": "Machine learning control",
      "url": "https://en.wikipedia.org/wiki/Machine_learning_control",
      "summary": "Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\nwhich solves optimal control problems with methods of machine learning.\nKey applications are complex nonlinear systems\nfor which linear control theory methods are not applicable.\n\n"
    },
    {
      "id": "53970843",
      "title": "Machine learning in bioinformatics",
      "url": "https://en.wikipedia.org/wiki/Machine_learning_in_bioinformatics",
      "summary": "Machine learning in bioinformatics is the application of machine learning algorithms to bioinformatics, including genomics, proteomics, microarrays, systems biology, evolution, and text mining.Prior to the emergence of machine learning, bioinformatics algorithms had to be programmed by hand; for problems such as protein structure prediction, this proved difficult. Machine learning techniques, such as deep learning can learn features of data sets, instead of requiring the programmer to define them individually. The algorithm can further learn how to combine low-level features into more abstract features, and so on. This multi-layered approach allows such systems to make sophisticated predictions when appropriately trained. These methods contrast with other computational biology approaches which, while exploiting existing datasets, do not allow the data to be interpreted and analyzed in unanticipated ways. \n\n"
    },
    {
      "id": "68735447",
      "title": "Machine learning in earth sciences",
      "url": "https://en.wikipedia.org/wiki/Machine_learning_in_earth_sciences",
      "summary": "Applications of machine learning in earth sciences include geological mapping, gas leakage detection and geological features identification. Machine learning (ML) is a type of artificial intelligence (AI) that enables computer systems to classify, cluster, identify and analyze vast and complex sets of data while eliminating the need for explicit instructions and programming. Earth science is the study of the origin, evolution, and future of the planet Earth. The Earth system can be subdivided into four major components including the solid earth, atmosphere, hydrosphere and biosphere.A variety of algorithms may be applied depending on the nature of the earth science exploration. Some algorithms may perform significantly better than others for particular objectives. For example, convolutional neural networks (CNN) are good at interpreting images, artificial neural networks (ANN) perform well in soil classification but more computationally expensive to train than support-vector machine (SVM) learning. The application of machine learning has been popular in recent decades, as the development of other technologies such as unmanned aerial vehicles (UAVs), ultra-high resolution remote sensing technology and high-performance computing units lead to the availability of large high-quality datasets and more advanced algorithms.\n\n"
    },
    {
      "id": "61373032",
      "title": "Machine learning in physics",
      "url": "https://en.wikipedia.org/wiki/Machine_learning_in_physics",
      "summary": "Applying classical methods of machine learning to the study of quantum systems is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other examples include learning Hamiltonians, learning quantum phase transitions, and automatically generating new quantum experiments. Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials or directly solving the Schr\u00f6dinger equation with a variational method.\n\n"
    },
    {
      "id": "11920671",
      "title": "Machine perception",
      "url": "https://en.wikipedia.org/wiki/Machine_perception",
      "summary": "Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.Machine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, machine touch, and machine smelling, as artificial scents are, at a chemical compound, molecular, atomic level, indiscernible and identical.The end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing. This purpose is very similar to the proposed purposes for artificial intelligence generally, except that machine perception would only grant machines limited sentience, rather than bestow upon machines full consciousness, self-awareness, and intentionality.\n\n"
    },
    {
      "id": "19980",
      "title": "Machine translation",
      "url": "https://en.wikipedia.org/wiki/Machine_translation",
      "summary": "Machine translation is use of  either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches to translation of text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages."
    },
    {
      "id": "5371104",
      "title": "Mallet (software project)",
      "url": "https://en.wikipedia.org/wiki/Mallet_(software_project)",
      "summary": "MALLET is a Java \"Machine Learning for Language Toolkit\".\n\n"
    },
    {
      "id": "75795581",
      "title": "Mamba (deep learning architecture)",
      "url": "https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture)",
      "summary": "Mamba is a deep learning architecture focused on sequence modeling. It was developed by researchers from Carnegie Mellon University and Princeton University to address some limitations of transformer models, especially in processing long sequences, and it is based on the Structured State Space sequence (S4) model.\n\n"
    },
    {
      "id": "2073470",
      "title": "Manifold",
      "url": "https://en.wikipedia.org/wiki/Manifold",
      "summary": "In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -dimensional manifold, or \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -dimensional Euclidean space.\nOne-dimensional manifolds include lines and circles, but not self-crossing curves such as a figure 8. Two-dimensional manifolds are also called surfaces. Examples include the plane, the sphere, and the torus, and also the Klein bottle and real projective plane.\nThe concept of a manifold is central to many parts of geometry and modern mathematical physics because it allows complicated structures to be described in terms of well-understood topological properties of simpler spaces. Manifolds naturally arise as solution sets of systems of equations and as graphs of functions. The concept has applications in computer-graphics given the need to associate pictures with coordinates (e.g. CT scans).\nManifolds can be equipped with additional structure. One important class of manifolds are differentiable manifolds; their differentiable structure allows calculus to be done. A Riemannian metric on a manifold allows distances and angles to be measured. Symplectic manifolds serve as the phase spaces in the Hamiltonian formalism of classical mechanics, while four-dimensional Lorentzian manifolds model spacetime in general relativity.\nThe study of manifolds requires working knowledge of calculus and topology.\n\n"
    },
    {
      "id": "68581881",
      "title": "Manifold hypothesis",
      "url": "https://en.wikipedia.org/wiki/Manifold_hypothesis",
      "summary": "The manifold hypothesis posits that many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space. As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features.\nThe manifold hypothesis is related to the effectiveness of nonlinear dimensionality reduction techniques in machine learning. Many techniques of dimensional reduction make the assumption that data lies along a low-dimensional submanifold, such as manifold sculpting, manifold alignment, and manifold regularization.\nThe major implications of this hypothesis is that\n\nMachine learning models only have to fit relatively simple, low-dimensional, highly structured subspaces within their potential input space (latent manifolds).\nWithin one of these manifolds, it\u2019s always possible to interpolate between two inputs, that is to say, morph one into another via a continuous path along which all points fall on the manifold.The ability to interpolate between samples is the key to generalization in deep learning.\n\n"
    },
    {
      "id": "309261",
      "title": "Nonlinear dimensionality reduction",
      "url": "https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction",
      "summary": "Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself. The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis."
    },
    {
      "id": "48777199",
      "title": "Manifold regularization",
      "url": "https://en.wikipedia.org/wiki/Manifold_regularization",
      "summary": "In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\n\n"
    },
    {
      "id": "516931",
      "title": "Map (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Map_(mathematics)",
      "summary": "In mathematics, a map or mapping is a function in its general sense.  These terms may have originated as from the process of making a geographical map: mapping the Earth surface to a sheet of paper.The term map may be used to distinguish some special types of functions, such as homomorphisms. For example, a linear map is a homomorphism of vector spaces, while the term linear function may have this meaning or it may mean a linear polynomial. In category theory, a map may refer to a morphism. The term transformation can be used interchangeably, but transformation often refers to a function from a set to itself. There are also a few less common uses in logic and graph theory."
    },
    {
      "id": "350872",
      "title": "Market basket",
      "url": "https://en.wikipedia.org/wiki/Market_basket",
      "summary": "A market basket or commodity bundle is a fixed list of items, in given proportions.  Its most common use is to track the progress of inflation in an economy or specific market. That is, to measure the changes in the value of money over time.  A market basket is also used with the theory of purchasing price parity to measure the value of money in different places.\n\n"
    },
    {
      "id": "1125883",
      "title": "Markov decision process",
      "url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "summary": "In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.\nAt each time step, the process is in some state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  , and the decision maker may choose any action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   that is available in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  . The process responds at the next time step by randomly moving into a new state \n  \n    \n      \n        \n          s\n          \u2032\n        \n      \n    \n    {\\displaystyle s'}\n  , and giving the decision maker a corresponding reward \n  \n    \n      \n        \n          R\n          \n            a\n          \n        \n        (\n        s\n        ,\n        \n          s\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle R_{a}(s,s')}\n  .\nThe probability that the process moves into its new state \n  \n    \n      \n        \n          s\n          \u2032\n        \n      \n    \n    {\\displaystyle s'}\n   is influenced by the chosen action. Specifically, it is given by the state transition function \n  \n    \n      \n        \n          P\n          \n            a\n          \n        \n        (\n        s\n        ,\n        \n          s\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle P_{a}(s,s')}\n  . Thus, the next state \n  \n    \n      \n        \n          s\n          \u2032\n        \n      \n    \n    {\\displaystyle s'}\n   depends on the current state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   and the decision maker's action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  . But given \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfy the Markov property.\nMarkov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. \"wait\") and all rewards are the same (e.g. \"zero\"), a Markov decision process reduces to a Markov chain."
    },
    {
      "id": "48396",
      "title": "Mathematical analysis",
      "url": "https://en.wikipedia.org/wiki/Mathematical_analysis",
      "summary": "Analysis is the branch of mathematics dealing with continuous functions, limits, and related theories, such as differentiation, integration, measure, infinite sequences, series, and analytic functions.These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.\nAnalysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).\n\n"
    },
    {
      "id": "18881",
      "title": "Mathematical induction",
      "url": "https://en.wikipedia.org/wiki/Mathematical_induction",
      "summary": "Mathematical induction is a method for proving that a statement \n  \n    \n      \n        P\n        (\n        n\n        )\n      \n    \n    {\\displaystyle P(n)}\n   is true for every natural number \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , that is, that the infinitely many cases \n  \n    \n      \n        P\n        (\n        0\n        )\n        ,\n        P\n        (\n        1\n        )\n        ,\n        P\n        (\n        2\n        )\n        ,\n        P\n        (\n        3\n        )\n        ,\n        \u2026\n      \n    \n    {\\displaystyle P(0),P(1),P(2),P(3),\\dots }\n  \u2009 all hold. This is done by first proving a simple case, then also showing that if we assume the claim is true for a given case, then the next case is also true. Informal metaphors help to explain this technique, such as falling dominoes or climbing a ladder:\n\nMathematical induction proves that we can climb as high as we like on a ladder, by proving that we can climb onto the bottom rung (the basis) and that from each rung we can climb up to the next one (the step).\nA proof by induction consists of two cases. The first, the base case, proves the statement for \n  \n    \n      \n        n\n        =\n        0\n      \n    \n    {\\displaystyle n=0}\n   without assuming any knowledge of other cases. The second case, the induction step, proves that if the statement holds for any given case \n  \n    \n      \n        n\n        =\n        k\n      \n    \n    {\\displaystyle n=k}\n  , then it must also hold for the next case \n  \n    \n      \n        n\n        =\n        k\n        +\n        1\n      \n    \n    {\\displaystyle n=k+1}\n  . These two steps establish that the statement holds for every natural number \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  . The base case does not necessarily begin with \n  \n    \n      \n        n\n        =\n        0\n      \n    \n    {\\displaystyle n=0}\n  , but often with \n  \n    \n      \n        n\n        =\n        1\n      \n    \n    {\\displaystyle n=1}\n  , and possibly with any fixed natural number \n  \n    \n      \n        n\n        =\n        N\n      \n    \n    {\\displaystyle n=N}\n  , establishing the truth of the statement for all natural numbers \n  \n    \n      \n        n\n        \u2265\n        N\n      \n    \n    {\\displaystyle n\\geq N}\n  .\nThe method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction is an inference rule used in formal proofs, and is the foundation of most correctness proofs for computer programs.Although its name may suggest otherwise, mathematical induction should not be confused with inductive reasoning as used in philosophy (see Problem of induction). The mathematical method examines infinitely many cases to prove a general statement, but does so by a finite chain of deductive reasoning involving the variable \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  , which can take infinitely many values.\n\n"
    },
    {
      "id": "52033",
      "title": "Mathematical optimization",
      "url": "https://en.wikipedia.org/wiki/Mathematical_optimization",
      "summary": "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. It is generally divided into two subfields: discrete optimization and continuous optimization. Optimization problems arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the more general approach, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains."
    },
    {
      "id": "1727027",
      "title": "Mathematical software",
      "url": "https://en.wikipedia.org/wiki/Mathematical_software",
      "summary": "Mathematical software is software used to model, analyze or calculate numeric, symbolic or geometric data.\n\n"
    },
    {
      "id": "20556859",
      "title": "Matrix (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Matrix_(mathematics)",
      "summary": "In mathematics, a matrix (pl.: matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.\nFor example, \n\nis a matrix with two rows and three columns. This is often referred to as a \"two by three matrix\", a \"\n  \n    \n      \n        2\n        \u00d7\n        3\n      \n    \n    {\\displaystyle 2\\times 3}\n   matrix\", or a matrix of dimension \n  \n    \n      \n        2\n        \u00d7\n        3\n      \n    \n    {\\displaystyle 2\\times 3}\n  .\nMatrices are used to represent linear maps and allow explicit computations in linear algebra. Therefore, the study of matrices is a large part of linear algebra, and most properties and operations of abstract linear algebra can be expressed in terms of matrices. For example, matrix multiplication represents the composition of linear maps.\nNot all matrices are related to linear algebra. This is, in particular, the case in graph theory, of incidence matrices, and adjacency matrices. This article focuses on matrices related to linear algebra, and, unless otherwise specified, all matrices represent linear maps or may be viewed as such.\nSquare matrices, matrices with the same number of rows and columns, play a major role in matrix theory. Square matrices of a given dimension form a noncommutative ring, which is one of the most common examples of a noncommutative ring. The determinant of a square matrix is a number associated to the matrix, which is fundamental for the study of a square matrix; for example, a square matrix is invertible if and only if it has a nonzero determinant, and the eigenvalues of a square matrix are the roots of a polynomial determinant.\nIn geometry, matrices are widely used for specifying and representing geometric transformations (for example rotations) and coordinate changes. In numerical analysis, many computational problems are solved by reducing them to a matrix computation, and this often involves computing with matrices of huge dimension. Matrices are used in most areas of mathematics and most scientific fields, either directly, or through their use in geometry and numerical analysis.\nMatrix theory is the branch of mathematics that focuses on the study of matrices. It was initially a sub-branch of linear algebra, but soon grew to include subjects related to graph theory, algebra, combinatorics and statistics."
    },
    {
      "id": "253873",
      "title": "Matrix decomposition",
      "url": "https://en.wikipedia.org/wiki/Matrix_decomposition",
      "summary": "In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems."
    },
    {
      "id": "10999922",
      "title": "Mean shift",
      "url": "https://en.wikipedia.org/wiki/Mean_shift",
      "summary": "Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing."
    },
    {
      "id": "19013767",
      "title": "Medical diagnosis",
      "url": "https://en.wikipedia.org/wiki/Medical_diagnosis",
      "summary": "Medical diagnosis (abbreviated Dx, Dx, or Ds) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as a diagnosis with the medical context being implicit. The information required for a diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as medical tests, are also done during the process. Sometimes the posthumous diagnosis is considered a kind of medical diagnosis.\nDiagnosis is often challenging because many signs and symptoms are nonspecific. For example, redness of the skin (erythema), by itself, is a sign of many disorders and thus does not tell the healthcare professional what is wrong. Thus differential diagnosis, in which several possible explanations are compared and contrasted, must be performed. This involves the correlation of various pieces of information followed by the recognition and differentiation of patterns. Occasionally the process is made easy by a sign or symptom (or a group of several) that is pathognomonic.\nDiagnosis is a major component of the procedure of a doctor's visit. From the point of view of statistics, the diagnostic procedure involves classification tests.\n\n"
    },
    {
      "id": "20976642",
      "title": "Mehryar Mohri",
      "url": "https://en.wikipedia.org/wiki/Mehryar_Mohri",
      "summary": "Mehryar Mohri is a Professor and theoretical computer scientist at the Courant Institute of Mathematical Sciences. He is also a Research Director \nat Google Research where he heads the Learning Theory team."
    },
    {
      "id": "14246162",
      "title": "Memristor",
      "url": "https://en.wikipedia.org/wiki/Memristor",
      "summary": "A memristor (; a portmanteau of memory resistor) is a non-linear two-terminal electrical component relating electric charge and magnetic flux linkage. It was described and named in 1971 by Leon Chua, completing a theoretical quartet of fundamental electrical components which also comprises the resistor, capacitor and inductor.Chua and Kang later generalized the concept to memristive systems. Such a system comprises a circuit, of multiple conventional components, which mimics key properties of the ideal memristor component and is also commonly referred to as a memristor. Several such memristor system technologies have been developed, notably ReRAM.\nThe identification of memristive properties in electronic devices has attracted controversy. Experimentally, the ideal memristor has yet to be demonstrated."
    },
    {
      "id": "56906363",
      "title": "Memtransistor",
      "url": "https://en.wikipedia.org/wiki/Memtransistor",
      "summary": "The memtransistor (a blend word from Memory Transfer Resistor) is an experimental multi-terminal passive electronic component that might be used in the construction of artificial neural networks. It is a combination of the memristor and transistor technology. This technology is different from the 1T-1R approach since the devices are merged into one single entity. Multiple memristers can be embedded with a single transistor, enabling it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation."
    },
    {
      "id": "4615464",
      "title": "Meta-learning (computer science)",
      "url": "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)",
      "summary": "Meta learning\nis a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.Flexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood.\nBy using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for J\u00fcrgen Schmidhuber's early work (1987) and Yoshua Bengio et al.'s work (1991), considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain. In an open-ended hierarchical meta learning system using genetic programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc.\n\n"
    },
    {
      "id": "20455",
      "title": "Michael Jordan",
      "url": "https://en.wikipedia.org/wiki/Michael_Jordan",
      "summary": "Michael Jeffrey Jordan (born February 17, 1963), also known by his initials MJ, is an American businessman and former professional basketball player. He played fifteen seasons in the National Basketball Association (NBA) between 1984 and 2003, winning six NBA championships with the Chicago Bulls. He was integral in popularizing basketball and the NBA around the world in the 1980s and 1990s, becoming a global cultural icon. His profile on the NBA website states that \"by acclamation, Michael Jordan is the greatest basketball player of all time.\"Jordan played college basketball for three seasons under coach Dean Smith with the North Carolina Tar Heels. As a freshman, he was a member of the Tar Heels' national championship team in 1982. Jordan joined the Bulls in 1984 as the third overall draft pick and quickly emerged as a league star, entertaining crowds with his prolific scoring while gaining a reputation as one of the game's best defensive players. His leaping ability, demonstrated by performing slam dunks from the free-throw line in Slam Dunk Contests, earned him the nicknames \"Air Jordan\" and \"His Airness\". Jordan won his first NBA title with the Bulls in 1991 and followed that achievement with titles in 1992 and 1993, securing a three-peat. Jordan abruptly retired from basketball before the 1993\u201394 NBA season to play Minor League Baseball but returned to the Bulls in March 1995 and led them to three more championships in 1996, 1997, and 1998, as well as a then-record 72 regular season wins in the 1995\u201396 NBA season. He retired for the second time in January 1999, returning for two more NBA seasons from 2001 to 2003 as a member of the Washington Wizards. During his professional career, he was selected to play for the United States national team, winning four gold medals\u2014at the 1983 Pan American Games, 1984 Summer Olympics, 1992 Tournament of the Americas and 1992 Summer Olympics\u2014while also being undefeated.Jordan's individual accolades and accomplishments include six NBA Finals Most Valuable Player (MVP) awards, ten NBA scoring titles (both all-time records), five NBA MVP awards, ten All-NBA First Team designations, nine All-Defensive First Team honors, fourteen NBA All-Star Game selections, three NBA All-Star Game MVP awards, three NBA steals titles, and the 1988 NBA Defensive Player of the Year Award. He holds the NBA records for career regular season scoring average (30.1 points per game) and career playoff scoring average (33.4 points per game). In 1999, he was named the 20th century's greatest North American athlete by ESPN and was second to Babe Ruth on the Associated Press' list of athletes of the century. Jordan was twice inducted into the Naismith Memorial Basketball Hall of Fame, once in 2009 for his individual career, and again in 2010 as part of the 1992 United States men's Olympic basketball team (\"The Dream Team\"). He became a member of the United States Olympic Hall of Fame in 2009, a member of the North Carolina Sports Hall of Fame in 2010, and an individual member of the FIBA Hall of Fame in 2015 and a \"Dream Team\" member in 2017. Jordan was named to the NBA 50th Anniversary Team in 1996 and to the NBA 75th Anniversary Team in 2021.One of the most effectively marketed athletes of his generation, Jordan made many product endorsements. He fueled the success of Nike's Air Jordan sneakers, which were introduced in 1984 and remain popular. He starred as himself in the live-action/animation hybrid film Space Jam (1996) and was the central focus of the Emmy-winning documentary series The Last Dance (2020). He became part-owner and head of basketball operations for the Charlotte Hornets (then named the Bobcats) in 2006 and bought a controlling interest in 2010, before selling his majority stake in 2023. He is also the owner of 23XI Racing in the NASCAR Cup Series. In 2016, he became the first billionaire player in NBA history. That year, President Barack Obama awarded him the Presidential Medal of Freedom. As of 2023, his net worth is estimated at $3 billion by Forbes."
    },
    {
      "id": "69902039",
      "title": "Michal Aharon",
      "url": "https://en.wikipedia.org/wiki/Michal_Aharon",
      "summary": "Michal Aharon is an Israeli computer scientist known for her research on sparse dictionary learning, image denoising, and the K-SVD algorithm in machine learning. She is a researcher on advertisement ranking for Yahoo! in Haifa."
    },
    {
      "id": "21017",
      "title": "Microcontroller",
      "url": "https://en.wikipedia.org/wiki/Microcontroller",
      "summary": "A microcontroller (MC, UC, or \u03bcC) or microcontroller unit (MCU) is a small computer on a single integrated circuit. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips.\nIn modern terminology, a microcontroller is similar to, but less sophisticated than, a system on a chip (SoC). An SoC may include a microcontroller as one of its components, but usually integrates it with advanced peripherals like a graphics processing unit (GPU), a Wi-Fi module, or one or more coprocessors.\nMicrocontrollers are used in automatically controlled products and devices, such as automobile engine control systems, implantable medical devices, remote controls, office machines, appliances, power tools, toys and other embedded systems. By reducing the size and cost compared to a design that uses a separate microprocessor, memory, and input/output devices, microcontrollers make it economical to digitally control even more devices and processes.  Mixed signal microcontrollers are common, integrating analog components needed to control non-digital electronic systems. In the context of the internet of things, microcontrollers are an economical and popular means of data collection, sensing and actuating the physical world as edge devices.\nSome microcontrollers may use four-bit words and operate at frequencies as low as 4 kHz for low power consumption (single-digit milliwatts or microwatts). They generally have the ability to retain functionality while waiting for an event such as a button press or other interrupt; power consumption while sleeping (CPU clock and most peripherals off) may be just nanowatts, making many of them well suited for long lasting battery applications. Other microcontrollers may serve performance-critical roles, where they may need to act more like a digital signal processor (DSP), with higher clock speeds and power consumption.\n\n"
    },
    {
      "id": "49830146",
      "title": "Microsoft Cognitive Toolkit",
      "url": "https://en.wikipedia.org/wiki/Microsoft_Cognitive_Toolkit",
      "summary": "Microsoft Cognitive Toolkit, previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.\n\n"
    },
    {
      "id": "61289371",
      "title": "Mila (research institute)",
      "url": "https://en.wikipedia.org/wiki/Mila_(research_institute)",
      "summary": "Mila - Quebec AI Institute (originally Montreal Institute for Learning Algorithms) is a research institute in Montreal, Quebec, focusing mainly on machine learning research. Approximately 1000 students and researchers and 100 faculty members, were part of Mila in 2022. Along with Alberta's  Amii and Toronto's  Vector Institute, Mila is part of the Pan-Canadian Artificial Intelligence Strategy."
    },
    {
      "id": "877295",
      "title": "Mixed reality",
      "url": "https://en.wikipedia.org/wiki/Mixed_reality",
      "summary": "Mixed reality (MR) is a term used to describe the merging of a real-world environment and a computer-generated one. Physical and virtual objects may co-exist in mixed reality environments and interact in real time.\nMixed reality that incorporates haptics has sometimes been referred to as Visuo-haptic mixed reality.In a physics context, the term \"interreality system\" refers to a virtual reality system coupled with its real-world counterpart. A 2007 paper describes an interreality system comprising a real physical pendulum coupled to a pendulum that only exists in virtual reality. This system has two stable states of motion: a \"Dual Reality\" state in which the motion of the two pendula are uncorrelated, and a \"Mixed Reality\" state in which the pendula exhibit stable phase-locked motion, which is highly correlated. The use of the terms \"mixed reality\" and \"interreality\" is clearly defined in the context of physics and may be slightly different in other fields, however, it is generally seen as, \"bridging the physical and virtual world\"."
    },
    {
      "id": "41585002",
      "title": "Mlpack",
      "url": "https://en.wikipedia.org/wiki/Mlpack",
      "summary": "mlpack is a machine learning software library for C++, built on top of the Armadillo library and the ensmallen numerical optimization library. mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users. Its intended target users are scientists and engineers.\nIt is open-source software distributed under the BSD license, making it useful for developing both open source and proprietary software. Releases 1.0.11 and before were released under the LGPL license.  The project is supported by the Georgia Institute of Technology and contributions from around the world."
    },
    {
      "id": "1773278",
      "title": "Model of computation",
      "url": "https://en.wikipedia.org/wiki/Model_of_computation",
      "summary": "In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.\n\n"
    },
    {
      "id": "346781",
      "title": "Modeling language",
      "url": "https://en.wikipedia.org/wiki/Modeling_language",
      "summary": "A modeling language is any artificial language that can be used to express data, information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language.\n\n"
    },
    {
      "id": "62285602",
      "title": "Multi-agent reinforcement learning",
      "url": "https://en.wikipedia.org/wiki/Multi-agent_reinforcement_learning",
      "summary": "Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple learning agents that coexist in a shared environment. Each agent is motivated by its own rewards, and does actions to advance its own interests; in some environments these interests are opposed to the interests of other agents, resulting in complex group dynamics.\nMulti-agent reinforcement learning is closely related to game theory and especially repeated games, as well as multi-agent systems. Its study combines the pursuit of finding ideal algorithms that maximize rewards with a more sociological set of concepts. While research in single-agent reinforcement learning is concerned with finding the algorithm that gets the biggest number of points for one agent, research in multi-agent reinforcement learning evaluates and quantifies social metrics, such as cooperation, reciprocity, equity, social influence, language and discrimination."
    },
    {
      "id": "938833",
      "title": "Multi-agent system",
      "url": "https://en.wikipedia.org/wiki/Multi-agent_system",
      "summary": "A multi-agent system (MAS or \"self-organized system\") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be \"intelligent\") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, target surveillance and social structure modelling.\n\n"
    },
    {
      "id": "938663",
      "title": "Multi-task learning",
      "url": "https://en.wikipedia.org/wiki/Multi-task_learning",
      "summary": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called \"hints\".\nIn a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.\nIn the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.\n\n"
    },
    {
      "id": "42171777",
      "title": "Multimedia database",
      "url": "https://en.wikipedia.org/wiki/Multimedia_database",
      "summary": "A Multimedia database (MMDB) is a collection of related for multimedia data. The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.\nA Multimedia Database Management System (MMDBMS) is a framework that manages different types of data potentially represented in a wide diversity of formats on a wide array of media sources. It provides support for multimedia data types, and facilitate for creation, storage, access, query and control of a multimedia database.\n\n"
    },
    {
      "id": "46975535",
      "title": "Multimodal learning",
      "url": "https://en.wikipedia.org/wiki/Multimodal_learning",
      "summary": "Multimodal learning, in the context of machine learning, is a type of deep learning using a combination of various modalities of data, often arising in real-world applications. An example of multi-modal data is data that combines text (typically represented as feature vector) with imaging data consisting of pixel intensities and annotation tags. As these modalities have fundamentally different statistical properties, combining them is non-trivial, which is why specialized modelling strategies and algorithms are required. The model is then trained to able to understand and work with multiple forms of data."
    },
    {
      "id": "64020",
      "title": "Multiprocessing",
      "url": "https://en.wikipedia.org/wiki/Multiprocessing",
      "summary": "Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system. The term also refers to the ability of a system to support more than one processor or the ability to allocate tasks between them. There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined (multiple cores on one die, multiple dies in one package, multiple packages in one system unit, etc.).\nAccording to some on-line dictionaries, a multiprocessor is a computer system having two or more processing units (multiple processors) each sharing main memory and peripherals, in order to simultaneously process programs. A 2009 textbook defined multiprocessor system similarly, but noting that the processors may share \"some or all of the system\u2019s memory and I/O facilities\"; it also gave tightly coupled system as a synonymous term.At the operating system level, multiprocessing is sometimes used to refer to the execution of multiple concurrent processes in a system, with each process running on a separate CPU or core, as opposed to a single process at any one instant. When used with this definition, multiprocessing is sometimes contrasted with multitasking, which may use just a single processor but switch it in time slices between tasks (i.e. a time-sharing system). Multiprocessing however means true parallel execution of multiple processes using more than one processor. Multiprocessing doesn't necessarily mean that a single process or task uses more than one processor simultaneously; the term parallel processing is generally used to denote that scenario. Other authors prefer to refer to the operating system techniques as multiprogramming and reserve the term multiprocessing for the hardware aspect of having more than one processor. The remainder of this article discusses multiprocessing only in this hardware sense.\nIn Flynn's taxonomy, multiprocessors as defined above are MIMD machines. As the term \"multiprocessor\" normally refers to tightly coupled systems in which all processors share memory, multiprocessors are not the entire class of MIMD machines, which also contains message passing multicomputer systems.\n\n"
    },
    {
      "id": "10520679",
      "title": "Multithreading (computer architecture)",
      "url": "https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)",
      "summary": "In computer architecture, multithreading is the ability of a central processing unit (CPU) (or a single core in a multi-core processor) to provide multiple threads of execution concurrently, supported by the operating system. This approach differs from multiprocessing. In a multithreaded application, the threads share the resources of a single or multiple cores, which include the computing units, the CPU caches, and the translation lookaside buffer (TLB).\nWhere multiprocessing systems include multiple complete processing units in one or more cores, multithreading aims to increase utilization of a single core by using thread-level parallelism, as well as instruction-level parallelism. As the two techniques are complementary, they are combined in nearly all modern systems architectures with multiple multithreading CPUs and with CPUs with multiple multithreading cores.\n\n"
    },
    {
      "id": "50347",
      "title": "Multivariate normal distribution",
      "url": "https://en.wikipedia.org/wiki/Multivariate_normal_distribution",
      "summary": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.  One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value."
    },
    {
      "id": "21659435",
      "title": "Music and artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Music_and_artificial_intelligence",
      "summary": "Artificial intelligence and music (AIM) is a common subject in the International Computer Music Conference, the Computing Society Conference and the International Joint Conference on Artificial Intelligence. The first International Computer Music Conference (ICMC) was held in 1974 at Michigan State University. Current research includes the application of AI in music composition, performance, theory and digital sound processing.\nA key part of this field is the development of music software programs which use AI to generate music. As with applications in other fields, AI in music also simulates mental tasks. A prominent feature is the capability of an AI algorithm to learn based on past data, such as in computer accompaniment technology, wherein the AI is capable of listening to a human performer and performing accompaniment. Artificial intelligence also drives interactive composition technology, wherein a computer composes music in response to a live performance. There are other AI applications in music that cover not only music composition, production, and performance but also how music is marketed and consumed. Several music player programs have also been developed to use voice recognition and natural language processing technology for music voice control.\n\n"
    },
    {
      "id": "555213",
      "title": "Mutation (genetic algorithm)",
      "url": "https://en.wikipedia.org/wiki/Mutation_(genetic_algorithm)",
      "summary": "Mutation is a genetic operator used to maintain genetic diversity of the chromosomes of a population of a genetic or, more generally, an evolutionary algorithm (EA). It is analogous to biological mutation.\nThe classic example of a mutation operator of a binary coded genetic algorithm (GA) involves a probability that an arbitrary bit in a genetic sequence will be flipped from its original state. A common method of implementing the mutation operator involves generating a random variable for each bit in a sequence. This random variable tells whether or not a particular bit will be flipped. This mutation procedure, based on the biological point mutation, is called single point mutation. Other types of mutation operators are commonly used for representations other than binary, such as floating-point encodings or representations for combinatorial problems.\nThe purpose of mutation in EAs is to introduce diversity into the sampled population. Mutation operators are used in an attempt to avoid local minima by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping convergence to the global optimum. This reasoning also leads most EAs to avoid only taking the fittest of the population in generating the next generation, but rather selecting a random (or semi-random) set with a weighting toward those that are fitter.The following requirements apply to all mutation operators used in an EA:\nevery point in the search space must be reachable by one or more mutations.\nthere must be no preference for parts or directions in the search space (no drift).\nsmall mutations should be more probable than large ones.For different genome types, different mutation types are suitable. Some mutations are Gaussian, Uniform, Zigzag, Scramble, Insertion, Inversion, Swap, and so on. An overview and more operators than those presented below can be found in the introductory book by Eiben and Smith or in.\n\n"
    },
    {
      "id": "87339",
      "title": "Naive Bayes classifier",
      "url": "https://en.wikipedia.org/wiki/Naive_Bayes_classifier",
      "summary": "In statistics, naive Bayes classifiers are a family of linear \"probabilistic classifiers\" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,:\u200a718\u200a which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\nIn the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method."
    },
    {
      "id": "55825672",
      "title": "Naomi Altman",
      "url": "https://en.wikipedia.org/wiki/Naomi_Altman",
      "summary": "Naomi Altman is a statistician known for her work on kernel smoothing[KS] and kernel regression,[KR]\nand interested in applications of statistics to gene expression and genomics. She is a professor of statistics at Pennsylvania State University, and a regular columnist for the \"Points of Significance\" column in Nature Methods."
    },
    {
      "id": "98778",
      "title": "Natural-language understanding",
      "url": "https://en.wikipedia.org/wiki/Natural-language_understanding",
      "summary": "Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subset  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.There is considerable commercial interest in the field because of its application to automated reasoning, machine translation, question answering, news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.\n\n"
    },
    {
      "id": "21652",
      "title": "Natural language processing",
      "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
      "summary": "Natural language processing (NLP) is an interdisciplinary subfield of computer science and linguistics. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic (i.e. statistical and, most recently, neural network-based) machine learning approaches. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\nChallenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n\n"
    },
    {
      "id": "59973182",
      "title": "Nature Machine Intelligence",
      "url": "https://en.wikipedia.org/wiki/Nature_Machine_Intelligence",
      "summary": "Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. The editor-in-chief is Liesbeth Venema.\n\n"
    },
    {
      "id": "23348504",
      "title": "Nature Methods",
      "url": "https://en.wikipedia.org/wiki/Nature_Methods",
      "summary": "Nature Methods is a monthly peer-reviewed scientific journal covering new scientific techniques. It was established in 2004 and is published by Springer Nature under the Nature Portfolio. Like other Nature journals, there is no external editorial board and editorial decisions are made by an in-house team, although peer review by external experts forms a part of the review process. The editor-in-chief is Allison Doerr.Every year, the journal highlights a field, approach, or technique that has enabled recent major advances in life sciences research as the \"Method of the Year\".\nAccording to the Journal Citation Reports, the journal had a 2021 impact factor of 47.990, ranking it first in the category \"Biochemical Research Methods\"."
    },
    {
      "id": "21120",
      "title": "Neuron",
      "url": "https://en.wikipedia.org/wiki/Neuron",
      "summary": "Within a nervous system, a neuron, neurone, or nerve cell is an electrically excitable cell that fires electric signals called action potentials across a neural network. Neurons communicate with other cells via synapses, which are specialized connections that commonly use minute amounts of chemical neurotransmitters to pass the electric signal from the presynaptic neuron to the target cell through the synaptic gap. \nThe neuron is the main component of nervous tissue in all animals except sponges and placozoa. Non-animals like plants and fungi do not have nerve cells. Molecular evidence suggests that the ability to generate electric signals first appeared in evolution some 700 to 800 million years ago, during the Tonian period. Predecessors of neurons were the peptidergic secretory cells. They eventually gained new gene modules which enabled cells to create post-synaptic scaffolds and ion channels that generate fast electrical signals. The ability to generate electric signals was a key innovation in the evolution of the nervous system.Neurons are typically classified into three types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to control everything from muscle contractions to glandular output. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord. When multiple neurons are functionally connected together, they form what is called a neural circuit.\nNeurons are special cells which are made up of some structures that are common to all other eukaryotic cells such as the cell body (soma), a nucleus, smooth and rough endoplasmic reticulum, Golgi apparatus, mitochondria, and other cellular components. Additionally, neurons have other unique structures  such as dendrites, and a single axon. The soma is a compact structure, and the axon and dendrites are filaments extruding from the soma. Dendrites typically branch profusely and extend a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit a signal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite or an axon, particularly when the cell is undifferentiated.\nMost neurons receive signals via the dendrites and soma and send out signals down the axon. At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another. However, synapses can connect an axon to another axon or a dendrite to another dendrite.\nThe signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.\nIn most cases, neurons are generated by neural stem cells during brain development and childhood. Neurogenesis largely ceases during adulthood in most areas of the brain.\n\n"
    },
    {
      "id": "9399111",
      "title": "Netflix Prize",
      "url": "https://en.wikipedia.org/wiki/Netflix_Prize",
      "summary": "The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users being identified except by numbers assigned for the contest.\nThe competition was held by Netflix, a video streaming service, and was open to anyone who is neither connected with Netflix (current and former employees, agents, close relatives of Netflix employees, etc.) nor a resident of certain blocked countries (such as Cuba or North Korea). On September 21, 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team which bested Netflix's own algorithm for predicting ratings by 10.06%.\n\n"
    },
    {
      "id": "41406",
      "title": "Network architecture",
      "url": "https://en.wikipedia.org/wiki/Network_architecture",
      "summary": "Network architecture is the design of a computer network. It is a framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used.\nIn telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated.\nThe network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links."
    },
    {
      "id": "1522954",
      "title": "Network performance",
      "url": "https://en.wikipedia.org/wiki/Network_performance",
      "summary": "Network performance refers to measures of service quality of a network as seen by the customer.\nThere are many different ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled and simulated instead of measured; one example of this is using state transition diagrams to model queuing performance or to use a Network Simulator.\n\n"
    },
    {
      "id": "28030850",
      "title": "Communication protocol",
      "url": "https://en.wikipedia.org/wiki/Communication_protocol",
      "summary": "A communication protocol is a system of rules that allows two or more entities of a communications system to transmit information via any variation of a physical quantity. The protocol defines the rules, syntax, semantics, and synchronization of communication and possible error recovery methods. Protocols may be implemented by hardware, software, or a combination of both.Communicating systems use well-defined formats for exchanging various messages. Each message has an exact meaning intended to elicit a response from a range of possible responses pre-determined for that particular situation. The specified behavior is typically independent of how it is to be implemented. Communication protocols have to be agreed upon by the parties involved. To reach an agreement, a protocol may be developed into a technical standard. A programming language describes the same for computations, so there is a close analogy between protocols and programming languages: protocols are to communication what programming languages are to computations. An alternate formulation states that protocols are to communication what algorithms are to computation.Multiple protocols often describe different aspects of a single communication. A group of protocols designed to work together is known as a protocol suite; when implemented in software they are a protocol stack.\nInternet communication protocols are published by the Internet Engineering Task Force (IETF). The IEEE (Institute of Electrical and Electronics Engineers) handles wired and wireless networking and the International Organization for Standardization (ISO) handles other types. The ITU-T handles telecommunications protocols and formats for the public switched telephone network (PSTN). As the PSTN and Internet converge, the standards are also being driven towards convergence."
    },
    {
      "id": "38050347",
      "title": "Network scheduler",
      "url": "https://en.wikipedia.org/wiki/Network_scheduler",
      "summary": "A network scheduler, also called packet scheduler, queueing discipline (qdisc) or queueing algorithm, is an arbiter on a node in a packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the protocol stack and network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.\nThe network scheduler logic decides which network packet to forward next. The network scheduler is associated with a queuing system, storing the network packets temporarily until they are transmitted. Systems may have a single or multiple queues in which case each may hold the packets of one flow, classification, or priority.\nIn some cases it may not be possible to schedule all transmissions within the constraints of the system. In these cases the network scheduler is responsible for deciding which traffic to forward and what gets dropped.\n\n"
    },
    {
      "id": "592687",
      "title": "Network security",
      "url": "https://en.wikipedia.org/wiki/Network_security",
      "summary": "Network security consists of the policies, processes and practices adopted to prevent, detect and monitor unauthorized access, misuse, modification, or denial of a computer network and network-accessible resources. Network security involves the authorization of access to data in a network, which is controlled by the network administrator. Users choose or are assigned an ID and password or other authenticating information that allows them access to information and programs within their authority. Network security covers a variety of computer networks, both public and private, that are used in everyday jobs: conducting transactions and communications among businesses, government agencies and individuals. Networks can be private, such as within a company, and others which might be open to public access. Network security is involved in organizations, enterprises, and other types of institutions. It does as its title explains: it secures the network, as well as protecting and overseeing operations being done. The most common and simple way of protecting a network resource is by assigning it a unique name and a corresponding password.\n\n"
    },
    {
      "id": "788676",
      "title": "Network service",
      "url": "https://en.wikipedia.org/wiki/Network_service",
      "summary": "In computer networking, a network service is an application running at the network application layer and above, that provides data storage, manipulation, presentation, communication or other capability which is often implemented using a client\u2013server or peer-to-peer architecture based on application layer network protocols.Each service is usually provided by a server component running on one or more computers (often a dedicated server computer offering multiple services) and accessed via a network by client components running on other devices. However, the client and server components can both be run on the same machine.\nClients and servers will often have a user interface, and sometimes other hardware associated with it.\n\n"
    },
    {
      "id": "1446517",
      "title": "Network simulation",
      "url": "https://en.wikipedia.org/wiki/Network_simulation",
      "summary": "In computer network research, network simulation is a technique whereby a software program replicates the behavior of a real network. This is achieved by calculating the interactions between the different network entities such as routers, switches, nodes, access points, links, etc. Most simulators use discrete event simulation in which the modeling of systems in which state variables change at discrete points in time.  The behavior of the network and the various applications and services it supports can then be observed in a test lab; various attributes of the environment can also be modified in a controlled manner to assess how the network/protocols would behave under different conditions."
    },
    {
      "id": "1699254",
      "title": "Networking hardware",
      "url": "https://en.wikipedia.org/wiki/Networking_hardware",
      "summary": "Networking hardware, also known as network equipment or computer networking devices, are electronic devices that are required for communication and interaction between devices on a computer network. Specifically, they mediate data transmission in a computer network. Units which are the last receiver or generate data are called hosts, end systems or data terminal equipment.\n\n"
    },
    {
      "id": "17422480",
      "title": "Neural Computation (journal)",
      "url": "https://en.wikipedia.org/wiki/Neural_Computation_(journal)",
      "summary": "Neural Computation is a monthly peer-reviewed scientific journal covering all aspects of neural computation, including modeling the brain and the design and construction of neurally-inspired information processing systems. It was established in 1989 and is published by MIT Press. The editor-in-chief is Terrence J. Sejnowski (Salk Institute for Biological Studies).According to the Journal Citation Reports, the journal has a 2021 impact factor of 3.278.\n\n"
    },
    {
      "id": "47012074",
      "title": "Neural Designer",
      "url": "https://en.wikipedia.org/wiki/Neural_Designer",
      "summary": "Neural Designer is a software tool for machine learning based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.\nIn 2015, Neural Designer was chosen by the European Commission, within the Horizon 2020 program, as a disruptive technology in the ICT field."
    },
    {
      "id": "50568884",
      "title": "Neural Turing machine",
      "url": "https://en.wikipedia.org/wiki/Neural_Turing_machine",
      "summary": "A neural Turing machine (NTM) is a recurrent neural network model of a Turing machine. The approach was published by Alex Graves et al. in 2014. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. \nAn NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.The authors of the original NTM paper did not publish their source code. The first stable open-source implementation was published in 2018 at the 27th International Conference on Artificial Neural Networks, receiving a best-paper award. Other open source implementations of NTMs exist but as of 2018 they are not sufficiently stable for production use. The developers either report that the gradients of their implementation sometimes become NaN during training for unknown reasons and cause training to fail; report slow convergence; or do not report the speed of learning of their implementation.Differentiable neural computers are an outgrowth of Neural Turing machines, with attention mechanisms that control where the memory is active, and improve performance.\n\n"
    },
    {
      "id": "47961606",
      "title": "Neural machine translation",
      "url": "https://en.wikipedia.org/wiki/Neural_machine_translation",
      "summary": "Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.\nIt is the dominant approach today:\u200a293\u200a:\u200a1\u200a and can produce translations that rival human translations when translating between high-resource languages under specific conditions. However, there still remain challenges, especially with languages where less high-quality data is available,:\u200a293\u200a and with domain shift between the data a system was trained on and the texts it is supposed to translate.:\u200a293\u200a NMT systems also tend to produce fairly literal translations.\n\n"
    },
    {
      "id": "4390806",
      "title": "NeuroSolutions",
      "url": "https://en.wikipedia.org/wiki/NeuroSolutions",
      "summary": "NeuroSolutions is a neural network development environment developed by NeuroDimension. It combines a modular, icon-based (component-based) network design interface with an implementation of advanced learning procedures, such as conjugate gradients, the Levenberg-Marquardt algorithm, and backpropagation through time. The software is used to design, train and deploy artificial neural network (supervised learning and unsupervised learning) models to perform a wide variety of tasks such as data mining, classification, function approximation, multivariate regression and time-series prediction.\n\n"
    },
    {
      "id": "453086",
      "title": "Neuromorphic engineering",
      "url": "https://en.wikipedia.org/wiki/Neuromorphic_engineering",
      "summary": "Neuromorphic computing is an approach to computing that is inspired by the structure and function of the human brain. A neuromorphic computer/chip is any device that uses physical artificial neurons to do computations. In recent times, the term neuromorphic has been used to describe analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration). The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors, spintronic memories, threshold switches, transistors, among others. Training software-based neuromorphic systems of spiking neural networks can be achieved using error backpropagation, e.g., using Python based frameworks such as snnTorch, or using canonical learning rules from the biological learning literature, e.g., using BindsNet.A key aspect of neuromorphic engineering is understanding how the morphology of individual neurons, circuits, applications, and overall architectures creates desirable computations, affects how information is represented, influences robustness to damage, incorporates learning and development, adapts to local change (plasticity), and facilitates evolutionary change.\nNeuromorphic engineering is an interdisciplinary subject that takes inspiration from biology, physics, mathematics, computer science, and electronic engineering to design artificial neural systems, such as vision systems, head-eye systems, auditory processors, and autonomous robots, whose physical architecture and design principles are based on those of biological nervous systems. One of the first applications for neuromorphic engineering was proposed by Carver Mead in the late 1980s.\n\n"
    },
    {
      "id": "486492",
      "title": "The New England Journal of Medicine",
      "url": "https://en.wikipedia.org/wiki/The_New_England_Journal_of_Medicine",
      "summary": "The New England Journal of Medicine (NEJM) is a weekly medical journal published by the Massachusetts Medical Society. It is among the most prestigious peer-reviewed medical journals as well as the oldest continuously published one.\n\n"
    },
    {
      "id": "8016113",
      "title": "Nils John Nilsson",
      "url": "https://en.wikipedia.org/wiki/Nils_John_Nilsson",
      "summary": "Nils John Nilsson (February 6, 1933 \u2013 April 23, 2019) was an American computer scientist. He was one of the founding researchers in the discipline of artificial intelligence. He was the first Kumagai Professor of Engineering in computer science at Stanford University from 1991 until his retirement. He is particularly known for his contributions to search, planning, knowledge representation, and robotics.\n\n"
    },
    {
      "id": "3681279",
      "title": "Non-negative matrix factorization",
      "url": "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization",
      "summary": "Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\nNMF finds applications in such fields as astronomy, computer vision, document clustering, missing data imputation, chemometrics, audio signal processing, recommender systems, and bioinformatics."
    },
    {
      "id": "21506",
      "title": "Numerical analysis",
      "url": "https://en.wikipedia.org/wiki/Numerical_analysis",
      "summary": "Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). It is the study of numerical methods that attempt to find approximate solutions of problems rather than the exact ones. Numerical analysis finds application in all fields of engineering and the physical sciences, and in the 21st century also the life and social sciences, medicine, business and even the arts. Current growth in computing power has enabled the use of more complex numerical analysis, providing detailed and realistic mathematical models in science and engineering. Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology.\nBefore modern computers, numerical methods often relied on hand interpolation formulas, using data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas continue to be used in software algorithms.The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.\nNumerical analysis continues this long tradition: rather than giving exact symbolic answers translated into digits and applicable only to real-world measurements, approximate solutions within specified error bounds are used.\n\n"
    },
    {
      "id": "22509799",
      "title": "OPTICS algorithm",
      "url": "https://en.wikipedia.org/wiki/OPTICS_algorithm",
      "summary": "Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and J\u00f6rg Sander.\nIts basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram."
    },
    {
      "id": "44577560",
      "title": "Occam learning",
      "url": "https://en.wikipedia.org/wiki/Occam_learning",
      "summary": "In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\nOccam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.\n\n"
    },
    {
      "id": "1555671",
      "title": "Online advertising",
      "url": "https://en.wikipedia.org/wiki/Online_advertising",
      "summary": "Online advertising, also known as online marketing, Internet advertising, digital advertising or web advertising, is a form of marketing and advertising that uses the Internet to promote products and services to audiences and platform users. Online advertising includes email marketing, search engine marketing (SEM), social media marketing, many types of display advertising (including web banner advertising), and mobile advertising. Advertisements are increasingly being delivered via automated software systems operating across multiple websites, media services and platforms, known as programmatic advertising.Like other advertising media, online advertising frequently involves a publisher, who integrates advertisements into its online content, and an advertiser, who provides the advertisements to be displayed on the publisher's content. Other potential participants include advertising agencies that help generate and place the ad copy, an ad server which technologically delivers the ad and tracks statistics, and advertising affiliates who do independent promotional work for the advertiser.\nIn 2016, Internet advertising revenues in the United States surpassed those of cable television and broadcast television.:\u200a14\u200a In 2017, Internet advertising revenues in the United States totaled $83.0 billion, a 14% increase over the $72.50 billion in revenues in 2016. And research estimates for 2019's online advertising spend put it at $125.2 billion in the United States, some $54.8 billion higher than the spend on television ($70.4 billion).Many common online advertising practices are controversial and, as a result, have become increasingly subject to regulation. Many internet users also find online advertising disruptive and have increasingly turned to ad blocking for a variety of reasons. Online ad revenues also may not adequately replace other publishers' revenue streams. Declining ad revenue has led some publishers to place their content behind paywalls."
    },
    {
      "id": "11053817",
      "title": "Ontology learning",
      "url": "https://en.wikipedia.org/wiki/Ontology_learning",
      "summary": "Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\nTypically, the process starts by extracting terms and concepts or noun phrases from plain text using linguistic processors such as part-of-speech tagging and phrase chunking. Then statistical \nor symbolic\ntechniques are used to extract relation signatures, often based on pattern-based or definition-based hypernym extraction techniques."
    },
    {
      "id": "277663",
      "title": "Open-source software",
      "url": "https://en.wikipedia.org/wiki/Open-source_software",
      "summary": "Open-source software (OSS) is computer software that is released under a license in which the copyright holder grants users the rights to use, study, change, and distribute the software and its source code to anyone and for any purpose. Open-source software may be developed in a collaborative, public manner. Open-source software is a prominent example of open collaboration, meaning any capable user is able to participate online in development, making the number of possible contributors indefinite. The ability to examine the code facilitates public trust in the software.Open-source software development can bring in diverse perspectives beyond those of a single company. A 2008 report by the Standish Group stated that adoption of open-source software models has resulted in savings of about $60 billion per year for consumers.Open-source code can be used for studying and allows capable end users to adapt software to their personal needs in a similar way user scripts and custom style sheets allow for web sites, and eventually publish the modification as a fork for users with similar preferences, and directly submit possible improvements as pull requests.\n\n"
    },
    {
      "id": "48795986",
      "title": "OpenAI",
      "url": "https://en.wikipedia.org/wiki/OpenAI",
      "summary": "OpenAI is a U.S. based artificial intelligence (AI) research organization founded in December 2015, researching artificial intelligence with the goal of developing \"safe and beneficial\" artificial general intelligence, which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".\nAs one of the leading organizations of the AI Spring, it has developed several large language models, advanced image generation models, and previously, released open-source models. Its release of ChatGPT has been credited with starting the artificial intelligence spring.The organization consists of the non-profit OpenAI, Inc. registered in Delaware and its for-profit subsidiary OpenAI Global, LLC. It was founded by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk serving as the initial board members. Microsoft provided OpenAI Global LLC with a $1 billion investment in 2019 and a $10 billion investment in 2023, with a significant portion of the investment in the form of compute resources on Microsoft's Azure cloud service.On November 17, 2023, the board removed Altman as CEO, while Brockman was removed as chairman and then resigned as president. Four days later, both returned after negotiations with the board, and most of the board members resigned. The new initial board included former Salesforce co-CEO Bret Taylor as chairman. It was also announced that Microsoft will have a non-voting board seat.\n\n"
    },
    {
      "id": "58714104",
      "title": "OpenAI Five",
      "url": "https://en.wikipedia.org/wiki/OpenAI_Five",
      "summary": "OpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\nBy choosing a game as complex as Dota 2 to study machine learning, OpenAI thought they could more accurately capture the unpredictability and continuity seen in the real world, thus constructing more general problem-solving systems. The algorithms and code used by OpenAI Five were eventually borrowed by another neural network in development by the company, one which controlled a physical robotic hand. OpenAI Five has been compared to other similar cases of artificial intelligence (AI) playing against and defeating humans, such as AlphaStar in the video game StarCraft II, AlphaGo in the board game Go, Deep Blue in chess, and Watson on the television game show Jeopardy!.\n\n"
    },
    {
      "id": "42129549",
      "title": "OpenNN",
      "url": "https://en.wikipedia.org/wiki/OpenNN",
      "summary": "OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open-source, licensed under the GNU Lesser General Public License.\n\n"
    },
    {
      "id": "22194",
      "title": "Operating system",
      "url": "https://en.wikipedia.org/wiki/Operating_system",
      "summary": "An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.\nTime-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, peripherals, and other resources.\nFor hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer \u2013 from cellular phones and video game consoles to web servers and supercomputers.\nIn the personal computer market, as of September 2023, Microsoft Windows holds a dominant market share of around 68%. macOS by Apple Inc. is in second place (20%), and the varieties of Linux, including ChromeOS, are collectively in third place (7%). In the mobile sector (including smartphones and tablets), as of September 2023, Android's share is 68.92%, followed by Apple's iOS and iPadOS with 30.42%, and other operating systems with .66%. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems), such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.\nSome operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. USB stick).\n\n"
    },
    {
      "id": "65692",
      "title": "Operational definition",
      "url": "https://en.wikipedia.org/wiki/Operational_definition",
      "summary": "An operational definition specifies concrete, replicable procedures designed to represent a construct. In the words of American psychologist S.S. Stevens (1935), \"An operation is the performance which we execute in order to make known a concept.\" For example, an operational definition of \"fear\" (the construct) often includes measurable physiologic responses that occur in response to a perceived threat. Thus, \"fear\" might be operationally defined as specified changes in heart rate, galvanic skin response, pupil dilation, and blood pressure.\n\n"
    },
    {
      "id": "43476",
      "title": "Operations research",
      "url": "https://en.wikipedia.org/wiki/Operations_research",
      "summary": "Operations research (British English: operational research) (U.S. Air Force Specialty Code: Operations Analysis), often shortened to the initialism OR, is a discipline that deals with the development and application of analytical methods to improve decision-making. The term management science is occasionally used as a synonym.Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to decision-making problems. Because of its emphasis on practical applications, operations research has overlapped with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.\n\n"
    },
    {
      "id": "49091",
      "title": "Optical character recognition",
      "url": "https://en.wikipedia.org/wiki/Optical_character_recognition",
      "summary": "Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast).Widely used as a form of data entry from printed paper data records \u2013 whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printed data, or any suitable documentation \u2013 it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed online, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.\nEarly versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of accuracy for most fonts are now common, and with support for a variety of image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components."
    },
    {
      "id": "51687009",
      "title": "Oracle Cloud",
      "url": "https://en.wikipedia.org/wiki/Oracle_Cloud",
      "summary": "Oracle Cloud is a cloud computing service offered by Oracle Corporation providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers. The company allows these services to be provisioned on demand over the Internet.\nOracle Cloud provides Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS), and Data as a Service (DaaS). These services are used to build, deploy, integrate, and extend applications in the cloud. This platform supports numerous open standards (SQL, HTML5, REST, etc.), open-source applications (Kubernetes, Spark, Hadoop, Kafka, MySQL, Terraform, etc.), and a variety of programming languages, databases, tools, and frameworks including Oracle-specific, Open Source, and third-party software and systems."
    },
    {
      "id": "8740192",
      "title": "Oracle Data Mining",
      "url": "https://en.wikipedia.org/wiki/Oracle_Data_Mining",
      "summary": "Oracle Data Mining (ODM) is an option of Oracle Database Enterprise Edition. It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\n\n"
    },
    {
      "id": "1651906",
      "title": "Ordinary least squares",
      "url": "https://en.wikipedia.org/wiki/Ordinary_least_squares",
      "summary": "In statistics, ordinary least squares (OLS) is a type of linear least squares method for choosing the unknown parameters in a linear regression model (with fixed level-one effects of a linear function of a set of explanatory variables) by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the input dataset and the output of the (linear) function of the independent variable.\nGeometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface\u2014the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a simple linear regression, in which there is a single regressor on the right side of the regression equation.\nThe OLS estimator is consistent for the level-one fixed effects when the regressors are exogenous and forms perfect colinearity (rank condition), consistent for the variance estimate of the residuals when regressors have finite fourth moments  and\u2014by the Gauss\u2013Markov theorem\u2014optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. Under the additional assumption that the errors are normally distributed with zero mean, OLS is the maximum likelihood estimator that outperforms any non-linear unbiased estimator."
    },
    {
      "id": "1088262",
      "title": "Organizational behavior",
      "url": "https://en.wikipedia.org/wiki/Organizational_behavior",
      "summary": "Organizational behavior or organisational behaviour (see spelling differences) is the: \"study of human behavior in organizational settings, the interface between human behavior and the organization, and the organization itself\". Organizational behavioral research can be categorized in at least three ways:\nindividuals in organizations (micro-level)\nwork groups (meso-level)\nhow organizations behave (macro-level)Chester Barnard recognized that individuals behave differently when acting in their organizational role than when acting separately from the organization. Organizational behavior researchers study the behavior of individuals primarily in their organizational roles. One of the main goals of organizational behavior research is \"to revitalize organizational theory and develop a better conceptualization of organizational life\"."
    },
    {
      "id": "169633",
      "title": "Outline of computer science",
      "url": "https://en.wikipedia.org/wiki/Outline_of_computer_science",
      "summary": "Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.\nComputer science can be described as all of the following:\n\nAcademic discipline\nScience\nApplied science\n\n"
    },
    {
      "id": "53587467",
      "title": "Outline of machine learning",
      "url": "https://en.wikipedia.org/wiki/Outline_of_machine_learning",
      "summary": "The following outline is provided as an overview of and topical guide to machine learning:\nMachine learning \u2013 subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions."
    },
    {
      "id": "173332",
      "title": "Overfitting",
      "url": "https://en.wikipedia.org/wiki/Overfitting",
      "summary": "In mathematical modeling, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably\". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.:\u200a45\u200aUnderfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\nThe possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend. \nAs an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. \nOverfitting is directly related to approximation error of the selected function class and the optimization error of the optimization procedure. A function class that is too large, in a suitable sense, relative to the dataset size is likely to overfit. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.\nTo lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.\n\n"
    },
    {
      "id": "22989201",
      "title": "Pan-genome",
      "url": "https://en.wikipedia.org/wiki/Pan-genome",
      "summary": "In the fields of molecular biology and genetics, a pan-genome (pangenome or supragenome) is the entire set of genes from all strains within a clade. More generally, it is the union of all the genomes of a clade. The pan-genome can be broken down into a \"core pangenome\" that contains genes present in all individuals, a \"shell pangenome\" that contains genes present in two or more strains, and a \"cloud pangenome\" that contains genes only found in a single strain. Some authors also refer to the cloud genome as \"accessory genome\" containing 'dispensable' genes present in a subset of the strains and strain-specific genes. Note that the use of the term 'dispensable' has been questioned, at least in plant genomes, as accessory genes play \"an important role in genome evolution and in the complex interplay between the genome and the environment\". The field of study of pangenomes is called pangenomics.The genetic repertoire of a bacterial species is much larger than the gene content of an individual strain.\n Some species have open (or extensive) pangenomes, while others have closed pangenomes. For species with a closed pan-genome, very few genes are added per sequenced genome (after sequencing many strains), and the size of the full pangenome can be theoretically predicted. Species with an open pangenome have enough genes added per additional sequenced genome that predicting the size of the full pangenome is impossible. Population size and niche versatility have been suggested as the most influential factors in determining pan-genome size.Pangenomes were originally constructed for species of bacteria and archaea, but more recently eukaryotic pan-genomes have been developed, particularly for plant species. Plant studies have shown that pan-genome dynamics are linked to transposable elements. The significance of the pan-genome arises in an evolutionary context, especially with relevance to metagenomics, but is also used in a broader genomics context. An open access book reviewing the pangenome concept and its implications, edited by Tettelin and Medini, was published in the spring of 2020.\n\n"
    },
    {
      "id": "145162",
      "title": "Parallel computing",
      "url": "https://en.wikipedia.org/wiki/Parallel_computing",
      "summary": "Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.Parallel computing is closely related to concurrent computing\u2014they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency, and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.\nParallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.\nIn some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.\nA theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law, which states that it is limited by the fraction of time for which the parallelization can be utilised.\n\n"
    },
    {
      "id": "707411",
      "title": "Paraphrase",
      "url": "https://en.wikipedia.org/wiki/Paraphrase",
      "summary": "A paraphrase or rephrase () is the rendering of the same text in different words without losing the meaning of the text itself. More often than not, a paraphrased text can convey its meaning better than the original words. In other words, it is a copy of the text in meaning, but which is different from the original. For example, when someone tells a story they heard in their own words, they paraphrase, with the meaning being the same. The term itself is derived via Latin paraphrasis, from Ancient Greek  \u03c0\u03b1\u03c1\u03ac\u03c6\u03c1\u03b1\u03c3\u03b9\u03c2 (par\u00e1phrasis) 'additional manner of expression'. The act of paraphrasing is also called paraphrasis.\n\n"
    },
    {
      "id": "126706",
      "title": "Pattern recognition",
      "url": "https://en.wikipedia.org/wiki/Pattern_recognition",
      "summary": "Pattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent  pattern.   PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power.\nPattern recognition systems are commonly trained from labeled \"training\" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and signal processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition.\nIn machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is \"spam\"). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform \"most likely\" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors.\n\n"
    },
    {
      "id": "47790413",
      "title": "Pedro Domingos",
      "url": "https://en.wikipedia.org/wiki/Pedro_Domingos",
      "summary": "Pedro Domingos is a Professor Emeritus of computer science and engineering at the University of Washington. He is a researcher in machine learning known for Markov logic network enabling uncertain inference.\n\n"
    },
    {
      "id": "24511",
      "title": "Protein primary structure",
      "url": "https://en.wikipedia.org/wiki/Protein_primary_structure",
      "summary": "Protein primary structure is the linear sequence of amino acids in a peptide or protein. By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.\n\n"
    },
    {
      "id": "172777",
      "title": "Perceptron",
      "url": "https://en.wikipedia.org/wiki/Perceptron",
      "summary": "In machine learning, the perceptron (or McCulloch\u2013Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n\n"
    },
    {
      "id": "60644",
      "title": "Peripheral",
      "url": "https://en.wikipedia.org/wiki/Peripheral",
      "summary": "A peripheral  device, or simply peripheral, is an auxiliary hardware device used to transfer information into and out of a computer. The term peripheral device refers to all hardware components that are attached to a computer and are controlled by the computer system, but they are not the core components of the computer.\nSeveral categories of peripheral devices may be identified, based on their relationship with the computer:\n\nAn input device sends data or instructions to the computer, such as a mouse, keyboard, graphics tablet, image scanner, barcode reader, game controller, light pen, light gun, microphone and webcam;\nAn output device provides output data from the computer, such as a computer monitor, projector, printer, headphones, and computer speakers;\nAn input/output device performs both input and output functions, such as a computer data storage device (including a disk drive, solid-state drive, USB flash drive, memory card and tape drive), modem, router, gateway, network adapter and multi-function printer.Many modern electronic devices, such as Internet-enabled digital watches, video game consoles, smartphones, and tablet computers, have interfaces for use as computer peripheral \ndevices."
    },
    {
      "id": "11897790",
      "title": "Peter E. Hart",
      "url": "https://en.wikipedia.org/wiki/Peter_E._Hart",
      "summary": "Peter E. Hart (born 1941) is an American computer scientist and entrepreneur. He was chairman and president of Ricoh Innovations, which he founded in 1997. He made significant contributions in the field of computer science in a series of widely cited publications from the years 1967 to 1975 while associated with the Artificial Intelligence Center of SRI International, a laboratory where he also served as director.\n\n"
    },
    {
      "id": "566666",
      "title": "Peter Norvig",
      "url": "https://en.wikipedia.org/wiki/Peter_Norvig",
      "summary": "Peter Norvig (born December 14, 1956) is an American computer scientist and Distinguished Education Fellow at the Stanford Institute for Human-Centered AI. He previously served as a director of research and search quality at Google. Norvig is the co-author with Stuart J. Russell of the most popular textbook in the field of AI: Artificial Intelligence: A Modern Approach used in more than 1,500 universities in 135 countries."
    },
    {
      "id": "2958015",
      "title": "Philosophy of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence",
      "summary": "The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will. Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers. These factors contributed to the emergence of the philosophy of artificial intelligence. \nThe philosophy of artificial intelligence attempts to answer such questions as follows:\nCan a machine act intelligently? Can it solve any problem that a person would solve by thinking?\nAre human intelligence and machine intelligence the same?  Is the human brain essentially a computer?\nCan a machine have a mind, mental states, and consciousness in the same sense that a human being can? Can it feel how things are? (i.e does it have qualia?)Questions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\nImportant propositions in the philosophy of AI include some of the following:\n\nTuring's \"polite convention\": If a machine behaves as intelligently as a human being, then it is as intelligent as a human being.\nThe Dartmouth proposal: \"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\"\nAllen Newell and Herbert A. Simon's physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nJohn Searle's strong AI hypothesis: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\nHobbes' mechanism: \"For 'reason' ... is nothing but 'reckoning,' that is adding and subtracting, of the consequences of general names agreed upon for the 'marking' and 'signifying' of our thoughts...\"\n\n"
    },
    {
      "id": "577742",
      "title": "Photograph manipulation",
      "url": "https://en.wikipedia.org/wiki/Photograph_manipulation",
      "summary": "Photograph manipulation involves the transformation or alteration of a photograph. Some photograph manipulations are considered to be skillful artwork, while others are considered to be unethical practices, especially when used to deceive. Photographs may be manipulated for political propaganda, to improve the appearance of a subject, for entertainment, or as humor.\nDepending on the application and intent, some photograph manipulations are considered an art form because they involve creation of unique images and in some instances, signature expressions of art by photographic artists. For example, Ansel Adams used darkroom exposure techniques, burning (darkening) and dodging (lightening) a photograph. Other techniques include retouching using ink or paint, airbrushing, double exposure, piecing photos or negatives together in the darkroom, and scratching instant films. Software tools applied to digital images range from professional applications to basic imaging software for casual users. Photoshopping is a verb for photograph manipulation as a genericized trademark of Adobe Photoshop.\n\n"
    },
    {
      "id": "24833746",
      "title": "Physical neural network",
      "url": "https://en.wikipedia.org/wiki/Physical_neural_network",
      "summary": "A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse or a higher-order (dendritic) neuron model. \"Physical\" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse."
    },
    {
      "id": "42167618",
      "title": "Platt scaling",
      "url": "https://en.wikipedia.org/wiki/Platt_scaling",
      "summary": "In machine learning, Platt scaling or Platt calibration is a way of transforming the outputs of a classification model into a probability distribution over classes. The method was invented by John Platt in the context of support vector machines,\nreplacing an earlier method by Vapnik,\nbut can be applied to other classification models.\nPlatt scaling works by fitting a logistic regression model to a classifier's scores.\n\n"
    },
    {
      "id": "21893202",
      "title": "Polynomial regression",
      "url": "https://en.wikipedia.org/wiki/Polynomial_regression",
      "summary": "In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data.  For this reason, polynomial regression is considered to be a special case of multiple linear regression.\nThe explanatory (independent) variables resulting from the polynomial expansion of the \"baseline\" variables are known as higher-degree terms. Such variables are also used in classification settings.\n\n"
    },
    {
      "id": "1504425",
      "title": "Possibility theory",
      "url": "https://en.wikipedia.org/wiki/Possibility_theory",
      "summary": "Possibility theory is a mathematical theory for dealing with certain types of uncertainty and is an alternative to probability theory. It uses measures of possibility and necessity between 0 and 1, ranging from impossible to possible and unnecessary to necessary, respectively. Professor Lotfi Zadeh first introduced possibility theory in 1978 as an extension of his theory of fuzzy sets and fuzzy logic. Didier Dubois and Henri Prade further contributed to its development. Earlier, in the 1950s, economist G. L. S. Shackle proposed the min/max algebra to describe degrees of potential surprise."
    },
    {
      "id": "144068",
      "title": "Precision agriculture",
      "url": "https://en.wikipedia.org/wiki/Precision_agriculture",
      "summary": "Precision agriculture (PA) is a farming management strategy based on observing, measuring and responding to temporal and spatial variability to improve agricultural production sustainability. It is used in both crop and livestock production. Precision agriculture often employs technologies to automate agricultural operations, improving their diagnosis, decision-making or performing. The goal of precision agriculture research is to define a decision support system for whole farm management with the goal of optimizing returns on inputs while preserving resources.Among these many approaches is a phytogeomorphological approach which ties multi-year crop growth stability/characteristics to topological terrain attributes. The interest in the phytogeomorphological approach stems from the fact that the geomorphology component typically dictates the hydrology of the farm field.The practice of precision agriculture has been enabled by the advent of GPS and GNSS. The farmer's and/or researcher's ability to locate their precise position in a field allows for the creation of maps of the spatial variability of as many variables as can be measured (e.g. crop yield, terrain features/topography, organic matter content, moisture levels, nitrogen levels, pH, EC, Mg, K, and others). Similar data is collected by sensor arrays mounted on GPS-equipped combine harvesters. These arrays consist of real-time sensors that measure everything from chlorophyll levels to plant water status, along with multispectral imagery. This data is used in conjunction with satellite imagery by variable rate technology (VRT) including seeders, sprayers, etc. to optimally distribute resources. However, recent technological advances have enabled the use of real-time sensors directly in soil, which can wirelessly transmit data without the need of human presence.Precision agriculture has also been enabled by unmanned aerial vehicles that are relatively inexpensive and can be operated by novice pilots.  These agricultural drones can be equipped with multispectral or RGB cameras to capture many images of a field that can be stitched together using photogrammetric methods to create orthophotos. These multispectral images contain multiple values per pixel in addition to the traditional red, green blue values such as near infrared and red-edge spectrum values used to process and analyze vegetative indexes such as NDVI maps. These drones are capable of capturing imagery and providing additional geographical references such as elevation, which allows software to perform map algebra functions to build precise topography maps. These topographic maps can be used to correlate crop health with topography, the results of which can be used to optimize crop inputs such as water, fertilizer or chemicals such as herbicides and growth regulators through variable rate applications.\n\n"
    },
    {
      "id": "2538775",
      "title": "Predictive modelling",
      "url": "https://en.wikipedia.org/wiki/Predictive_modelling",
      "summary": "Predictive modelling uses statistics to predict outcomes. Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.In many cases, the model is chosen on the basis of detection theory to try to guess the probability of an outcome given a set amount of input data, for example given an email determining how likely that it is spam.\nModels can use one or more classifiers in trying to determine the probability of a set of data belonging to another set. For example, a model might be used to determine whether an email is spam or \"ham\" (non-spam).\nDepending on definitional boundaries, predictive modelling is synonymous with, or largely overlapping with, the field of machine learning, as it is more commonly referred to in academic or research and development contexts. When deployed commercially, predictive modelling is often referred to as predictive analytics.\nPredictive modelling is often contrasted with causal modelling/analysis.  In the former, one may be entirely satisfied to make use of indicators of, or proxies for, the outcome of interest. In the latter, one seeks to determine true cause-and-effect relationships. This distinction has given rise to a burgeoning literature in the fields of research methods and statistics and to the common statement that \"correlation does not imply causation\".\n\n"
    },
    {
      "id": "239887",
      "title": "Pricing",
      "url": "https://en.wikipedia.org/wiki/Pricing",
      "summary": "Pricing is the process whereby a business sets the price at which it will sell its products and services, and may be part of the business's marketing plan. In setting prices, the business will take into account the price at which it could acquire the goods, the manufacturing cost, the marketplace, competition, market condition, brand, and quality of product.\nPricing is a fundamental aspect of product management and is one of the four Ps of the marketing mix, the other three aspects being product, promotion, and place. Price is the only revenue generating element amongst the four Ps, the rest being cost centers. However, the other Ps of marketing will contribute to decreasing price elasticity and so enable price increases to drive greater revenue and profits.\nPricing can be a manual or automatic process of applying prices to purchase and sales orders, based on factors such as: a fixed amount, quantity break, promotion or sales campaign, specific vendor quote, price prevailing on entry, shipment or invoice date, combination of multiple orders or lines, and many others. An automated pricing system requires more setup and maintenance but may prevent pricing errors. The needs of the consumer can be converted into demand only if the consumer has the willingness and capacity to buy the product. Thus, pricing is the most important concept in the field of marketing, it is used as a tactical decision in response to changing competitive, market and organizational situations."
    },
    {
      "id": "65910",
      "title": "Printed circuit board",
      "url": "https://en.wikipedia.org/wiki/Printed_circuit_board",
      "summary": "A printed circuit board (PCB), also called printed wiring board (PWB), is a medium used to connect or \"wire\" components to one another in a circuit. It takes the form of a laminated sandwich structure of conductive and insulating layers: each of the conductive layers is designed with a pattern of traces, planes and other features (similar to wires on a flat surface) etched from one or more sheet layers of copper laminated onto and/or between sheet layers of a non-conductive substrate. Electrical components may be fixed to conductive pads on the outer layers in the shape designed to accept the component's terminals, generally by means of soldering, to both electrically connect and mechanically fasten them to it. Another manufacturing process adds vias, plated-through holes that allow interconnections between layers.\nPrinted circuit boards are used in nearly all electronic products. Alternatives to PCBs include wire wrap and point-to-point construction, both once popular but now rarely used. PCBs require additional design effort to lay out the circuit, but manufacturing and assembly can be automated. Electronic design automation software is available to do much of the work of layout. Mass-producing circuits with PCBs is cheaper and faster than with other wiring methods, as components are mounted and wired in one operation. Large numbers of PCBs can be fabricated at the same time, and the layout has to be done only once.  PCBs can also be made manually in small quantities, with reduced benefits.PCBs can be single-sided (one copper layer), double-sided (two copper layers on both sides of one substrate layer), or multi-layer (outer and inner layers of copper, alternating with layers of substrate). Multi-layer PCBs allow for much higher component density, because circuit traces on the inner layers would otherwise take up surface space between components. The rise in popularity of multilayer PCBs with more than two, and especially with more than four, copper planes was concurrent with the adoption of surface mount technology. However, multilayer PCBs make repair, analysis, and field modification of circuits much more difficult and usually impractical.\nThe world market for bare PCBs exceeded $60.2 billion in 2014 and is estimated to reach $79 billion by 2024.\n\n"
    },
    {
      "id": "42378348",
      "title": "Probabilistic classification",
      "url": "https://en.wikipedia.org/wiki/Probabilistic_classification",
      "summary": "In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles.\n\n"
    },
    {
      "id": "5017608",
      "title": "Probabilistic logic",
      "url": "https://en.wikipedia.org/wiki/Probabilistic_logic",
      "summary": "Probabilistic logic (also probability logic and probabilistic reasoning) involves the use of probability and logic to deal with uncertain situations. Probabilistic logic extends traditional logic truth tables with probabilistic expressions. A difficulty of probabilistic logics is their tendency to multiply the computational complexities of their probabilistic and logical components.  Other difficulties include the possibility of counter-intuitive results, such as in case of belief fusion in Dempster\u2013Shafer theory. Source trust and epistemic uncertainty about the probabilities they provide, such as defined in subjective logic, are additional elements to consider. The need to deal with a broad variety of contexts and issues has led to many different proposals.\n\n"
    },
    {
      "id": "22934",
      "title": "Probability",
      "url": "https://en.wikipedia.org/wiki/Probability",
      "summary": "Probability is the branch of mathematics concerning events and numerical descriptions of how likely they are to occur.  The probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes ('heads' and 'tails') are both equally probable; the probability of 'heads' equals the probability of 'tails'; and since no other outcomes are possible, the probability of either 'heads' or 'tails' is 1/2 (which could also be written as 0.5 or 50%).\nThese concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems."
    },
    {
      "id": "154725",
      "title": "Probability mass function",
      "url": "https://en.wikipedia.org/wiki/Probability_mass_function",
      "summary": "In probability and statistics, a probability mass function (sometimes called probability function or frequency function) is a function that gives the probability that a discrete random variable is exactly equal to some value.  Sometimes it is also known as the discrete probability density function. The probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete.\nA probability mass function differs from a probability density function (PDF) in that the latter is associated with continuous rather than discrete random variables. A PDF must be integrated over an interval to yield a probability.The value of the random variable having the largest probability mass is called the mode.\n\n"
    },
    {
      "id": "380008",
      "title": "Probably approximately correct learning",
      "url": "https://en.wikipedia.org/wiki/Probably_approximately_correct_learning",
      "summary": "In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the \"probably\" part), the selected function will have low generalization error (the \"approximately correct\" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or distribution of the samples.\nThe model was later extended to treat noise (misclassified samples).\nAn important innovation of the PAC framework is the introduction of computational complexity theory concepts to machine learning. In particular, the learner is expected to find efficient functions (time and space requirements bounded to a polynomial of the example size), and the learner itself must implement an efficient procedure (requiring an example count bounded to a polynomial of the concept size, modified by the approximation and likelihood bounds).\n\n"
    },
    {
      "id": "658183",
      "title": "Industrial process control",
      "url": "https://en.wikipedia.org/wiki/Industrial_process_control",
      "summary": "An industrial process control or simply process control in continuous production processes is a discipline that uses industrial control systems and control theory to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large industrial process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops."
    },
    {
      "id": "276598",
      "title": "Product placement",
      "url": "https://en.wikipedia.org/wiki/Product_placement",
      "summary": "Product placement, also known as embedded marketing, is a marketing technique where references to specific brands or products are incorporated into another work, such as a film or television program, with specific promotional intent.  Much of this is done by loaning products, especially when expensive items, such as vehicles, are involved.  In 2021, the agreements between brand owners and films and television programs were worth more than US$20 billion.While references to brands (real or fictional) may be voluntarily incorporated into works to maintain a feeling of realism or be a subject of commentary, product placement is the deliberate incorporation of references to a brand or product in exchange for compensation. Product placements may range from unobtrusive appearances within an environment, to prominent integration and acknowledgement of the product within the work. Common categories of products used for placements include automobiles and consumer electronics. Works produced by vertically integrated companies (such as Sony) may use placements to promote their other divisions as a form of corporate synergy.\nDuring the 21st century, the use of product placement on television has grown, particularly to combat the wider use of digital video recorders that can skip traditional commercial breaks, as well as to engage with younger demographics. Digital editing  technology is also being used to tailor product placement to specific demographics or markets, and in some cases, add placements to works that did not originally have embedded advertising, or update existing placements.\n\n"
    },
    {
      "id": "23015",
      "title": "Programming language",
      "url": "https://en.wikipedia.org/wiki/Programming_language",
      "summary": "A programming language is a system of notation for writing computer programs.\nA programming language is described by its syntax (form) and semantics (meaning). It gets its basis from formal languages.A language usually has at least one implementation in the form of a compiler or interpreter, allowing programs written in the language to be executed.\nProgramming language theory is the subfield of computer science that studies the design, implementation, analysis, characterization, and classification of programming languages.\n\n"
    },
    {
      "id": "3882218",
      "title": "Programming language theory",
      "url": "https://en.wikipedia.org/wiki/Programming_language_theory",
      "summary": "Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics. There are a number of academic conferences and journals in the area.\n\n"
    },
    {
      "id": "189897",
      "title": "Programming paradigm",
      "url": "https://en.wikipedia.org/wiki/Programming_paradigm",
      "summary": "A programming paradigm is a relatively high-level way to structure and conceptualize the implementation of a computer program. A programming language can be classified as supporting one or more paradigms.Paradigms are separated along and described by different dimensions of programming. Some paradigms are about implications of the execution model, such as allowing side effects, or whether the sequence of operations is defined by the execution model. Other paradigms are about the way code is organized, such as grouping into units that include both state and behavior. Yet others are about syntax and grammar.\nSome common programming paradigms include (shown in hierarchical relationship):\nImperative \u2013 code directly controls execution flow and state change\nprocedural \u2013 organized as procedures that call each other\nobject-oriented \u2013 organized as objects that contain both data structure and associated behavior\nDeclarative \u2013 code declares properties of the desired result, but not how to compute it\nfunctional \u2013 a desired result is declared as the value of a series of function evaluations\nlogic \u2013 a desired result is declared as the answer to a question about a system of facts and rules\nreactive \u2013 a desired result is declared with data streams and the propagation of change\n\n"
    },
    {
      "id": "26615065",
      "title": "Programming team",
      "url": "https://en.wikipedia.org/wiki/Programming_team",
      "summary": "A programming team is a team of people who develop or maintain computer software.  They may be organised in numerous ways, but the egoless programming team and chief programmer team have been common structures.\n\n"
    },
    {
      "id": "370882",
      "title": "Programming tool",
      "url": "https://en.wikipedia.org/wiki/Programming_tool",
      "summary": "A programming tool or software development tool is a computer program that software developers use to create, debug, maintain, or otherwise support other programs and applications. The term usually refers to relatively simple programs, that can be combined to accomplish a task, much as one might use multiple hands to fix a physical object. The most basic tools are a source code editor and a compiler or interpreter, which are used ubiquitously and continuously. Other tools are used more or less depending on the language, development methodology, and individual engineer, often used for a discrete task, like a debugger or profiler. Tools may be discrete programs, executed separately \u2013 often from the command line \u2013 or may be parts of a single large program, called an integrated development environment (IDE). In many cases, particularly for simpler use, simple ad hoc techniques are used instead of a tool, such as print debugging instead of using a debugger, manual timing (of overall program or section of code) instead of a profiler, or tracking bugs in a  text file or spreadsheet instead of a bug tracking system.\nThe distinction between tools and applications is murky. For example, developers use simple databases (such as a file containing a list of important values) all the time as tools. However a full-blown database is usually thought of as an application or software in its own right. For many years, computer-assisted software engineering (CASE) tools were sought. Successful tools have proven elusive. In one sense, CASE tools emphasized design and architecture support, such as for UML. But the most successful of these tools are IDEs.\n\n"
    },
    {
      "id": "16598232",
      "title": "Progress in artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence",
      "summary": "Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a multidisciplinary branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, economic-financial applications, robot control, law, scientific discovery, video games, and toys. However, many AI applications are not perceived as AI:  \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\" In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field was rarely credited for these successes at the time.\nKaplan and Haenlein structure artificial intelligence along three evolutionary stages: 1) artificial narrow intelligence \u2013 applying AI only to specific tasks; 2) artificial general intelligence \u2013 applying AI to several areas and able to autonomously solve problems they were never even designed for; and 3) artificial super intelligence \u2013 applying AI to any area capable of scientific creativity, social skills, and general wisdom.To allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems.  Such tests have been termed subject matter expert Turing tests.  Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\nHumans still substantially outperform both GPT-4 and models trained on the ConceptARC benchmark that scored 60% on most, and 77% on one category, while humans 91% on all and 97% on one category.\n\n"
    },
    {
      "id": "57719367",
      "title": "Project Debater",
      "url": "https://en.wikipedia.org/wiki/Project_Debater",
      "summary": "Project Debater is an IBM artificial intelligence project, designed to participate in a full live debate with expert human debaters. It follows on from the Watson project which played Jeopardy!"
    },
    {
      "id": "69071767",
      "title": "Prompt engineering",
      "url": "https://en.wikipedia.org/wiki/Prompt_engineering",
      "summary": "Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.A prompt for a text-to-text language model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison \u2192 house, chat \u2192 cat, chien \u2192\" (the expected response being dog), an approach called few-shot learning.When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic."
    },
    {
      "id": "55330205",
      "title": "Proper generalized decomposition",
      "url": "https://en.wikipedia.org/wiki/Proper_generalized_decomposition",
      "summary": "The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions, such as the Poisson's equation or the Laplace's equation.\nThe PGD algorithm computes an approximation of the solution of the BVP by successive enrichment. This means that, in each iteration, a new component (or mode) is computed and added to the approximation. In principle, the more modes obtained, the closer the approximation is to its theoretical solution. Unlike POD principal components, PGD modes are not necessarily orthogonal to each other.\nBy selecting only the most relevant PGD modes, a reduced order model of the solution is obtained. Because of this, PGD is considered a dimensionality reduction algorithm.\n\n"
    },
    {
      "id": "49761",
      "title": "Punched tape",
      "url": "https://en.wikipedia.org/wiki/Punched_tape",
      "summary": "Punched tape or perforated paper tape is a form of data storage device that consists of a long strip of paper through which small holes are punched. It was developed from and was subsequently used alongside punched cards, the difference being that the tape is continuous.\nPunched cards, and chains of punched cards, were used for control of looms in the 18th century. Use for telegraphy systems started in 1842. Punched tapes were used throughout the 19th and for much of the 20th centuries for programmable looms, teleprinter communication, for input to computers of the 1950s and 1960s, and later as a storage medium for minicomputers and CNC machine tools. During the Second World War, high-speed punched tape systems using optical readout methods were used in code breaking systems. Punched tape was used to transmit data for manufacture of read-only memory chips.\n\n"
    },
    {
      "id": "30051",
      "title": "Torch",
      "url": "https://en.wikipedia.org/wiki/Torch",
      "summary": "A torch is a stick with combustible material at one end which can be used as a light source or to set something on fire. Torches have been used throughout history, and are still used in processions, symbolic and religious events, and in juggling entertainment. In some countries, notably the United Kingdom and Australia, \"torch\" in modern usage is also the term for a battery-operated portable light."
    },
    {
      "id": "1281850",
      "title": "Q-learning",
      "url": "https://en.wikipedia.org/wiki/Q-learning",
      "summary": "Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations.For any finite Markov decision process, Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given finite Markov decision process, given infinite exploration time and a partly random policy. \"Q\" refers to the function that the algorithm computes \u2013 the expected rewards for an action taken in a given state."
    },
    {
      "id": "25220",
      "title": "Quantum computing",
      "url": "https://en.wikipedia.org/wiki/Quantum_computing",
      "summary": "A quantum computer is a computer that takes advantage of quantum mechanical phenomena.\nAt small scales, physical matter exhibits properties of both particles and waves, and quantum computing leverages this behavior, specifically quantum superposition and entanglement, using specialized hardware that supports the preparation and manipulation of quantum states.\nClassical physics cannot explain the operation of these quantum devices, and a scalable quantum computer could perform some calculations exponentially faster (with respect to input size scaling) than any modern \"classical\" computer. In particular, a large-scale quantum computer could break widely used encryption schemes and aid physicists in performing physical simulations; however, the current state of the art is largely experimental and impractical, with several obstacles to useful applications. Moreover, scalable quantum computers do not hold promise for many practical tasks, and for many important tasks quantum speedups are proven impossible.\nThe basic unit of information in quantum computing is the qubit, similar to the bit in traditional digital electronics. Unlike a classical bit, a qubit can exist in a superposition of its two \"basis\" states. When measuring a qubit, the result is a probabilistic output of a classical bit, therefore making quantum computers nondeterministic in general. If a quantum computer manipulates the qubit in a particular way, wave interference effects can amplify the desired measurement results. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\nPhysically engineering high-quality qubits has proven challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. Paradoxically, perfectly isolating qubits is also undesirable because quantum computations typically need to initialize qubits, perform controlled qubit interactions, and measure the resulting quantum states. Each of those operations introduces errors and suffers from noise, and such inaccuracies accumulate.\nNational governments have invested heavily in experimental research that aims to develop scalable qubits with longer coherence times and lower error rates. Two of the most promising technologies are superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single ion using electromagnetic fields).\nIn principle, a non-quantum (classical) computer can solve the same computational problems as a quantum computer, given enough time. Quantum advantage comes in the form of time complexity rather than computability, and quantum complexity theory shows that some quantum algorithms for carefully selected tasks require exponentially fewer computational steps than the best known non-quantum algorithms. Such tasks can in theory be solved on a large-scale quantum computer whereas classical computers would not finish computations in any reasonable amount of time. However, quantum speedup is not universal or even typical across computational tasks, since basic tasks such as sorting are proven to not allow any asymptotic quantum speedup. Claims of quantum supremacy have drawn significant attention to the discipline, but are demonstrated on contrived tasks, while near-term practical use cases remain limited. \nOptimism about quantum computing is fueled by a broad range of new theoretical hardware possibilities facilitated by quantum physics, but the improving understanding of quantum computing limitations counterbalances this optimism. In particular, quantum speedups have been traditionally estimated for noiseless quantum computers, whereas the impact of noise and the use of quantum error-correction can undermine low-polynomial speedups.\n\n"
    },
    {
      "id": "44108758",
      "title": "Quantum machine learning",
      "url": "https://en.wikipedia.org/wiki/Quantum_machine_learning",
      "summary": "Quantum machine learning is the integration of quantum algorithms within machine learning programs.The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\"."
    },
    {
      "id": "44300158",
      "title": "RCASE",
      "url": "https://en.wikipedia.org/wiki/RCASE",
      "summary": "Root Cause Analysis Solver Engine (informally RCASE) is a proprietary algorithm developed from research originally at the Warwick Manufacturing Group (WMG) at Warwick University. RCASE development commenced in 2003 to provide an automated version of root cause analysis, the method of problem solving that tries to identify the root causes of faults or problems. RCASE is now owned by the spin-out company Warwick Analytics where it is being applied to automated predictive analytics software.\n\n"
    },
    {
      "id": "33283283",
      "title": "Rakesh Agrawal (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Rakesh_Agrawal_(computer_scientist)",
      "summary": "Rakesh Agrawal (\u0939\u093f\u0928\u094d\u0926\u0940 - \u0930\u093e\u0915\u0947\u0936 \u0905\u0917\u094d\u0930\u0935\u093e\u0932) is a computer scientist who until recently was a Technical Fellow at the Microsoft Search Labs. Rakesh is well known for developing fundamental data mining concepts and technologies and pioneering key concepts in data privacy, including Hippocratic Database, Sovereign Information Sharing, and Privacy-Preserving Data Mining. IBM's commercial data mining product, Intelligent Miner, grew out of his work. His research has been incorporated into other IBM products, including DB2 Mining Extender, DB2 OLAP Server and WebSphere Commerce Server, and has influenced several other commercial and academic products, prototypes and applications. His other technical contributions include Polyglot object-oriented type system, Alert active database system, Ode (Object database and environment), Alpha (extension of relational databases with generalized transitive closure), Nest distributed system, transaction management, and database machines.\nPrior to joining Microsoft in March 2006, Rakesh was an IBM Fellow and led the Quest group at the IBM Almaden Research Center. Earlier, he was with the Bell Laboratories, Murray Hill from 1983 to 1989. He also worked for three years at a leading Indian company, the Bharat Heavy Electricals Ltd. He received his M.S. and Ph.D. degrees in Computer Science from the University of Wisconsin-Madison in 1983. He also holds a B.E. degree in Electronics and Communication Engineering from IIT-Roorkee, and a two-year Post Graduate Diploma in Industrial Engineering from the National Institute of Industrial Engineering (NITIE), Bombay.\n\n"
    },
    {
      "id": "1363880",
      "title": "Random forest",
      "url": "https://en.wikipedia.org/wiki/Random_forest",
      "summary": "Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set.:\u200a587\u2013588\u200aThe first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered \"Random Forests\" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance."
    },
    {
      "id": "1089270",
      "title": "Random sample consensus",
      "url": "https://en.wikipedia.org/wiki/Random_sample_consensus",
      "summary": "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\nRANSAC uses repeated random sub-sampling. A basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.\n\n"
    },
    {
      "id": "25685",
      "title": "Random variable",
      "url": "https://en.wikipedia.org/wiki/Random_variable",
      "summary": "A random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events. The term 'random variable' can be misleading as its mathematical definition is not actually random nor a variable, but rather it is a function from possible outcomes (e.g., the possible upper sides of a flipped coin such as heads \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   and tails \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  ) in a sample space (e.g., the set \n  \n    \n      \n        {\n        H\n        ,\n        T\n        }\n      \n    \n    {\\displaystyle \\{H,T\\}}\n  ) to a measurable space (e.g., \n  \n    \n      \n        {\n        \u2212\n        1\n        ,\n        1\n        }\n      \n    \n    {\\displaystyle \\{-1,1\\}}\n   in which 1 is corresponding to \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n   and \u22121 is corresponding to \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  , respectively), often to the real numbers.\n\nInformally, randomness typically represents some fundamental element of chance, such as in the roll of a die; it may also represent uncertainty, such as measurement error. However, the interpretation of probability is philosophically complicated, and even in specific cases is not always straightforward. The purely mathematical analysis of random variables is independent of such interpretational difficulties, and can be based upon a rigorous axiomatic setup.\nIn the formal mathematical language of measure theory, a random variable is defined as a measurable function from a probability measure space (called the sample space) to a measurable space. This allows consideration of the pushforward measure, which is called the distribution of the random variable; the distribution is thus a probability measure on the set of all possible values of the random variable. It is possible for two random variables to have identical distributions but to differ in significant ways; for instance, they may be independent.\nIt is common to consider the special cases of discrete random variables and absolutely continuous random variables, corresponding to whether a random variable is valued in a countable subset or in an interval of real numbers. There are other important possibilities, especially in the theory of stochastic processes, wherein it is natural to consider random sequences or random functions. Sometimes a random variable is taken to be automatically valued in the real numbers, with more general random quantities instead being called random elements.\nAccording to George Mackey, Pafnuty Chebyshev was the first person \"to think systematically in terms of random variables\"."
    },
    {
      "id": "495383",
      "title": "Randomized algorithm",
      "url": "https://en.wikipedia.org/wiki/Randomized_algorithm",
      "summary": "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.\nOne has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, for example Quicksort), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, for example the Monte Carlo algorithm for the MFAS problem) or fail to produce a result either by signaling a failure or failing to terminate. In some cases, probabilistic algorithms are the only practical means of solving a problem.In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator.\n\n"
    },
    {
      "id": "5100865",
      "title": "RapidMiner",
      "url": "https://en.wikipedia.org/wiki/RapidMiner",
      "summary": "RapidMiner is a data science platform that analyses the collective impact of an organization's data. It was acquired by Altair Engineering in September 2022."
    },
    {
      "id": "402673",
      "title": "Ray Solomonoff",
      "url": "https://en.wikipedia.org/wiki/Ray_Solomonoff",
      "summary": "Ray Solomonoff (July 25, 1926 \u2013 December 7, 2009) was an American mathematician who invented algorithmic probability, his General Theory of Inductive Inference (also known as Universal Inductive Inference), and was a founder of algorithmic information theory. He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.Solomonoff first described algorithmic probability in 1960, publishing the theorem that launched Kolmogorov complexity and algorithmic information theory.  He first described these results at a conference at Caltech in 1960, and in a report, Feb. 1960, \"A Preliminary Report on a General Theory of Inductive Inference.\" He clarified these ideas more fully in his 1964 publications, \"A Formal Theory of Inductive Inference,\" Part I and Part II.Algorithmic probability is a mathematically formalized combination of Occam's razor, and the Principle of Multiple Explanations.\nIt is a machine independent method of assigning a probability value to each hypothesis (algorithm/program) that explains a given observation, with the simplest hypothesis (the shortest program) having the highest probability and the increasingly complex hypotheses receiving increasingly small probabilities.\nSolomonoff founded the theory of universal inductive inference, which is based on solid philosophical foundations and has its root in Kolmogorov complexity and algorithmic information theory. The theory uses algorithmic probability in a Bayesian framework. The universal prior is taken over the class of all computable measures; no hypothesis will have a zero probability. This enables Bayes' rule (of causation) to be used to predict the most likely next event in a series of events, and how likely it will be.Although he is best known for algorithmic probability and his general theory of inductive inference, he made many other important discoveries throughout his life, most of them directed toward his goal in artificial intelligence: to develop a machine that could solve hard problems using probabilistic methods.\n\n"
    },
    {
      "id": "248454",
      "title": "Raytheon",
      "url": "https://en.wikipedia.org/wiki/Raytheon",
      "summary": "Raytheon, now a business segment of RTX Corporation, was formerly known as the Raytheon Company. The Raytheon Company was a major U.S. defense contractor and industrial corporation with manufacturing concentrations in weapons and military and commercial electronics. It was previously involved in corporate and special-mission aircraft until early 2007. Raytheon was the world's largest producer of guided missiles. In April 2020, the Raytheon Company merged with United Technologies Corporation to form Raytheon Technologies, which changed its name to RTX Corporation in July 2023.\nRaytheon was originally established in 1922, reincorporated in 1928, and adopted the Raytheon Company name in 1959. During 2018, the company had around 67,000 employees worldwide and annual revenues of approximately US$25.35 billion. More than 90% of Raytheon's revenues were obtained from military contracts and, as of 2012, it was the fifth-largest military contractor in the world. As of 2015, it was the third largest defense contractor in the United States by defense revenue.In 2003, Raytheon's headquarters moved from Lexington, Massachusetts, to Waltham, Massachusetts. The company had previously been headquartered in Cambridge, Massachusetts, from 1922 to 1928, Newton, Massachusetts, from 1928 to 1941, Waltham from 1941 to 1961 and Lexington from 1961 to 2003.\n\n"
    },
    {
      "id": "25767",
      "title": "Real-time computing",
      "url": "https://en.wikipedia.org/wiki/Real-time_computing",
      "summary": "Real-time computing (RTC) is the computer science term for hardware and software systems subject to a \"real-time constraint\", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as \"deadlines\".The term \"real-time\" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock.\nReal-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually guarantee a response within any timeframe, although typical or expected response times may be given. Real-time processing fails if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.\nA real-time system has been described as one which \"controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time\". The term \"real-time\" is used in process control and enterprise systems to mean \"without significant delay\".\nReal-time software may use one or more of the following: synchronous programming languages, real-time operating systems (RTOSes), and real-time networks, each of which provide essential frameworks on which to build a real-time software application.\nSystems used for many safety-critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes, both of which demand immediate and accurate mechanical response.\n\n"
    },
    {
      "id": "20646438",
      "title": "Real number",
      "url": "https://en.wikipedia.org/wiki/Real_number",
      "summary": "In mathematics, a real number is a number that can be used to measure a continuous one-dimensional quantity such as a distance, duration or temperature. Here, continuous means that pairs of values can have arbitrarily small differences. Every real number can be almost uniquely represented by an infinite decimal expansion.The real numbers are fundamental in calculus (and more generally in all mathematics), in particular by their role in the classical definitions of limits, continuity and derivatives.The set of real numbers is denoted R or \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n   and is sometimes called \"the reals\".\nThe adjective real, used in the 17th century by Ren\u00e9 Descartes, distinguishes real numbers from imaginary numbers such as the square roots of \u22121.The real numbers include the rational numbers, such as the integer \u22125 and the fraction 4\u200a/\u200a3. The rest of the real numbers are called irrational numbers.  Some irrational numbers (as well as all the rationals) are the root of a polynomial with integer coefficients, such as the square root \u221a2 = 1.414...; these are called algebraic numbers.  There are also real numbers which are not, such as \u03c0 = 3.1415...; these are called transcendental numbers.Real numbers can be thought of as all points on a line called the number line or real line, where the points corresponding to integers (..., \u22122, \u22121, 0, 1, 2, ...) are equally spaced.\n\nConversely, analytic geometry is the association of points on lines (especially axis lines) to real numbers such that geometric displacements are proportional to differences between corresponding numbers.\nThe informal descriptions above of the real numbers are not sufficient for ensuring the correctness of proofs of theorems involving real numbers. The realization that a better definition was needed, and the elaboration of such a definition was a major development of 19th-century mathematics and is the foundation of real analysis, the study of real functions and real-valued sequences. A current axiomatic definition is that real numbers form the unique (up to an isomorphism) Dedekind-complete ordered field. Other common definitions of real numbers include equivalence classes of Cauchy sequences (of rational numbers), Dedekind cuts, and infinite decimal representations. All these definitions satisfy the axiomatic definition and are thus equivalent."
    },
    {
      "id": "922505",
      "title": "Receiver operating characteristic",
      "url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic",
      "summary": "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values.\nThe ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting.\nThe ROC can also be thought of as a plot of the statistical power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of false positive rate. \nGiven the probability distributions for both true positive and false positive are known, the ROC curve is obtained as the cumulative distribution function (CDF, area under the probability distribution from \n  \n    \n      \n        \u2212\n        \u221e\n      \n    \n    {\\displaystyle -\\infty }\n   to the discrimination threshold) of the detection probability in the y-axis versus the CDF of the false positive probability on the x-axis.\nROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making."
    },
    {
      "id": "596646",
      "title": "Recommender system",
      "url": "https://en.wikipedia.org/wiki/Recommender_system",
      "summary": "A recommender system, or a recommendation system (sometimes replacing \"system\" with terms such as \"platform\", \"engine\", or \"algorithm\"), is a subclass of information filtering system that provides suggestions for items that are most pertinent to a particular user.  Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer.Typically, the suggestions refer to various decision-making processes, such as what product to purchase, what music to listen to, or what online news to read.\nRecommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single type of input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services."
    },
    {
      "id": "37862937",
      "title": "Rectifier (neural networks)",
      "url": "https://en.wikipedia.org/wiki/Rectifier_(neural_networks)",
      "summary": "In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument:\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          x\n          \n            +\n          \n        \n        =\n        max\n        (\n        0\n        ,\n        x\n        )\n        =\n        \n          \n            \n              x\n              +\n              \n                |\n              \n              x\n              \n                |\n              \n            \n            2\n          \n        \n        =\n        \n          \n            {\n            \n              \n                \n                  x\n                \n                \n                  \n                    if \n                  \n                  x\n                  >\n                  0\n                  ,\n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    otherwise\n                  \n                  ,\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)=x^{+}=\\max(0,x)={\\frac {x+|x|}{2}}={\\begin{cases}x&{\\text{if }}x>0,\\\\0&{\\text{otherwise}},\\end{cases}}}\n  where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience.\n\n"
    },
    {
      "id": "1706303",
      "title": "Recurrent neural network",
      "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
      "summary": "A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\nAdditional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\n\n"
    },
    {
      "id": "1167455",
      "title": "Recursive self-improvement",
      "url": "https://en.wikipedia.org/wiki/Recursive_self-improvement",
      "summary": "Recursive self-improvement (RSI) is a process in which an early or weak artificial general intelligence (AGI) system enhances its own capabilities and intelligence without human intervention leading to a superintelligence or intelligence explosion.The development of recursive self-improvement raises significant ethical and safety concerns, as such systems may evolve in unforeseen ways and could potentially surpass human control or understanding. There has been a number of proponents that have pushed to pause or slow down AI development for the potential risks of runaway AI systems."
    },
    {
      "id": "2009061",
      "title": "Regularization (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Regularization_(mathematics)",
      "summary": "In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be \"simpler\". It is often used to obtain results for ill-posed problems or to prevent overfitting.Although regularization procedures can be divided in many ways, the following delineation is particularly helpful:\n\nExplicit regularization is regularization whenever one explicitly adds a term to the optimization problem.  These terms could be priors, penalties, or constraints. Explicit regularization is commonly employed with ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique.\nImplicit regularization is all other forms of regularization.  This includes, for example, early stopping, using a robust loss function, and discarding outliers. Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees).In explicit regularization, independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement and a regularization term that corresponds to a prior. By combining both using Bayesian statistics, one can compute a posterior, that includes both information sources and therefore stabilizes the estimation process. By trading off both objectives, one chooses to be more addictive to the data or to enforce generalization (to prevent overfitting). There is a whole research branch dealing with all possible regularizations. In practice, one usually tries a specific regularization and then figures out the probability density that corresponds to that regularization to justify the choice. It can also be physically motivated by common sense or intuition.\nIn machine learning, the data term corresponds to the training data and the regularization is either the choice of the model or modifications to the algorithm. It is always intended to reduce the generalization error, i.e. the error score with the trained model on the evaluation set and not the training data.One of the earliest uses of regularization is Tikhonov regularization, related to the method of least squares."
    },
    {
      "id": "63451675",
      "title": "Regulation of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence",
      "summary": "The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally, including in the European Union (which has governmental regulatory power) and in supra-national bodies like the IEEE, OECD (which do not) and others. Since 2016, a wave of AI ethics guidelines have been published in order to maintain social control over the technology. Regulation is considered necessary to both encourage AI and manage associated risks. In addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone."
    },
    {
      "id": "66294",
      "title": "Reinforcement learning",
      "url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "summary": "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the long term reward, whose feedback might be incomplete or delayed.\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process and they target large Markov decision processes where exact methods become infeasible."
    },
    {
      "id": "4195092",
      "title": "Relevance vector machine",
      "url": "https://en.wikipedia.org/wiki/Relevance_vector_machine",
      "summary": "In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.\nThe RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\nIt is actually equivalent to a Gaussian process model with covariance function:\n\n  \n    \n      \n        k\n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            1\n            \n              \u03b1\n              \n                j\n              \n            \n          \n        \n        \u03c6\n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        \u03c6\n        (\n        \n          \n            x\n          \n          \u2032\n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle k(\\mathbf {x} ,\\mathbf {x'} )=\\sum _{j=1}^{N}{\\frac {1}{\\alpha _{j}}}\\varphi (\\mathbf {x} ,\\mathbf {x} _{j})\\varphi (\\mathbf {x} ',\\mathbf {x} _{j})}\n  where \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n   is the kernel function (usually Gaussian), \n  \n    \n      \n        \n          \u03b1\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{j}}\n   are the variances of the prior on the weight vector\n\n  \n    \n      \n        w\n        \u223c\n        N\n        (\n        0\n        ,\n        \n          \u03b1\n          \n            \u2212\n            1\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle w\\sim N(0,\\alpha ^{-1}I)}\n  , and \n  \n    \n      \n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            x\n          \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{1},\\ldots ,\\mathbf {x} _{N}}\n   are the input vectors of the training set.Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).\nThe relevance vector machine was patented in the United States by Microsoft (patent expired September 4, 2019)."
    },
    {
      "id": "6604",
      "title": "Rendering (computer graphics)",
      "url": "https://en.wikipedia.org/wiki/Rendering_(computer_graphics)",
      "summary": "Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program.  The resulting image is referred to as the render.  Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure.  The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term \"rendering\" is analogous to the concept of an artist's impression of a scene.  The term \"rendering\" is also used to describe the process of calculating effects in a video editing program to produce the final video output.\nRendering is one of the major sub-topics of 3D computer graphics, and in practice it is always connected to the others. It is the last major step in the graphics pipeline, giving models and animation their final appearance. With the increasing sophistication of computer graphics since the 1970s, it has become a more distinct subject.\nRendering has uses in architecture, video games, simulators, movie and TV visual effects, and design visualization, each employing a different balance of features and techniques. A wide variety of renderers are available for use. Some are integrated into larger modeling and animation packages, some are stand-alone, and some are free open-source projects. On the inside, a renderer is a carefully engineered program based on multiple disciplines, including light physics, visual perception, mathematics, and software development.\nThough the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image on a screen from a 3D representation stored in a scene file are handled by the graphics pipeline in a rendering device such as a GPU. A GPU is a purpose-built device that assists a CPU in performing complex rendering calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software must solve the rendering equation. The rendering equation does not account for all lighting phenomena, but instead acts as a general lighting model for computer-generated imagery.\nIn the case of 3D graphics, scenes can be pre-rendered or generated in realtime.  Pre-rendering is a slow, computationally intensive process that is typically used for movie creation, where scenes can be generated ahead of time, while real-time rendering is often done for 3D video games and other applications that must dynamically create scenes.  3D hardware accelerators can improve realtime rendering performance."
    },
    {
      "id": "522449",
      "title": "Requirements analysis",
      "url": "https://en.wikipedia.org/wiki/Requirements_analysis",
      "summary": "In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.Requirements analysis is critical to the success or failure of a systems or software project. The requirements should be documented, actionable, measurable, testable, traceable, related to identified business needs or opportunities, and defined to a level of detail sufficient for system design.\n\n"
    },
    {
      "id": "10667750",
      "title": "Reservoir computing",
      "url": "https://en.wikipedia.org/wiki/Reservoir_computing",
      "summary": "Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir. After the input signal is fed into the reservoir, which is treated as a \"black box,\" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output. The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed. The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.\n\n"
    },
    {
      "id": "55867424",
      "title": "Residual neural network",
      "url": "https://en.wikipedia.org/wiki/Residual_neural_network",
      "summary": "A residual neural network (also referred to as a residual network or ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. It behaves like a highway network whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections, often referred to as \"residual connections\", are also used in the 1997 LSTM networks, Transformer models (e.g., BERT, GPT models such as ChatGPT), the AlphaGo Zero system, the AlphaStar system, and the AlphaFold system.\nResidual networks were developed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, who won the 2015 ImageNet competition.\n\n"
    },
    {
      "id": "33742232",
      "title": "Restricted Boltzmann machine",
      "url": "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine",
      "summary": "A restricted Boltzmann machine (RBM) (also called a restricted Sherrington\u2013Kirkpatrick model with external field or restricted stochastic Ising\u2013Lenz\u2013Little model) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.RBMs were initially proposed under the name Harmonium by Paul Smolensky in 1986, and rose to prominence after Geoffrey Hinton and collaborators used fast learning algorithms for them in the mid-2000s. RBMs have found applications in dimensionality reduction, classification, collaborative filtering, feature learning, topic modelling and even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the \"visible\" and \"hidden\" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, \"unrestricted\" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by \"stacking\" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.\n\n"
    },
    {
      "id": "21280907",
      "title": "Richard O. Duda",
      "url": "https://en.wikipedia.org/wiki/Richard_O._Duda",
      "summary": "Richard O. Duda  is Professor Emeritus of Electrical Engineering at San Jose State University renowned for his work on sound localization and pattern recognition. He lives in Menlo Park, California.\n\n"
    },
    {
      "id": "954328",
      "title": "Ridge regression",
      "url": "https://en.wikipedia.org/wiki/Ridge_regression",
      "summary": "Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and engineering. Also known as Tikhonov regularization, named for Andrey Tikhonov, it is a method of regularization of ill-posed problems. It is particularly useful to mitigate the problem of multicollinearity in linear regression, which commonly occurs in models with large numbers of parameters. In general, the method provides improved efficiency in parameter estimation problems in exchange for a tolerable amount of bias (see bias\u2013variance tradeoff).The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers \"Ridge regressions: biased estimation of nonorthogonal problems\" and \"Ridge regressions: applications in nonorthogonal problems\". This was the result of ten years of research into the field of ridge analysis.Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variables\u2014by creating a ridge regression estimator (RR). This provides a more precise ridge parameters estimate, as its variance and mean square estimator are often smaller than the least square estimators previously derived.\n\n"
    },
    {
      "id": "31404681",
      "title": "Robert Tibshirani",
      "url": "https://en.wikipedia.org/wiki/Robert_Tibshirani",
      "summary": "Robert Tibshirani  (born July 10, 1956) is a professor in the Departments of Statistics and Biomedical Data Science at Stanford University. He was a professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics.\nHis most well-known contributions are the\nLasso method, which proposed the use of  L1  penalization in regression and related problems, and Significance Analysis of Microarrays.\n\n"
    },
    {
      "id": "175885",
      "title": "Robot control",
      "url": "https://en.wikipedia.org/wiki/Robot_control",
      "summary": "Robotic control is the system that contributes to the movement of robots. This involves the mechanical aspects and programmable systems that makes it possible to control robots. Robotics can be controlled by various means including manual, wireless, semi-autonomous (a mix of fully automatic and wireless control), and fully autonomous (using artificial intelligence).\n\n"
    },
    {
      "id": "3290880",
      "title": "Robot learning",
      "url": "https://en.wikipedia.org/wiki/Robot_learning",
      "summary": "Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).\nExample of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization,  as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation.\nRobot learning can be closely related to adaptive control, reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills.\nWhile machine learning is frequently used by computer vision algorithms employed in the context of robotics, these applications are usually not referred to as \"robot learning\".\n\n"
    },
    {
      "id": "1167036",
      "title": "Robot locomotion",
      "url": "https://en.wikipedia.org/wiki/Robot_locomotion",
      "summary": "Robot locomotion is the collective name for the various methods that robots use to transport themselves from place to place.\nWheeled robots are typically quite energy efficient and simple to control. However, other forms of locomotion may be more appropriate for a number of reasons, for example traversing rough terrain, as well as moving and interacting in human environments. Furthermore, studying bipedal and insect-like robots may beneficially impact on biomechanics.\nA major goal in this field is in developing capabilities for robots to autonomously decide how, when, and where to move.  However, coordinating numerous robot joints for even simple matters, like negotiating stairs, is difficult. Autonomous robot locomotion is a major technological obstacle for many areas of robotics, such as humanoids (like Honda's Asimo).\n\n"
    },
    {
      "id": "20903754",
      "title": "Robotics",
      "url": "https://en.wikipedia.org/wiki/Robotics",
      "summary": "Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, materials and biomedical engineering. \nThe goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes."
    },
    {
      "id": "51997474",
      "title": "Rule-based machine learning",
      "url": "https://en.wikipedia.org/wiki/Rule-based_machine_learning",
      "summary": "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply. The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system.\nRule-based machine learning approaches include learning classifier systems, association rule learning, artificial immune systems, and any other method that relies on a set of rules, each covering contextual knowledge.\nWhile rule-based machine learning is conceptually a type of rule-based system, it is distinct from traditional rule-based systems, which are often hand-crafted, and other rule-based decision makers. This is because rule-based machine learning applies some form of learning algorithm to automatically identify useful rules, rather than a human needing to apply prior domain knowledge to manually construct rules and curate a rule set."
    },
    {
      "id": "990677",
      "title": "SAS (software)",
      "url": "https://en.wikipedia.org/wiki/SAS_(software)",
      "summary": "SAS (previously \"Statistical Analysis System\") is a statistical software suite developed by SAS Institute for  data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation, and predictive analytics.\nSAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.\n\n"
    },
    {
      "id": "20588127",
      "title": "SPSS Modeler",
      "url": "https://en.wikipedia.org/wiki/SPSS_Modeler",
      "summary": "IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming.\nOne of its main aims from the outset was to get rid of unnecessary complexity in data transformations, and to make complex predictive models very easy to use.\nThe first version incorporated decision trees (ID3), and neural networks (backprop), which could both be trained without underlying knowledge of how those techniques worked.\nIBM SPSS Modeler was originally named Clementine by its creators, Integral Solutions Limited. This name continued for a while after SPSS's acquisition of the product. SPSS later changed the name to SPSS Clementine, and then later to PASW Modeler. Following IBM's 2009 acquisition of SPSS, the product was renamed IBM SPSS Modeler, its current name.\n\n"
    },
    {
      "id": "2079997",
      "title": "Statistica",
      "url": "https://en.wikipedia.org/wiki/Statistica",
      "summary": "Statistica is an advanced analytics software package originally developed by StatSoft and currently maintained by TIBCO Software Inc.\nStatistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures.\n\n"
    },
    {
      "id": "160361",
      "title": "Sampling (statistics)",
      "url": "https://en.wikipedia.org/wiki/Sampling_(statistics)",
      "summary": "In statistics, quality assurance, and survey methodology, sampling is the selection of a subset or a statistical sample (termed sample for short) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population. Sampling has lower costs and faster data collection compared to recording data from the entire population, and thus, it can provide insights in cases where it is infeasible to measure an entire population.\nEach observation measures one or more properties (such as weight, location, colour or mass) of independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. Acceptance sampling is used to determine if a production lot of material meets the governing specifications."
    },
    {
      "id": "33490859",
      "title": "Scikit-learn",
      "url": "https://en.wikipedia.org/wiki/Scikit-learn",
      "summary": "scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.\nIt features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project."
    },
    {
      "id": "28249",
      "title": "Search algorithm",
      "url": "https://en.wikipedia.org/wiki/Search_algorithm",
      "summary": "In computer science, a search algorithm is an algorithm designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values.\nAlthough search engines use search algorithms, they belong to the study of information retrieval, not algorithmics.\nThe appropriate search algorithm to use often depends on the data structure being searched, and may also include prior knowledge about the data. Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes.Search algorithms can be classified based on their mechanism of searching into three types of algorithms: linear, binary, and hashing. Linear search algorithms check every record for the one associated with a target key in a linear fashion. Binary, or half-interval, searches repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order. Digital search algorithms work based on the properties of digits in data structures by using numerical keys. Finally, hashing directly maps keys to records based on a hash function.Algorithms are often evaluated by their computational complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O(log n), or logarithmic time. In simple terms, the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.\n\n"
    },
    {
      "id": "4059023",
      "title": "Search engine",
      "url": "https://en.wikipedia.org/wiki/Search_engine",
      "summary": "A search engine is a software system that finds web pages that match a web search. It searches the World Wide Web in a systematic way for particular information specified in a textual web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of hyperlinks to web pages, images, videos, infographics, articles, and other types of files. As of January 2022, Google is by far the world's most used search engine, with a market share of 90.6%, and the world's other most used search engines were Bing, Yahoo!, Baidu, Yandex, and DuckDuckGo.Unlike web directories and social bookmarking sites, which are maintained by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. Modern search engines are based on techniques and methods developed in the field of Information retrieval. Some search engines also mine data available in databases or open directories.\nInternet-based content that cannot be indexed and searched by a web search engine falls under the category of deep web."
    },
    {
      "id": "2471540",
      "title": "Security hacker",
      "url": "https://en.wikipedia.org/wiki/Security_hacker",
      "summary": "A security hacker is someone who explores methods for breaching defenses and exploiting weaknesses in a computer system or network. Hackers may be motivated by a multitude of reasons, such as profit, protest, information gathering, challenge, recreation, or evaluation of a system weaknesses to assist in formulating defenses against potential hackers.\nLongstanding controversy surrounds the meaning of the term \"hacker.\" In this controversy, computer programmers reclaim the term hacker, arguing that it refers simply to someone with an advanced understanding of computers and computer networks, and that cracker is the more appropriate term for those who break into computers, whether computer criminals (black hats) or computer security experts (white hats). A 2014 article noted that \"the black-hat meaning still prevails among the general public\". The subculture that has evolved around hackers is often referred to as the \"computer underground\".\n\n"
    },
    {
      "id": "29015809",
      "title": "Security service (telecommunication)",
      "url": "https://en.wikipedia.org/wiki/Security_service_(telecommunication)",
      "summary": "Security service is a service, provided by a layer of communicating open systems, which ensures adequate security of the systems or of data transfers as defined by ITU-T X.800 Recommendation. \nX.800 and ISO 7498-2 (Information processing systems \u2013 Open systems interconnection \u2013 Basic Reference Model \u2013 Part 2: Security architecture)  are technically aligned. This model is widely recognized A more general definition is in CNSS Instruction No. 4009 dated 26 April 2010 by Committee on National Security Systems of United States of America:\nA capability that supports one, or more, of the security requirements (Confidentiality, Integrity, Availability). Examples of security services are key management, access control, and authentication.Another authoritative definition is in W3C Web service Glossary  adopted by NIST SP 800-95:\nA processing or communication service that is provided by a system to give a specific kind of protection to resources, where said resources may reside with said system or reside with other systems, for example, an authentication service or a PKI-based document attribution and authentication service. A security service is a superset of AAA services. Security services typically implement portions of security policies and are implemented via security mechanisms.\n\n"
    },
    {
      "id": "76996",
      "title": "Self-organizing map",
      "url": "https://en.wikipedia.org/wiki/Self-organizing_map",
      "summary": "A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   variables measured in \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional \"map\" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.\nAn SOM is a type of artificial neural network but is trained using competitive learning rather than the error-correction learning (e.g., backpropagation with gradient descent) used by other artificial neural networks. The SOM was introduced by the Finnish professor Teuvo Kohonen in the 1980s and therefore is sometimes called a Kohonen map or Kohonen network. The Kohonen map or network is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.\nSOMs create internal representations reminiscent of the cortical homunculus, a distorted representation of the human body, based on a neurological \"map\" of the areas and proportions of the human brain dedicated to processing sensory functions, for different parts of the body.\n\n"
    },
    {
      "id": "70721875",
      "title": "Self-play",
      "url": "https://en.wikipedia.org/wiki/Self-play",
      "summary": "Self-play is a technique for improving the performance of reinforcement learning agents. Intuitively, agents learn to improve their performance by playing \"against themselves\"."
    },
    {
      "id": "67902375",
      "title": "Self-supervised learning",
      "url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
      "summary": "Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on external labels provided by humans. In the context of neural networks, self-supervised learning aims to leverage inherent structures or relationships within the input data to create meaningful training signals. SSL tasks are designed so that solving it requires capturing essential features or relationships in the data. The input data is typically augmented or transformed in a way that creates pairs of related samples. One sample serves as the input, and the other is used to formulate the supervisory signal. This augmentation can involve introducing noise, cropping, rotation, or other transformations. Self-supervised learning more closely imitates the way humans learn to classify objects.The typical SSL method is based on an artificial neural network or other model such as a decision list. The model learns in two steps. First, the task is solved based on an auxiliary or pretext classification task using pseudo-labels which help to initialize the model parameters. Second, the actual task is performed with supervised or unsupervised learning. Other auxiliary tasks involve pattern completion from masked input patterns (silent pauses in speech or image portions masked in black).\nSelf-supervised learning has produced promising results in recent years and has found practical application in audio processing and is being used by Facebook and others for speech recognition."
    },
    {
      "id": "14271782",
      "title": "Semantic analysis (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)",
      "summary": "In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents. A metalanguage based on predicate logic can analyze the speech of humans.:\u200a93-\u200a Another strategy to understand the semantics of a text is symbol grounding. If language is grounded, it is equal to recognizing a machine readable meaning. For the restricted domain of spatial analysis, a computer based language understanding system was demonstrated.:\u200a123\u200aLatent semantic analysis (sometimes latent semantic indexing), is a class of techniques where documents are represented as vectors in term space. A prominent example is PLSI.\nLatent Dirichlet allocation involves attributing document terms to topics.\nn-grams and hidden Markov models work by representing the term stream as a Markov chain where each term is derived from the few terms before it.\n\n"
    },
    {
      "id": "397608",
      "title": "Semantics (computer science)",
      "url": "https://en.wikipedia.org/wiki/Semantics_(computer_science)",
      "summary": "In programming language theory, semantics is the rigorous mathematical study of the meaning of programming languages. Semantics assigns computational meaning to valid strings in a programming language syntax. It is closely related to, and often crosses over with, the semantics of mathematical proofs.\nSemantics describes the processes a computer follows when executing a program in that specific language. This can be done by describing the relationship between the input and output of a program, or giving an explanation of how the program will be executed on a certain platform; hence creating a model of computation.\n\n"
    },
    {
      "id": "60968880",
      "title": "Weak supervision",
      "url": "https://en.wikipedia.org/wiki/Weak_supervision",
      "summary": "Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.  It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning paradigm). In other words, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled or imprecisely labeled. Intuitively, it can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam. Technically, it could be viewed as performing clustering and then labeling the clusters with the labeled data, pushing the decision boundary away from high-density regions, or learning an underlying one-dimensional manifold where the data reside.\n\n"
    },
    {
      "id": "5599330",
      "title": "Sensitivity and specificity",
      "url": "https://en.wikipedia.org/wiki/Sensitivity_and_specificity",
      "summary": "In medicine and statistics, sensitivity and specificity mathematically describe the accuracy of a test that reports the presence or absence of a medical condition. If individuals who have the condition are considered \"positive\" and those who do not are considered \"negative\", then sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives:\n\nSensitivity (true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.\nSpecificity (true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.If the true status of the condition cannot be known, sensitivity and specificity can be defined relative to a \"gold standard test\" which is assumed correct. For all testing, both diagnoses and screening, there is usually a trade-off between sensitivity and specificity, such that higher sensitivities will mean lower specificities and vice versa.\nA test which reliably detects the presence of a condition, resulting in a high number of true positives and low number of false negatives, will have a high sensitivity. This is especially important when the consequence of failing to treat the condition is serious and/or the treatment is very effective and has minimal side effects.\nA test which reliably excludes individuals who do not have the condition, resulting in a high number of true negatives and low number of false positives, will have a high specificity. This is especially important when people who are identified as having a condition may be subjected to more testing, expense, stigma, anxiety, etc.\n\nThe terms \"sensitivity\" and \"specificity\" were introduced by American biostatistician Jacob Yerushalmy in 1947.\nThere are different definitions within laboratory quality control, wherein \"analytical sensitivity\" is defined as  the smallest amount of substance in a sample that can accurately be measured by an assay (synonymously to detection limit), and \"analytical specificity\" is defined as the ability of an assay to measure one particular organism or substance, rather than others. However, this article deals with diagnostic sensitivity and specificity as defined at top.\n\n"
    },
    {
      "id": "143394",
      "title": "Senser",
      "url": "https://en.wikipedia.org/wiki/Senser",
      "summary": "Senser are an English rap rock band, originally formed in South West London from a group of friends in the late 1980s. The initial line-up was called \u2018Senser Element\u2019 and consisted of Nick Michaelson (guitar), Kerstin Haigh (vocals), Steve Morton (drums and beatbox), and James Barrett (bass).  The band played in Steve\u2019s front room in his mums flat in Wimbledon Park. Steve\u2019s mum and dad were classical violinists playing for the Royal Philharmonic Orchestra but his dad was ejected due to his outspoken and rebellious behavior in rehearsals. Steve\u2019s Dad; Garth, introduced Steve and his friends to the music of jazz legends like Miles Davis, Billy Cobham and Mahavishnu Orchestra and psychedelic music from artists like Can, Jimi Hendrix, Hawkwind and Syd Barrett.\nIn the 1980\u2019s Steve Morton and James Barrett were also getting immersed in the London Electro scene which brought a fusion of Funk and Hip Hop sounds from New York. Steve also participated in beatbox battles and received respect for his skills at just 14 years old at the London Electro club \u2018Spatz\u2019. Early influences like Man Parrish, Beastie Boys, N.W.A, West Street Mob, KRS-One, Public Enemy added so much to the burgeoning sound of the band. At the same time Nick was honing his guitar skills to bands like Slayer, Anthrax and Black Sabbath. Kerstin had been a busker, singing 70\u2019s rock music, having scoured through her family's vinyl collection of Led Zeppelin, The Rolling Stones, Jimi Hendrix & Crosby, Stills & Nash. Classical instruments like tabla and learning sitar, from her travels in India drew her to albums like Peter Gabriel\u2019s soundtrack album Passion and singer Sheila Chandra. These diverse and juxtaposed elements were being drawn together and the sound of Senser started to form.\nHeitham Al-Sayed joined initially as a percussionist. What he really wanted to add was rapping, which was received with a mixed response, but Kerstin was positive that this would create something exciting in the music and Heitham started to write rhymes. The band were in Albert Hall Studios when the steps to creating their first album Stacked Up were taken, with an early demo tape recording Music for the Mind and Body that included songs like; \"Journey of Life\" and \"What\u2019s Going On\". \nIn 1988 Steve Morton had to leave due to health issues and John Morgan (drums) joined. \nThe band met Alan \"Hagos/Haggis\" Haggarty (engineer, producer, programmer) at the George Robey Pub in North London and liked his psychedelic treatment of the live sound. The band asked him if he wanted to join as a live producer who would bring the sound to the songs that were being written.\nThe band started to look for a DJ to join and in 1992 they were joined by Spiral Tribe D.J, Andy Clinton. The band toured in support of psychedelic rockers the Ozric Tentacles in 1992. In 1993, the band were signed to Ultimate Records.\n\n"
    },
    {
      "id": "6435232",
      "title": "Sentiment analysis",
      "url": "https://en.wikipedia.org/wiki/Sentiment_analysis",
      "summary": "Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.\n\n"
    },
    {
      "id": "62607005",
      "title": "Seq2seq",
      "url": "https://en.wikipedia.org/wiki/Seq2seq",
      "summary": "Seq2seq is a family of machine learning approaches used for natural language processing. Applications include language translation, image captioning, conversational models, and text summarization.\nSeq2seq uses sequence transformation: it turns one sequence into another sequence.\n\n"
    },
    {
      "id": "37895661",
      "title": "SequenceL",
      "url": "https://en.wikipedia.org/wiki/SequenceL",
      "summary": "SequenceL is a general purpose functional programming language and auto-parallelizing (Parallel computing) compiler and tool set, whose primary design objectives are performance on multi-core processor hardware, ease of programming, platform portability/optimization, and code clarity and readability.  Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without programmers needing to be concerned with identifying parallelisms, specifying vectorization, avoiding race conditions, and other challenges of manual directive-based programming approaches such as OpenMP.\nPrograms written in SequenceL can be compiled to multithreaded code that runs in parallel, with no explicit indications from a programmer of how or what to parallelize. As of 2015, versions of the SequenceL compiler generate parallel code in C++ and OpenCL, which allows it to work with most popular programming languages, including C, C++, C#, Fortran, Java, and Python.  A platform-specific runtime manages the threads safely, automatically providing parallel performance according to the number of cores available, currently supporting x86, POWER8, and ARM platforms.\n\n"
    },
    {
      "id": "4986804",
      "title": "Sequential pattern mining",
      "url": "https://en.wikipedia.org/wiki/Sequential_pattern_mining",
      "summary": "Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence. It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity.  Sequential pattern mining is a special case of structured data mining.\nThere are several key traditional computational problems addressed within this field.  These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members.  In general, sequence mining problems can be classified as string mining which is typically based on string processing algorithms  and itemset mining which is typically based on association rule learning. Local process models  extend sequential pattern mining to more complex patterns that can include (exclusive) choices, loops, and concurrency constructs in addition to the sequential ordering construct."
    },
    {
      "id": "9517150",
      "title": "Shogun (toolbox)",
      "url": "https://en.wikipedia.org/wiki/Shogun_(toolbox)",
      "summary": "Shogun is a free, open-source machine learning software library  written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.\nIt is licensed under the terms of the GNU General Public License version 3 or later."
    },
    {
      "id": "87210",
      "title": "Sigmoid function",
      "url": "https://en.wikipedia.org/wiki/Sigmoid_function",
      "summary": "A sigmoid function is any mathematical function whose graph has a characteristic S-shaped curve or sigmoid curve.\nA common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula:\n\n  \n    \n      \n        \u03c3\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              1\n              +\n              \n                e\n                \n                  \u2212\n                  x\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              e\n              \n                x\n              \n            \n            \n              1\n              +\n              \n                e\n                \n                  x\n                \n              \n            \n          \n        \n        =\n        1\n        \u2212\n        \u03c3\n        (\n        \u2212\n        x\n        )\n        .\n      \n    \n    {\\displaystyle \\sigma (x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{1+e^{x}}}=1-\\sigma (-x).}\n  Other standard sigmoid functions are given in the Examples section. In some fields, most notably in the context of artificial neural networks, the term \"sigmoid function\" is used as an alias for the logistic function.\nSpecial cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams). Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing. Sigmoid functions most often show a return value (y axis) in the range 0 to 1. Another commonly used range is from \u22121 to 1.\nA wide variety of sigmoid functions including the logistic and hyperbolic tangent functions have been used as the activation function of artificial neurons. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic density, the normal density, and Student's t probability density functions. The logistic sigmoid function is invertible, and its inverse is the logit function.\n\n"
    },
    {
      "id": "38059657",
      "title": "Similarity learning",
      "url": "https://en.wikipedia.org/wiki/Similarity_learning",
      "summary": "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.\n\n"
    },
    {
      "id": "49648894",
      "title": "Simulation-based optimization",
      "url": "https://en.wikipedia.org/wiki/Simulation-based_optimization",
      "summary": "Simulation-based optimization (also known as simply simulation optimization) integrates optimization techniques into simulation modeling and analysis. Because of the complexity of the simulation, the objective function may become difficult and expensive to evaluate. Usually, the underlying simulation model is stochastic, so that the objective function must be estimated using statistical estimation techniques (called output analysis in simulation methodology).\nOnce a system is mathematically modeled, computer-based simulations provide information about its behavior. Parametric simulation methods can be used to improve the performance of a system. In this method, the input of each variable is varied with other parameters remaining constant and the effect on the design objective is observed. This is a time-consuming method and improves the performance partially. To obtain the optimal solution with minimum computation and time, the problem is solved iteratively where in each iteration the solution moves closer to the optimum solution. Such methods are known as \u2018numerical optimization\u2019, \u2018simulation-based optimization\u2019 or 'simulation-based multi-objective optimization' used when more than one objective is involved.\nIn simulation experiment, the goal is to evaluate the effect of different values of input variables on a system. However, the interest is sometimes in finding the optimal value for input variables in terms of the system outcomes. One way could be running simulation experiments for all possible input variables. However, this approach is not always practical due to several possible situations and it just makes it intractable to run experiments for each scenario. For example, there might be too many possible values for input variables, or the simulation model might be too complicated and expensive to run for a large set of input variable values. In these cases, the goal is to iterative find optimal values for the input variables rather than trying all possible values. This process is called simulation optimization.Specific simulation\u2013based optimization methods can be chosen according to Figure 1 based on the decision variable types.\nOptimization exists in two main branches of operations research:\nOptimization parametric (static) \u2013 The objective is to find the values of the parameters, which are \u201cstatic\u201d for all states, with the goal of maximizing or minimizing a function. In this case, one can use mathematical programming, such as linear programming. In this scenario, simulation helps when the parameters contain noise or the evaluation of the problem would demand excessive computer time, due to its complexity.Optimization control (dynamic) \u2013 This is used largely in computer science and electrical engineering. The optimal control is per state and the results change in each of them. One can use mathematical programming, as well as dynamic programming. In this scenario, simulation can generate random samples and solve complex and large-scale problems."
    },
    {
      "id": "25765920",
      "title": "Situated approach (artificial intelligence)",
      "url": "https://en.wikipedia.org/wiki/Situated_approach_(artificial_intelligence)",
      "summary": "In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\nThe approach was originally proposed as an alternative to traditional approaches (that is, approaches popular before 1985 or so).\nAfter several decades, classical AI technologies started to face intractable issues (e.g. combinatorial explosion) when confronted with real-world modeling problems. All approaches to address these issues focus on modeling intelligences situated in an environment. They have become known as the situated approach to AI.\n\n"
    },
    {
      "id": "1416993",
      "title": "Social computing",
      "url": "https://en.wikipedia.org/wiki/Social_computing",
      "summary": "Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.   \n\n"
    },
    {
      "id": "34327569",
      "title": "Social network",
      "url": "https://en.wikipedia.org/wiki/Social_network",
      "summary": "A social network is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.\nSocial networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and \"web of group affiliations\". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science."
    },
    {
      "id": "222034",
      "title": "Social software",
      "url": "https://en.wikipedia.org/wiki/Social_software",
      "summary": "Social software, also known as social apps or social platform includes communications and interactive tools that are often based on the Internet. Communication tools typically handle capturing, storing and presenting communication, usually written but increasingly including audio and video as well. Interactive tools handle mediated interactions between a pair or group of users. They focus on establishing and maintaining a connection among users, facilitating the mechanics of conversation and talk. Social software generally refers to software that makes collaborative behaviour, the organisation and moulding of communities, self-expression, social interaction and feedback possible for individuals. Another element of the existing definition of social software is that it allows for the structured mediation of opinion between people, in a centralized or self-regulating manner. The most improved area for social software is that Web 2.0 applications can all promote co-operation between people and the creation of online communities more than ever before. The opportunities offered by social software are instant connections and opportunities to learn.An additional defining feature of social software is that apart from interaction and collaboration, it aggregates the collective behaviour of its users, allowing not only crowds to learn from an individual but individuals to learn from the crowds as well. Hence, the interactions enabled by social software can be one-to-one, one-to-many, or many-to-many."
    },
    {
      "id": "6152185",
      "title": "Softmax function",
      "url": "https://en.wikipedia.org/wiki/Softmax_function",
      "summary": "The softmax function, also known as softargmax:\u200a184\u200a or normalized exponential function,:\u200a198\u200a converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n\n"
    },
    {
      "id": "430106",
      "title": "Software agent",
      "url": "https://en.wikipedia.org/wiki/Software_agent",
      "summary": "In computer science, a software agent is a computer program that acts for a user or another program in a relationship of agency.\nThe term agent is derived from the Latin agere (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate. Some agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or as software such as a chatbot executing on a computer, such as a mobile device, e.g. Siri. Software agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\nRelated and derived concepts include intelligent agents (in particular exhibiting some aspects of artificial intelligence, such as reasoning), autonomous agents (capable of modifying the methods of achieving their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors)."
    },
    {
      "id": "165180",
      "title": "Software configuration management",
      "url": "https://en.wikipedia.org/wiki/Software_configuration_management",
      "summary": "In software engineering, software configuration management (SCM or S/W CM; also expanded as source configuration management process and software change and configuration management) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine the \"what, when, why and who\" of the change.  If a configuration is working well, SCM can determine how to replicate it across many hosts."
    },
    {
      "id": "33408418",
      "title": "Software construction",
      "url": "https://en.wikipedia.org/wiki/Software_construction",
      "summary": "Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.\n\n"
    },
    {
      "id": "1688759",
      "title": "Software deployment",
      "url": "https://en.wikipedia.org/wiki/Software_deployment",
      "summary": "Software deployment is all of the activities that make a software system available for use.The general deployment process consists of several interrelated activities with possible transitions between them. These activities can occur on the producer side or on the consumer side or both. Because every software system is unique, the precise processes or procedures within each activity can hardly be defined. Therefore, \"deployment\" should be interpreted as a general process that has to be customized according to specific requirements or characteristics."
    },
    {
      "id": "223325",
      "title": "Software design",
      "url": "https://en.wikipedia.org/wiki/Software_design",
      "summary": "Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints. The term is sometimes used broadly to refer to \"all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying\" the software, or more specifically \"the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.\"Software design usually involves problem-solving and planning a software solution. This includes both a low-level component and algorithm design and a high-level, architecture design.\n\n"
    },
    {
      "id": "248932",
      "title": "Software development",
      "url": "https://en.wikipedia.org/wiki/Software_development",
      "summary": "Software development is the process used to conceive, specify, design, program, document, test, and bug fix in order to create and maintain applications, frameworks, or other software components. Software development involves writing and maintaining the source code, but in a broader sense, it includes all processes from the conception of the desired software through the final manifestation, typically in a planned and structured process often overlapping with software engineering. Software development also includes research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.\n\n"
    },
    {
      "id": "23407868",
      "title": "Software development process",
      "url": "https://en.wikipedia.org/wiki/Software_development_process",
      "summary": "In software engineering, a software development process or software development life cycle (SDLC) is a process of planning and managing software development. It typically involves dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design and/or product management. The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.\nA life-cycle \"model\" is sometimes considered a more general term for a category of methodologies and a software development \"process\" is a particular instance as adopted by a specific organization. For example, there are many specific software development processes that fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle."
    },
    {
      "id": "768211",
      "title": "Software framework",
      "url": "https://en.wikipedia.org/wiki/Software_framework",
      "summary": "In computer programming, a software framework is an abstraction in which software, providing generic functionality, can be selectively changed by additional user-written code, thus providing application-specific software. It provides a standard way to build and deploy applications and is a universal, reusable software environment that provides particular functionality as part of a larger software platform to facilitate the development of software applications, products and solutions. \nSoftware frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system.\nFrameworks have key distinguishing features that separate them from normal libraries:\n\ninversion of control: In a framework, unlike in libraries or in standard user applications, the overall program's flow of control is not dictated by the caller, but by the framework. This is usually achieved with the Template Method Pattern.\ndefault behaviour: This can be provided with the invariant methods of the Template Method Pattern in an abstract class which is provided by the framework.\nextensibility: A user can extend the framework\u2013usually by selective overriding\u2013or programmers can add specialized user code to provide specific functionality. This is usually achieved by a hook method in a subclass that overrides a template method in the superclass.\nnon-modifiable framework code: The framework code, in general, is not supposed to be modified, while accepting user-implemented extensions. In other words, users can extend the framework, but cannot modify its code."
    },
    {
      "id": "780960",
      "title": "Software maintenance",
      "url": "https://en.wikipedia.org/wiki/Software_maintenance",
      "summary": "Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.A common perception of maintenance is that it merely involves fixing defects. However, one study indicated that over 80% of maintenance effort is used for non-corrective actions. This perception is perpetuated by users submitting problem reports that in reality are functionality enhancements to the system. More recent studies put the bug-fixing proportion closer to 21%."
    },
    {
      "id": "1301906",
      "title": "Software quality",
      "url": "https://en.wikipedia.org/wiki/Software_quality",
      "summary": "In the context of software engineering, software quality refers to two related but distinct notions:\nSoftware's functional quality reflects how well it complies with or conforms to a given design, based on functional requirements or specifications. That attribute can also be described as the fitness for purpose of a piece of software or how it compares to competitors in the marketplace as a worthwhile product. It is the degree to which the correct software was produced.\nSoftware structural quality refers to how it meets non-functional requirements that support the delivery of the functional requirements, such as robustness or maintainability. It has a lot more to do with the degree to which the software works as needed.Many aspects of structural quality can be evaluated only statically through the analysis of the software inner structure, its source code (see Software metrics), at the unit level, and at the system level (sometimes referred to as end-to-end testing), which is in effect how its architecture adheres to sound principles of software architecture outlined in a paper on the topic by Object Management Group (OMG).However some structural qualities, such as usability, can be assessed only dynamically (users or others acting on their behalf interact with the software or, at least, some prototype or partial implementation; even the interaction with a mock version made in cardboard represents a dynamic test because such version can be considered a prototype). Other aspects, such as reliability, might involve not only the software but also the underlying hardware, therefore, it can be assessed both statically and dynamically (stress test).Functional quality is typically assessed dynamically but it is also possible to use static tests (such as software reviews).Historically, the structure, classification and terminology of attributes and metrics applicable to software quality management have been derived or extracted from the ISO 9126 and the subsequent ISO/IEC 25000 standard. Based on these models (see Models), the Consortium for IT Software Quality (CISQ) has defined five major desirable structural characteristics needed for a piece of software to provide business value: Reliability, Efficiency, Security, Maintainability and (adequate) Size.Software quality measurement quantifies to what extent a software program or system rates along each of these five dimensions. An aggregated measure of software quality can be computed through a qualitative or a quantitative scoring scheme or a mix of both and then a weighting system reflecting the priorities. This view of software quality being positioned on a linear continuum is supplemented by the analysis of \"critical programming errors\" that under specific circumstances can lead to catastrophic outages or performance degradations that make a given system unsuitable for use regardless of rating based on aggregated measurements. Such programming errors found at the system level represent up to 90 percent of production issues, whilst at the unit-level, even if far more numerous, programming errors account for less than 10 percent of production issues (see also Ninety\u2013ninety rule). As a consequence, code quality without the context of the whole system, as W. Edwards Deming described it, has limited value.To view, explore, analyze, and communicate software quality measurements, concepts and techniques of information visualization provide visual, interactive means useful, in particular, if several software quality measures have to be related to each other or to components of a software or system. For example, software maps represent a specialized approach that \"can express and combine information about software development, software quality, and system dynamics\".Software quality also plays a role in the release phase of a software project. Specifically, the quality and establishment of the release processes (also patch processes), configuration management are important parts of an overall software engineering process."
    },
    {
      "id": "9570763",
      "title": "Software repository",
      "url": "https://en.wikipedia.org/wiki/Software_repository",
      "summary": "A software repository, or repo for short, is a storage location for software packages. Often a table of contents is also stored, along with metadata. A software repository is typically managed by source or version control, or repository managers. Package managers allow automatically installing and updating repositories, sometimes called \"packages\".\n\n"
    },
    {
      "id": "442656",
      "title": "Software suite",
      "url": "https://en.wikipedia.org/wiki/Software_suite",
      "summary": "A software suite (also known as an application suite) is a collection of computer programs (application software, or programming software) of related functionality, sharing a similar user interface and the ability to easily exchange data with each other."
    },
    {
      "id": "457579",
      "title": "Solid modeling",
      "url": "https://en.wikipedia.org/wiki/Solid_modeling",
      "summary": "Solid modeling (or solid modelling) is a consistent set of principles for mathematical and computer modeling of three-dimensional shapes (solids). Solid modeling is distinguished within the broader related areas of geometric modeling and computer graphics, such as 3D modeling, by its emphasis on physical fidelity. Together, the principles of geometric and solid modeling form the foundation of 3D-computer-aided design and in general support the creation, exchange, visualization, animation, interrogation, and annotation of digital models of physical objects. \n\n"
    },
    {
      "id": "26817320",
      "title": "Soner",
      "url": "https://en.wikipedia.org/wiki/Soner",
      "summary": "Soner may refer to:\n\nSoner Ar\u0131ca (born 1966), Turkish singer and record producer\nSoner Aydo\u011fdu (born 1991), Turkish footballer\nSoner Cagaptay (born 1970), Turkish-American political scientist\nSoner Demirta\u015f (born 1991), Turkish freestyle sport wrestler\nSoner \u00d6zbilen (born 1947), Turkish folk singer, conductor, and compiler\nSoner Uysal (born 1977), Turkish football coach"
    },
    {
      "id": "6147487",
      "title": "Neural coding",
      "url": "https://en.wikipedia.org/wiki/Neural_coding",
      "summary": "Neural coding (or neural representation) is a neuroscience field concerned with characterising the hypothetical relationship between the stimulus and the neuronal responses, and the relationship among the electrical activities of the neurons in the ensemble. Based on the theory that\nsensory and other information is represented in the brain by networks of neurons, it is believed that neurons can encode both digital and analog information."
    },
    {
      "id": "341015",
      "title": "Sparse matrix",
      "url": "https://en.wikipedia.org/wiki/Sparse_matrix",
      "summary": "In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m \u00d7 n for an m \u00d7 n matrix) is sometimes referred to as the sparsity of the matrix.\nConceptually, sparsity corresponds to systems with few pairwise interactions. For example, consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls were to have springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in combinatorics and application areas such as network theory and numerical analysis, which typically have a low density of significant data or connections. Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations.\nWhen storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Specialized computers have been made for sparse matrices, as they are common in the machine learning field. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeros. Sparse data is by nature more easily compressed and thus requires significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms."
    },
    {
      "id": "29468",
      "title": "Speech recognition",
      "url": "https://en.wikipedia.org/wiki/Speech_recognition",
      "summary": "Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\nSome speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker-independent\" systems. Systems that use training are called \"speaker dependent\".\nSpeech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).\nThe term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.\n\n"
    },
    {
      "id": "35891416",
      "title": "SpiNNaker",
      "url": "https://en.wikipedia.org/wiki/SpiNNaker",
      "summary": "SpiNNaker (spiking neural network architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the Department of Computer Science, University of Manchester.  It is composed of 57,600 processing nodes, each with 18 ARM9 processors (specifically ARM968) and 128 MB of mobile DDR SDRAM, totalling 1,036,800 cores and over 7 TB of RAM.  The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).The completed design is housed in 10 19-inch racks, with each rack holding over 100,000 cores. The cards holding the chips are held in 5 blade enclosures, and each core emulates 1,000 neurons.  In total, the goal is to simulate the behaviour of aggregates of up to a billion neurons in real time.  This machine requires about 100 kW from a 240 V supply and an air-conditioned environment.SpiNNaker is being used as one component of the neuromorphic computing platform for the Human Brain Project.On 14 October 2018 the HBP announced that the million core milestone had been achieved.On 24 September 2019 HBP announced that an 8 million euro grant, that will fund construction of the second generation machine, (called SpiNNcloud) has been given to TU Dresden.\n\n"
    },
    {
      "id": "10159567",
      "title": "Spiking neural network",
      "url": "https://en.wikipedia.org/wiki/Spiking_neural_network",
      "summary": "Spiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential\u2014an intrinsic quality of the neuron related to its membrane electrical charge\u2014reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the brain encoded information through spike rates, which can be considered as the analogue variable output of a traditional ANN, research in the field of neurobiology has indicated that high speed processing cannot solely be performed through a rate based scheme. For example humans can perform an image recognition task at rate requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or\u2014if the firing threshold is reached\u2014the neuron fires. After firing, the state variable is reset to a lower value.\nVarious decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.\n\n"
    },
    {
      "id": "19135734",
      "title": "Splunk",
      "url": "https://en.wikipedia.org/wiki/Splunk",
      "summary": "Splunk Inc. is an American software company based in San Francisco, California, that produces software for searching, monitoring, and analyzing machine-generated data via a web-style interface.Its software helps capture, index and correlate real-time data in a searchable repository, from which it can generate graphs, reports, alerts, dashboards and visualizations. Splunk uses machine data for identifying data patterns, providing metrics, diagnosing problems and providing intelligence for business operations. Splunk is a horizontal technology used for application management, security and compliance, as well as business and web analytics.Splunk is due to be acquired by Cisco for $28 billion in an all-cash deal announced in September 2023."
    },
    {
      "id": "47538581",
      "title": "Springer Nature",
      "url": "https://en.wikipedia.org/wiki/Springer_Nature",
      "summary": "Springer Nature or the Springer Nature Group is a German-British academic publishing company created by the May 2015 merger of Springer Science+Business Media and Holtzbrinck Publishing Group's Nature Publishing Group, Palgrave Macmillan, and Macmillan Education.\n\n"
    },
    {
      "id": "1418949",
      "title": "Springer Science+Business Media",
      "url": "https://en.wikipedia.org/wiki/Springer_Science%2BBusiness_Media",
      "summary": "Springer Science+Business Media, commonly known as Springer, is a German multinational publishing company of books, e-books and peer-reviewed journals in science, humanities, technical and medical (STM) publishing.Originally founded in 1842 in Berlin, it expanded internationally in the 1960s, and through mergers in the 1990s and a sale to venture capitalists it fused with Wolters Kluwer and eventually became part of Springer Nature in 2015. Springer has major offices in Berlin, Heidelberg, Dordrecht, and New York City.\n\n"
    },
    {
      "id": "71642695",
      "title": "Stable Diffusion",
      "url": "https://en.wikipedia.org/wiki/Stable_Diffusion",
      "summary": "Stable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. It is considered to be a part of the ongoing AI spring. \nIt is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt. Its development involved  researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a computational donation by Stability AI and training data from non-profit organizations.Stable Diffusion is a latent diffusion model, a kind of deep generative artificial neural network. Its code and model weights have been open sourced, and it can run on most consumer hardware equipped with a modest GPU with at least 4 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services."
    },
    {
      "id": "10584297",
      "title": "State\u2013action\u2013reward\u2013state\u2013action",
      "url": "https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action",
      "summary": "State\u2013action\u2013reward\u2013state\u2013action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name \"Modified Connectionist Q-Learning\" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.\nThis name reflects the fact that the main function for updating the Q-value depends on the current state of the agent \"S1\", the action the agent chooses \"A1\", the reward \"R2\" the agent gets for choosing this action, the state \"S2\" that the agent enters after taking that action, and finally the next action \"A2\" the agent chooses in its new state. The acronym for the quintuple (St, At, Rt+1, St+1, At+1) is SARSA. Some authors use a slightly different convention and write the quintuple (St, At, Rt, St+1, At+1), depending on which time step the reward is formally assigned. The rest of the article uses the former convention.\n\n"
    },
    {
      "id": "1579244",
      "title": "Statistical classification",
      "url": "https://en.wikipedia.org/wiki/Statistical_classification",
      "summary": "In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).\nOften, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\nAn algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.\nTerminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis."
    },
    {
      "id": "38523090",
      "title": "Statistical learning in language acquisition",
      "url": "https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition",
      "summary": "Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in human infant language acquisition.\nThe earliest evidence for these statistical learning abilities comes from a study by Jenny Saffran, Richard Aslin, and Elissa Newport, in which 8-month-old infants were presented with nonsense streams of monotone speech. Each stream was composed of four three-syllable \"pseudowords\" that were repeated randomly. After exposure to the speech streams for two minutes, infants reacted differently to hearing \"pseudowords\" as opposed to \"nonwords\" from the speech stream, where nonwords were composed of the same syllables that the infants had been exposed to, but in a different order. This suggests that infants are able to learn statistical relationships between syllables even with very limited exposure to a language. That is, infants learn which syllables are always paired together and which ones only occur together relatively rarely, suggesting that they are parts of two different units. This method of learning is thought to be one way that children learn which groups of syllables form individual words.Since the initial discovery of the role of statistical learning in lexical acquisition, the same mechanism has been proposed for elements of phonological acquisition, and syntactical acquisition, as well as in non-linguistic domains. Further research has also indicated that statistical learning is likely a domain-general and even species-general learning mechanism, occurring for visual as well as auditory information, and in both primates and non-primates.\n\n"
    },
    {
      "id": "1053303",
      "title": "Statistical learning theory",
      "url": "https://en.wikipedia.org/wiki/Statistical_learning_theory",
      "summary": "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.\n\n"
    },
    {
      "id": "36408395",
      "title": "Statistical manifold",
      "url": "https://en.wikipedia.org/wiki/Statistical_manifold",
      "summary": "In mathematics, a statistical manifold is a Riemannian manifold, each of whose points is a probability distribution.  Statistical manifolds provide a setting for the field of information geometry.  The Fisher information metric provides a metric on these manifolds. Following this definition, the log-likelihood function is a differentiable map and the score is an inclusion.\n\n"
    },
    {
      "id": "2593441",
      "title": "Stephen Grossberg",
      "url": "https://en.wikipedia.org/wiki/Stephen_Grossberg",
      "summary": "Stephen Grossberg (born December 31, 1939) is a cognitive scientist, theoretical and computational psychologist, neuroscientist, mathematician, biomedical engineer, and neuromorphic technologist. He is the Wang Professor of Cognitive and Neural Systems and a Professor Emeritus of Mathematics & Statistics, Psychological & Brain Sciences, and Biomedical Engineering at Boston University."
    },
    {
      "id": "179426",
      "title": "Stevan Harnad",
      "url": "https://en.wikipedia.org/wiki/Stevan_Harnad",
      "summary": "Stevan Robert Harnad (Hern\u00e1d Istv\u00e1n R\u00f3bert, Hesslein Istv\u00e1n, born 1945) is a Canadian cognitive scientist based in Montreal."
    },
    {
      "id": "1180641",
      "title": "Stochastic gradient descent",
      "url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent",
      "summary": "Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.The basic idea behind stochastic approximation can be traced back to the Robbins\u2013Monro algorithm of the 1950s. Today, stochastic gradient descent has become an important optimization method in machine learning."
    },
    {
      "id": "47895",
      "title": "Stochastic process",
      "url": "https://en.wikipedia.org/wiki/Stochastic_process",
      "summary": "In probability theory and related fields, a stochastic () or random process is a mathematical object usually defined as a sequence of random variables in a probability space, where the index of the sequence often has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas molecule. Stochastic processes have applications in many disciplines such as biology, chemistry, ecology, neuroscience, physics, image processing, signal processing, control theory, information theory, computer science, and telecommunications. Furthermore, seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance.Applications and the study of phenomena have in turn inspired the proposal of new stochastic processes. Examples of such stochastic processes include the Wiener process or Brownian motion process, used by Louis Bachelier to study price changes on the Paris Bourse, and the Poisson process, used by A. K. Erlang to study the number of phone calls occurring in a certain period of time. These two stochastic processes are considered the most important and central in the theory of stochastic processes, and were discovered repeatedly and independently, both before and after Bachelier and Erlang, in different settings and countries.The term random function is also used to refer to a stochastic or random process, because a stochastic process can also be interpreted as a random element in a function space. The terms stochastic process and random process are used interchangeably, often with no specific mathematical space for the set that indexes the random variables. But often these two terms are used when the random variables are indexed by the integers or an interval of the real line. If the random variables are indexed by the Cartesian plane or some higher-dimensional Euclidean space, then the collection of random variables is usually called a random field instead. The values of a stochastic process are not always numbers and can be vectors or other mathematical objects.Based on their mathematical properties, stochastic processes can be grouped into various categories, which include random walks, martingales, Markov processes, L\u00e9vy processes, Gaussian processes, random fields, renewal processes, and branching processes. The study of stochastic processes uses mathematical knowledge and techniques from probability, calculus, linear algebra, set theory, and topology as well as branches of mathematical analysis such as real analysis, measure theory, Fourier analysis, and functional analysis. The theory of stochastic processes is considered to be an important contribution to mathematics and it continues to be an active topic of research for both theoretical reasons and applications.\n\n"
    },
    {
      "id": "6319351",
      "title": "Strong NP-completeness",
      "url": "https://en.wikipedia.org/wiki/Strong_NP-completeness",
      "summary": "In computational complexity, strong NP-completeness is a property of computational problems that is a special case of NP-completeness. A general computational problem may have numerical parameters.  For example, the input to the bin packing problem is a list of objects of specific sizes and a size for the bins that must contain the objects\u2014these object sizes and bin size are numerical parameters.\nA problem is said to be strongly NP-complete (NP-complete in the strong sense), if it remains NP-complete even when all of its numerical parameters are bounded by a polynomial in the length of the input. A problem is said to be strongly NP-hard if a strongly NP-complete problem has a polynomial reduction to it; in combinatorial optimization, particularly,  the phrase \"strongly NP-hard\" is reserved for problems that are not known to have a polynomial reduction to another strongly NP-complete problem.\nNormally numerical parameters to a problem are given in positional notation, so a problem of input size n might contain parameters whose size is exponential in n.  If we redefine the problem to have the parameters given in unary notation, then the parameters must be bounded by the input size.  Thus strong NP-completeness or NP-hardness may also be defined as the NP-completeness or NP-hardness of this unary version of the problem.\nFor example, bin packing is strongly NP-complete while the 0-1 Knapsack problem is only weakly NP-complete.  Thus the version of bin packing where the object and bin sizes are integers bounded by a polynomial remains NP-complete, while the corresponding version of the Knapsack problem can be solved in pseudo-polynomial time by dynamic programming.\nFrom a theoretical perspective any strongly NP-hard optimization problem with a polynomially bounded objective function cannot have a fully polynomial-time approximation scheme (or FPTAS) unless P = NP. However, the converse fails: e.g. if P does not equal NP,  knapsack with two constraints is not strongly NP-hard, but has no FPTAS even when the optimal objective is polynomially bounded.Some strongly NP-complete problems may still be easy to solve on average, but it's more likely that difficult instances will be encountered in practice.\n\n"
    },
    {
      "id": "3069503",
      "title": "Structural health monitoring",
      "url": "https://en.wikipedia.org/wiki/Structural_health_monitoring",
      "summary": "Structural health monitoring (SHM) involves the observation and analysis of a system over time using periodically sampled response measurements to monitor changes to the material and geometric properties of engineering structures such as bridges and buildings.\nIn an operational environment, structures degrade with age and use. Long term SHM outputs periodically updated information regarding the ability of the structure to continue performing its intended function. After extreme events, such as earthquakes or blast loading, SHM is used for rapid condition screening. SHM is intended to provide reliable information regarding the integrity of the structure in near real time.The SHM process involves selecting the excitation methods, the sensor types, number and locations, and the data acquisition/storage/transmittal hardware commonly called health and usage monitoring systems. Measurements may be taken to either directly detect any degradation or damage that may occur to a system or indirectly by measuring the size and frequency of loads experienced to allow the state of the system to be predicted.\nTo directly monitor the state of a system it is necessary to identify features in the acquired data that allows one to distinguish between the undamaged and damaged structure. One of the most common feature extraction methods is based on correlating measured system response quantities, such a vibration amplitude or frequency, with observations of the degraded system. Damage accumulation testing, during which significant structural components of the system under study are degraded by subjecting them to realistic loading conditions, can also be used to identify appropriate features. This process may involve induced-damage testing, fatigue testing, corrosion growth, or temperature cycling to accumulate certain types of damage in an accelerated fashion.\n\n"
    },
    {
      "id": "27260435",
      "title": "Structured prediction",
      "url": "https://en.wikipedia.org/wiki/Structured_prediction",
      "summary": "Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used."
    },
    {
      "id": "566689",
      "title": "Stuart J. Russell",
      "url": "https://en.wikipedia.org/wiki/Stuart_J._Russell",
      "summary": "Stuart Jonathan Russell  (born 1962) is a British computer scientist known for his contributions to artificial intelligence (AI). He is a professor of computer science at the University of California, Berkeley and was from 2008 to 2011 an adjunct professor of neurological surgery at the University of California, San Francisco. He holds the Smith-Zadeh Chair in Engineering at University of California, Berkeley. He founded and leads the Center for Human-Compatible Artificial Intelligence (CHAI) at UC Berkeley. Russell is the co-author with Peter Norvig of the authoritative textbook of the field of AI: Artificial Intelligence: A Modern Approach used in more than 1,500 universities in 135 countries."
    },
    {
      "id": "26980",
      "title": "Sun Microsystems",
      "url": "https://en.wikipedia.org/wiki/Sun_Microsystems",
      "summary": "Sun Microsystems, Inc. (Sun for short) was an American technology company that sold computers, computer components, software, and information technology services and created the Java programming language, the Solaris operating system, ZFS, the Network File System (NFS), and SPARC microprocessors. Sun contributed significantly to the evolution of several key computing technologies, among them Unix, RISC processors, thin client computing, and virtualized computing. Notable Sun acquisitions include Cray Business Systems Division, Storagetek, and Innotek GmbH, creators of VirtualBox.  Sun was founded on February 24, 1982. At its height, the Sun headquarters were in Santa Clara, California (part of Silicon Valley), on the former west campus of the Agnews Developmental Center.\nSun products included computer servers and workstations built on its own RISC-based SPARC processor architecture, as well as on x86-based AMD Opteron and Intel Xeon processors. Sun also developed its own storage systems and a suite of software products, including the Solaris operating system, developer tools, Web infrastructure software, and identity management applications. Technologies included the Java platform and NFS.\nIn general, Sun was a proponent of open systems, particularly Unix. It was also a major contributor to open-source software, as evidenced by its $1 billion purchase, in 2008, of MySQL, an open-source relational database management system.At various times, Sun had manufacturing facilities in several locations worldwide, including Newark, California; Hillsboro, Oregon; and Linlithgow, Scotland. However, by the time the company was acquired by Oracle Corporation, it had outsourced most manufacturing responsibilities.\nOn April 20, 2009, it was announced that Oracle would acquire Sun for US$7.4 billion. The deal was completed on January 27, 2010.\n\n"
    },
    {
      "id": "20926",
      "title": "Supervised learning",
      "url": "https://en.wikipedia.org/wiki/Supervised_learning",
      "summary": "Supervised learning (SL) is a paradigm in machine learning where input objects (for example, a vector of predictor variables) and a desired output value (also known as human-labeled supervisory signal) train a model. The training data is processed, building a function that maps new data on expected output values.  An optimal scenario will allow for the algorithm to correctly determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.\n\n"
    },
    {
      "id": "65309",
      "title": "Support vector machine",
      "url": "https://en.wikipedia.org/wiki/Support_vector_machine",
      "summary": "In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most studied models, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). \nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. SVMs can also be used for regression tasks, where the objective becomes \n  \n    \n      \n        \u03f5\n        \u2212\n      \n    \n    {\\displaystyle \\epsilon -}\n  sensitive.\nThe support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data to groups and, then, to map new data according to these clusters. \nThe popularity of SVMs is likely due to their amenability to theoretical analysis, their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression."
    },
    {
      "id": "762988",
      "title": "Swarm intelligence",
      "url": "https://en.wikipedia.org/wiki/Swarm_intelligence",
      "summary": "Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of \"intelligent\" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include ant colonies, bee colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence.\nThe application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. Swarm prediction has been used in the context of forecasting problems. Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence.\n\n"
    },
    {
      "id": "339417",
      "title": "Symbolic artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
      "summary": "In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\nSymbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the First AI Winter as funding dried up. A second boom (1969\u20131986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988\u20132011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.Neural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning."
    },
    {
      "id": "681654",
      "title": "Snaps",
      "url": "https://en.wikipedia.org/wiki/Snaps",
      "summary": "In Denmark and Sweden, snaps (pronounced [\u02c8snaps] in Danish and Swedish) is a small shot of a strong alcoholic beverage taken during the course of a meal.\nIn Denmark, a snaps will always be akvavit, although there are many varieties of it. In Sweden, snaps is a more general term; it is usually akvavit, although it may also be vodka, bitters/bitter liqueurs or some other kind of br\u00e4nnvin/br\u00e6ndevin. Spirits such as whisky or brandy are seldom drunk as snaps. One of Finland's strongest alcohol drinks served with snaps is Marskin ryyppy, named after Marshal C. G. E. Mannerheim.The word \"snaps\" also has the same meaning as German Schnapps (German: [\u0283naps]), in the sense of \"any strong alcoholic drink\"."
    },
    {
      "id": "6069850",
      "title": "Syntactic pattern recognition",
      "url": "https://en.wikipedia.org/wiki/Syntactic_pattern_recognition",
      "summary": "Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.\nSyntactic pattern recognition can be used instead of statistical pattern recognition if there is clear structure in the patterns. One way to present such structure is by means of a strings of symbols from a formal language. In this case the differences in the structures of the classes are encoded as different grammars.\nAn example of this would be diagnosis of the heart with ECG measurements. ECG waveforms can be approximated with diagonal and vertical line segments. If normal and unhealthy waveforms can be described as formal grammars, measured ECG signal can be classified as healthy or unhealthy by first describing it in term of the basic line segments and then trying to parse the descriptions according to the grammars. Another example is tessellation of tiling patterns.\nA second way to represent relations are graphs, where nodes are connected if corresponding subpatterns are related. An item can be labeled as belonging to a class if its graph representation is isomorphic with prototype graphs of the class.\nTypically, patterns are constructed from simpler sub patterns in a hierarchical fashion. This helps in dividing the recognition task into easier subtask of first identifying sub patterns and only then the actual patterns.\nStructural methods provide descriptions of items, which may be useful in their own right. For example, syntactic pattern recognition can be used to find out what objects are present in an image. Furthermore, structural methods are strong in finding a correspondence mapping between two images of an object. Under natural conditions, corresponding features will be in different positions and/or may be occluded in the two images, due to camera-attitude and perspective, as in face recognition. A graph matching algorithm will yield the optimal correspondence."
    },
    {
      "id": "100563",
      "title": "System on a chip",
      "url": "https://en.wikipedia.org/wiki/System_on_a_chip",
      "summary": "A system on a chip or system-on-chip (SoC ; pl. SoCs ) is an integrated circuit that integrates most or all components of a computer or other electronic system. These components almost always include on-chip central processing unit (CPU), memory interfaces, input/output devices and interfaces, and secondary storage interfaces, often alongside other components such as radio modems and a graphics processing unit (GPU) \u2013 all on a single substrate or microchip. SoCs may contain digital and also analog, mixed-signal and often radio frequency signal processing functions (otherwise it may be considered on a discrete application processor).\nHigher-performance SoCs are often paired with dedicated and physically separate memory and secondary storage (such as LPDDR and eUFS or eMMC, respectively) chips, that may be layered on top of the SoC in what's known as a package on package (PoP) configuration, or be placed close to the SoC. Additionally, SoCs may use separate wireless modems.An SoC integrates a microcontroller, microprocessor or perhaps several processor cores with peripherals like a GPU, Wi-Fi and cellular network radio modems, and/or one or more coprocessors. Similar to how a microcontroller integrates a microprocessor with peripheral circuits and memory, an SoC can be seen as integrating a microcontroller with even more advanced peripherals. For an overview of integrating system components, see system integration.\nCompared to a multi-chip architecture, an SoC with equivalent functionality will have reduced power consumption as well as a smaller semiconductor die area. This comes at the cost of reduced replaceability of components. By definition, SoC designs are fully or nearly fully integrated across different component modules. For these reasons, there has been a general trend towards tighter integration of components in the computer hardware industry, in part due to the influence of SoCs and lessons learned from the mobile and embedded computing markets. \nSoCs are very common in the mobile computing (as in smart devices such as smartphones and tablet computers) and edge computing markets.\n\n"
    },
    {
      "id": "39758474",
      "title": "T-distributed stochastic neighbor embedding",
      "url": "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding",
      "summary": "t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Geoffrey Hinton and Sam Roweis, where Laurens van der Maaten proposed the t-distributed variant. It is a nonlinear dimensionality reduction technique for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\nThe t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback\u2013Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate. A Riemannian variant is UMAP.\nt-SNE has been used for visualization in a wide range of applications, including genomics, computer security research, natural language processing, music analysis, cancer research, bioinformatics, geological domain interpretation, and biomedical signal processing.While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such \"clusters\" can be shown to even appear in non-clustered data, and thus may be false findings. Interactive exploration may thus be necessary to choose parameters and validate results. It has been demonstrated that t-SNE is often able to recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.For a data set with n elements, t-SNE runs in O(n2) time and requires O(n2) space.\n\n"
    },
    {
      "id": "33094374",
      "title": "Telecommunications",
      "url": "https://en.wikipedia.org/wiki/Telecommunications",
      "summary": "Telecommunication, often used in its plural form, is the transmission of information by various types of technologies over wire, radio, optical, or other electromagnetic systems. It has its origin in the desire of humans for communication over a distance greater than that feasible with the human voice, but with a similar scale of expediency; thus, slow systems (such as postal mail and  pneumatic tube communication systems) are excluded from the field.\nThe transmission media in telecommunication have evolved through numerous stages of technology, from beacons and other visual signals (such as smoke signals, semaphore telegraphs, signal flags, and optical heliographs), to electrical cable and electromagnetic radiation, including light. Such transmission paths are often divided into communication channels, which afford the advantages of multiplexing multiple concurrent communication sessions.\nOther examples of pre-modern long-distance communication included audio messages, such as coded drumbeats, lung-blown horns, and loud whistles. 20th- and 21st-century technologies for long-distance communication usually involve electrical and electromagnetic technologies, such as telegraph, telephone, television and teleprinter, networks, radio, microwave transmission, optical fibre, and communications satellites.\nThe early telecommunication networks were created with metallic wires as the physical medium for signal transmission. For many years, these networks were used for telegraph and voice services. A revolution in wireless communication began in the first decade of the 20th century with the pioneering developments in radio communications by Guglielmo Marconi, who won the Nobel Prize in Physics in 1909, and other notable pioneering inventors and developers in the field of electrical and electronic telecommunications. These included Charles Wheatstone and Samuel Morse (inventors of the telegraph), Antonio Meucci and Alexander Graham Bell (some of the inventors and developers of the telephone, see Invention of the telephone), Edwin Armstrong and Lee de Forest (inventors of radio), as well as Vladimir K. Zworykin, John Logie Baird and Philo Farnsworth (some of the inventors of television).\nWith the proliferation of digital technologies since the 1960s, voice communication has been gradually supplemented by data. The limitations of metallic data transmission prompted the development of optics. The development of media-independent Internet technologies provided access to world-wide services for individual users without limitations to location or time."
    },
    {
      "id": "1209759",
      "title": "Temporal difference learning",
      "url": "https://en.wikipedia.org/wiki/Temporal_difference_learning",
      "summary": "Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of bootstrapping, as illustrated with the following example:\n\nSuppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday \u2013 and thus be able to change, say, Saturday's model before Saturday arrives.\nTemporal difference methods are related to the temporal difference model of animal learning.\n\n"
    },
    {
      "id": "486876",
      "title": "Omeprazole",
      "url": "https://en.wikipedia.org/wiki/Omeprazole",
      "summary": "Omeprazole, sold under the brand names Prilosec and Losec, among others, is a medication used in the treatment of gastroesophageal reflux disease (GERD), peptic ulcer disease, and Zollinger\u2013Ellison syndrome. It is also used to prevent upper gastrointestinal bleeding in people who are at high risk. Omeprazole is a proton-pump inhibitor (PPI) and its effectiveness is similar to that of other PPIs. It can be taken by mouth or by injection into a vein. It is also available in the fixed-dose combination medication omeprazole/sodium bicarbonate as Zegerid and as Konvomep.Common side effects include nausea, vomiting, headaches, abdominal pain, and increased intestinal gas. Serious side effects may include Clostridium difficile colitis, an increased risk of pneumonia, an increased risk of bone fractures, and the potential of masking stomach cancer. Whether it is safe for use in pregnancy is unclear. It works by blocking the release of stomach acid.Omeprazole was patented in 1978, and approved for medical use in 1988. It is on the World Health Organization's List of Essential Medicines. It is available as a generic medication. In 2021, it was the eighth-most commonly prescribed medication in the United States, with more than 54 million prescriptions. It is also available without a prescription in the United States.\n\n"
    },
    {
      "id": "50673241",
      "title": "Tensor Processing Unit",
      "url": "https://en.wikipedia.org/wiki/Tensor_Processing_Unit",
      "summary": "Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google for neural network machine learning, using Google's own TensorFlow software. Google began using TPUs internally in 2015, and in 2018 made them available for third-party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.\n\n"
    },
    {
      "id": "499010",
      "title": "Tensor calculus",
      "url": "https://en.wikipedia.org/wiki/Tensor_calculus",
      "summary": "In mathematics, tensor calculus, tensor analysis, or Ricci calculus is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).\nDeveloped by Gregorio Ricci-Curbastro and his student Tullio Levi-Civita, it was used by Albert Einstein to develop his general theory of relativity. Unlike the infinitesimal calculus, tensor calculus allows presentation of physics equations in a form that is independent of the choice of coordinates on the manifold.\nTensor calculus has many applications in physics, engineering and computer science including elasticity, continuum mechanics, electromagnetism (see mathematical descriptions of the electromagnetic field), general relativity (see mathematics of general relativity), quantum field theory, and machine learning.\n\nWorking with a main proponent of the exterior calculus Elie Cartan, the influential geometer Shiing-Shen Chern summarizes the role of tensor calculus:In our subject of differential geometry, where you talk about manifolds, one difficulty is that the geometry is described by coordinates, but the coordinates do not have meaning. They are allowed to undergo transformation. And in order to handle this kind of situation, an important tool is the so-called tensor analysis, or Ricci calculus, which was new to mathematicians. In mathematics you have a function, you write down the function, you calculate, or you add, or you multiply, or you can differentiate. You have something very concrete. In geometry the geometric situation is described by numbers, but you can change your numbers arbitrarily. So to handle this, you need the Ricci calculus.\n\n"
    },
    {
      "id": "1514392",
      "title": "Training, validation, and test data sets",
      "url": "https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets",
      "summary": "In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided into multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation, and test sets.\nThe model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\nSuccessively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units\u2014layers and layer widths\u2014in a neural network). Validation data sets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).\nThis simple procedure is complicated in practice by the fact that the validation data set's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term \"validation set\" is sometimes used instead of \"test set\" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.\n\n"
    },
    {
      "id": "47937215",
      "title": "The Master Algorithm",
      "url": "https://en.wikipedia.org/wiki/The_Master_Algorithm",
      "summary": "The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field."
    },
    {
      "id": "47089",
      "title": "The Register",
      "url": "https://en.wikipedia.org/wiki/The_Register",
      "summary": "The Register is a British technology news website co-founded in 1994 by Mike Magee and John Lettice. The online newspaper's masthead sublogo is \"Biting the hand that feeds IT.\" The publication's primary focus is information technology news and opinions.\nSituation Publishing Ltd is the site's publisher. Drew Cullen is an owner and Linus Birtles is the managing director. Andrew Orlowski was the executive editor before leaving the website in May 2019."
    },
    {
      "id": "173070",
      "title": "The Wall Street Journal",
      "url": "https://en.wikipedia.org/wiki/The_Wall_Street_Journal",
      "summary": "The Wall Street Journal (WSJ) is an American business and economic-focused international daily newspaper based in New York City. The Journal is published six days a week by Dow Jones & Company, a division of News Corp. The newspaper is published in broadsheet format and online. The Journal has been printed continuously since its inception on July 8, 1889, and is regarded as a newspaper of record, particularly in terms of business and financial news. The newspaper has won 39 Pulitzer Prizes, the most recent in 2023.The Wall Street Journal is the second-largest newspaper in the United States by circulation, with a print circulation of around 654,000 and 3 million digital subscribers as of 2022. The Journal publishes the luxury news and lifestyle magazine WSJ, which was originally launched as a quarterly but expanded to 12 issues in 2014. An online version was launched in 1995, which has been accessible only to subscribers since it began. The editorial pages of the Journal are typically center-right in their positions, while the newspaper itself maintains commitment to journalistic standards in its reporting.\n\n"
    },
    {
      "id": "33520809",
      "title": "Theano (software)",
      "url": "https://en.wikipedia.org/wiki/Theano_(software)",
      "summary": "Theano is a Python library and optimizing compiler for manipulating and evaluating mathematical expressions, especially matrix-valued ones.\nIn Theano, computations are expressed using a NumPy-esque syntax and compiled to run efficiently on either CPU or GPU architectures.\n\n"
    },
    {
      "id": "323392",
      "title": "Theoretical computer science",
      "url": "https://en.wikipedia.org/wiki/Theoretical_computer_science",
      "summary": "Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, formal language theory, the lambda calculus and type theory.\nIt is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:\nTCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor."
    },
    {
      "id": "30402",
      "title": "Theory of computation",
      "url": "https://en.wikipedia.org/wiki/Theory_of_computation",
      "summary": "In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: \"What are the fundamental capabilities and limitations of computers?\".In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation. There are several models in use, but the most commonly examined is the Turing machine. Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible \"reasonable\" model of computation (see Church\u2013Turing thesis). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.\n\n"
    },
    {
      "id": "31600",
      "title": "Time (magazine)",
      "url": "https://en.wikipedia.org/wiki/Time_(magazine)",
      "summary": "Time (stylized in all caps as TIME) is an American news magazine based in New York City. It was published weekly for nearly a century. Starting in March 2020, it transitioned to every other week. It was first published in New York City on March 3, 1923, and for many years it was run by its influential co-founder, Henry Luce.\nA European edition (Time Europe, formerly known as Time Atlantic) is published in London and also covers the Middle East, Africa, and, since 2003, Latin America. An Asian edition (Time Asia) is based in Hong Kong. The South Pacific edition, which covers Australia, New Zealand, and the Pacific Islands, is based in Sydney.\nSince 2018, Time has been owned by Salesforce founder Marc Benioff, who acquired it from Meredith Corporation. Benioff currently publishes the magazine through the company Time USA, LLC.\n\n"
    },
    {
      "id": "405944",
      "title": "Time complexity",
      "url": "https://en.wikipedia.org/wiki/Time_complexity",
      "summary": "In theoretical computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.\nSince an algorithm's running time may vary among different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the average-case complexity, which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a function of the size of the input.:\u200a226\u200a Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases\u2014that is, the asymptotic behavior of the complexity. Therefore, the time complexity is commonly expressed using big O notation, typically \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  , \n  \n    \n      \n        O\n        (\n        n\n        log\n        \u2061\n        n\n        )\n      \n    \n    {\\displaystyle O(n\\log n)}\n  , \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            \u03b1\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{\\alpha })}\n  , \n  \n    \n      \n        O\n        (\n        \n          2\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle O(2^{n})}\n  , etc., where n is the size in units of bits needed to represent the input.\nAlgorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity \n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n   is a linear time algorithm and an algorithm with time complexity \n  \n    \n      \n        O\n        (\n        \n          n\n          \n            \u03b1\n          \n        \n        )\n      \n    \n    {\\displaystyle O(n^{\\alpha })}\n   for some constant \n  \n    \n      \n        \u03b1\n        >\n        1\n      \n    \n    {\\displaystyle \\alpha >1}\n   is a polynomial time algorithm."
    },
    {
      "id": "406624",
      "title": "Time series",
      "url": "https://en.wikipedia.org/wiki/Time_series",
      "summary": "In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\nA time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\nTime series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called \"time series analysis\", which refers in particular to relationships between different points in time within a single series.\nTime series data have a natural temporal ordering.  This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order).  Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility).\nTime series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).\n\n"
    },
    {
      "id": "12413470",
      "title": "Timeline of artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence",
      "summary": "This is a timeline of artificial intelligence, sometimes alternatively called synthetic intelligence.\n\n"
    },
    {
      "id": "50828755",
      "title": "Timeline of machine learning",
      "url": "https://en.wikipedia.org/wiki/Timeline_of_machine_learning",
      "summary": "This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included."
    },
    {
      "id": "33275304",
      "title": "Tom M. Mitchell",
      "url": "https://en.wikipedia.org/wiki/Tom_M._Mitchell",
      "summary": "Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n\n"
    },
    {
      "id": "25757480",
      "title": "Tomasz Imieli\u0144ski",
      "url": "https://en.wikipedia.org/wiki/Tomasz_Imieli%C5%84ski",
      "summary": "Tomasz Imieli\u0144ski (born July 11, 1954, in Toru\u0144, Poland) is a Polish-American computer scientist, most known in the areas of data mining, mobile computing, data extraction, and search engine technology. He is currently a professor of computer science at Rutgers University in New Jersey, United States.\nIn 2000, he co-founded Connotate Technologies, a web data extraction company based in New Brunswick, NJ. Since 2004 till 2010 he had held multiple positions at Ask.com, from vice president of data solutions intelligence to executive vice president of global search and answers and chief scientist. From 2010 to 2012 he served as VP of data solutions at IAC/Pronto.Tomasz Imieli\u0144ski served as chairman of the Computer Science Department at Rutgers University from 1996 to 2003.\nHe co-founded, with Celina Imieli\u0144ska and Konrad Imieli\u0144ski Art Data Laboratories LLC company, and its product, Articker is the largest known database that aggregates dynamic non-price information about the visual artists in the global art market. Articker has been under an exclusive partnership with Phillips auction house."
    },
    {
      "id": "995908",
      "title": "Tomographic reconstruction",
      "url": "https://en.wikipedia.org/wiki/Tomographic_reconstruction",
      "summary": "Tomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.This article applies in general to reconstruction methods for all kinds of tomography, but some of the terms and physical descriptions refer directly to the reconstruction of X-ray computed tomography.\n\n"
    },
    {
      "id": "28934119",
      "title": "Topic model",
      "url": "https://en.wikipedia.org/wiki/Topic_model",
      "summary": "In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\nTopic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics and computer vision.\n\n"
    },
    {
      "id": "42571226",
      "title": "Torch (machine learning)",
      "url": "https://en.wikipedia.org/wiki/Torch_(machine_learning)",
      "summary": "Torch is an open-source machine learning library, \na scientific computing framework, and a scripting language based on Lua. It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created by the Idiap Research Institute at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python.\n\n"
    },
    {
      "id": "68639524",
      "title": "Toronto Declaration",
      "url": "https://en.wikipedia.org/wiki/Toronto_Declaration",
      "summary": "The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation. It was published at RightsCon on May 16, 2018.The Declaration focuses on concerns of algorithmic bias and the potential for discrimination that arises from the use of machine learning and artificial intelligence in applications that may affect people's lives, \"from policing, to welfare systems, to healthcare provision, to platforms for online discourse.\" A secondary concern of the document is the potential for violations of information privacy.\nThe goal of the Declaration is to outline \"tangible and actionable standards for states and the private sector.\" The Declaration calls for tangible solutions, such as reparations for the victims of algorithmic discrimination."
    },
    {
      "id": "171623",
      "title": "Training Day",
      "url": "https://en.wikipedia.org/wiki/Training_Day",
      "summary": "Training Day is a 2001 American crime thriller film directed by Antoine Fuqua and written by David Ayer. It stars Denzel Washington as Alonzo Harris and Ethan Hawke as Jake Hoyt, two LAPD narcotics officers over a 24-hour period in the gang-ridden neighborhoods of Westlake, Echo Park, and South Central Los Angeles. It also features Scott Glenn, Eva Mendes, Cliff Curtis, Dr. Dre, Snoop Dogg, and Macy Gray in supporting roles.\nTraining Day was released on October 5, 2001, by Warner Bros. Pictures. It received generally positive reviews from critics, who praised Washington and Hawke's performances but were divided on the screenplay. It was a commercial success, grossing $104 million worldwide against a production budget of $45 million.\nThe film received numerous accolades and nominations, with Washington's performance earning him the Academy Award for Best Actor and Hawke being nominated for Best Supporting Actor at the 74th Academy Awards.A television series based on the film, produced by Jerry Bruckheimer, was announced in August 2015 and premiered on February 2, 2017, on CBS, but was cancelled after one season. \nOn February 28, 2022, a prequel titled Training Day: Day of the Riot was reported to be in development; in an interview, Fuqua said, \u201cwe\u2019re trying to find a director to do it at the moment, you know, and get the script right. That sort of thing. So it\u2019s in development.\u201d"
    },
    {
      "id": "61603971",
      "title": "Transformer (deep learning architecture)",
      "url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)",
      "summary": "A transformer is a deep learning architecture based on the multi-head attention mechanism, proposed in a 2017 paper \"Attention Is All You Need\". It has no recurrent units, and thus requires less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\nInput text is split into n-grams encoded as tokens and each token is converted into a vector via looking up from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished. The transformer paper, published in 2017, is based on the softmax-based attention mechanism was proposed by Bahdanau et. al. in 2014 for machine translation, and the Fast Weight Controller, similar to a transformer, was proposed in 1992.\nThis architecture is now used not only in natural language processing and computer vision, but also in audio and multi-modal processing. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (Bidirectional Encoder Representations from Transformers).\n\n"
    },
    {
      "id": "35425965",
      "title": "Trevor Hastie",
      "url": "https://en.wikipedia.org/wiki/Trevor_Hastie",
      "summary": "Trevor John Hastie (born 27 June 1953) is an American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at Stanford University. Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge."
    },
    {
      "id": "57179040",
      "title": "U-Net",
      "url": "https://en.wikipedia.org/wiki/U-Net",
      "summary": "U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg. The network is based on a fully convolutional neural network whose architecture was modified and extended to work with fewer training images and to yield more precise segmentation. Segmentation of a 512 \u00d7 512 image takes less than a second on a modern GPU.\nThe U-Net architecture has also been employed in diffusion models for iterative image denoising. This technology underlies many modern image generation models, such as DALL-E, Midjourney, and Stable Diffusion."
    },
    {
      "id": "31871",
      "title": "Ubiquitous computing",
      "url": "https://en.wikipedia.org/wiki/Ubiquitous_computing",
      "summary": "Ubiquitous computing (or \"ubicomp\") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets, smart phones and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials.\nThis paradigm is also described as pervasive computing, ambient intelligence, or \"everyware\". Each term emphasizes slightly different aspects. When primarily concerning the objects involved, it is also known as physical computing, the Internet of Things, haptic computing, and \"things that think\".\nRather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described.Ubiquitous computing themes include: distributed computing, mobile computing, location computing, mobile networking, sensor networks, human\u2013computer interaction, context-aware smart home technologies, and artificial intelligence."
    },
    {
      "id": "5987648",
      "title": "Uncertainty quantification",
      "url": "https://en.wikipedia.org/wiki/Uncertainty_quantification",
      "summary": "Uncertainty quantification (UQ) is the science of quantitative characterization and estimation of uncertainties in both computational and real world applications. It tries to determine how likely certain outcomes are if some aspects of the system are not exactly known. An example would be to predict the acceleration of a human body in a head-on crash with another car: even if the speed was exactly known, small differences in the manufacturing of individual cars, how tightly every bolt has been tightened, etc., will lead to different results that can only be predicted in a statistical sense.\nMany problems in the natural sciences and engineering are also rife with sources of uncertainty. Computer experiments on computer simulations are the most common approach to study problems in uncertainty quantification.\n\n"
    },
    {
      "id": "305843",
      "title": "Vapnik\u2013Chervonenkis theory",
      "url": "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory",
      "summary": "Vapnik\u2013Chervonenkis theory (also known as VC theory) was developed during 1960\u20131990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view."
    },
    {
      "id": "62078649",
      "title": "Variational autoencoder",
      "url": "https://en.wikipedia.org/wiki/Variational_autoencoder",
      "summary": "In machine learning, a variational autoencoder (VAE) is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling. It is part of the families of probabilistic graphical models and variational Bayesian methods.Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders are probabilistic generative models that require neural networks as only a part of their overall structure. The neural network components are typically referred to as the encoder and decoder for the first and second component respectively. The first neural network maps the input variable to a latent space that corresponds to the parameters of a variational distribution. In this way, the encoder can produce multiple different samples that all come from the same distribution. The decoder has the opposite function, which is to map from the latent space to the input space, in order to produce or generate data points. Both networks are typically trained together with the usage of the reparameterization trick, although the variance of the noise model can be learned separately.\nAlthough this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning.\n\n"
    },
    {
      "id": "32823",
      "title": "Very Large Scale Integration",
      "url": "https://en.wikipedia.org/wiki/Very_Large_Scale_Integration",
      "summary": "Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions or billions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were developed and then widely adopted, enabling complex semiconductor and telecommunication technologies. The microprocessor and memory chips are VLSI devices.\nBefore the introduction of VLSI technology, most ICs had a limited set of functions they could perform. An electronic circuit might consist of a CPU, ROM, RAM and other glue logic. VLSI enables IC designers to add all of these into one chip.\n\n"
    },
    {
      "id": "5363",
      "title": "Video game",
      "url": "https://en.wikipedia.org/wiki/Video_game",
      "summary": "A video game or computer game is an electronic game that involves interaction with a user interface or input device (such as a joystick, controller, keyboard, or motion sensing device) to generate visual feedback from a display device, most commonly shown in a video format on a television set, computer monitor, flat-panel display or touchscreen on handheld devices, or a virtual reality headset. Most modern video games are audiovisual, with audio complement delivered through speakers or headphones, and sometimes also with other types of sensory feedback (e.g., haptic technology that provides tactile sensations). Some video games also allow microphone and webcam inputs for in-game chatting and livestreaming.\nVideo games are typically categorized according to their hardware platform, which traditionally includes arcade video games, console games, and computer (PC) games; the latter also encompasses LAN games, online games, and browser games. More recently, the video game industry has expanded onto mobile gaming through mobile devices (such as smartphones and tablet computers), virtual and augmented reality systems, and remote cloud gaming. Video games are also classified into a wide range of genres based on their style of gameplay and target audience.\nThe first video game prototypes in the 1950s and 1960s were simple extensions of electronic games using video-like output from large, room-sized mainframe computers. The first consumer video game was the arcade video game Computer Space in 1971. In 1972 came the iconic hit game Pong and the first home console, the Magnavox Odyssey. The industry grew quickly during the \"golden age\" of arcade video games from the late 1970s to early 1980s but suffered from the crash of the North American video game market in 1983 due to loss of publishing control and saturation of the market. Following the crash, the industry matured, was dominated by Japanese companies such as Nintendo, Sega, and Sony, and established practices and methods around the development and distribution of video games to prevent a similar crash in the future, many of which continue to be followed. In the 2000s, the core industry centered on \"AAA\" games, leaving little room for riskier experimental games. Coupled with the availability of the Internet and digital distribution, this gave room for independent video game development (or \"indie games\") to gain prominence into the 2010s. Since then, the commercial importance of the video game industry has been increasing. The emerging Asian markets and proliferation of smartphone games in particular are altering player demographics towards casual gaming and increasing monetization by incorporating games as a service.\nToday, video game development requires numerous interdisciplinary skills, vision, teamwork, and liaisons between different parties, including developers, publishers, distributors, retailers, hardware manufacturers, and other marketers, to successfully bring a game to its consumers. As of 2020, the global video game market had estimated annual revenues of US$159 billion across hardware, software, and services, which is three times the size of the global music industry and four times that of the film industry in 2019, making it a formidable heavyweight across the modern entertainment industry. The video game market is also a major influence behind the electronics industry, where personal computer component, console, and peripheral sales, as well as consumer demands for better game performance, have been powerful driving factors for hardware design and innovation."
    },
    {
      "id": "226779",
      "title": "Vinod Khosla",
      "url": "https://en.wikipedia.org/wiki/Vinod_Khosla",
      "summary": "Vinod Khosla (born 28 January 1955) is an Indian-American businessman and venture capitalist. He is a co-founder of Sun Microsystems and the founder of Khosla Ventures. Khosla made his wealth from early venture capital investments in areas such as networking, software, and alternative energy technologies. He is considered one of the most successful and influential venture capitalists.In 2014, Forbes counted him among the 400 richest people in the United States. In 2021, he was ranked 92nd on the Forbes 400 list.\n\n"
    },
    {
      "id": "50228744",
      "title": "Vision processing unit",
      "url": "https://en.wikipedia.org/wiki/Vision_processing_unit",
      "summary": "A vision processing unit (VPU) is (as of 2023) an emerging class of microprocessor; it is a specific type of AI accelerator, designed to accelerate machine vision tasks."
    },
    {
      "id": "68212199",
      "title": "Vision transformer",
      "url": "https://en.wikipedia.org/wiki/Vision_transformer",
      "summary": "A vision transformer (ViT) is a transformer designed for computer vision. A ViT breaks down an input image into a series of patches (rather than breaking up text into tokens), serialises each patch into a vector, and maps it to a smaller dimension with a single matrix multiplication. These vector embeddings are then processed by a transformer encoder as if they were token embeddings.\nViT has found applications in image recognition, image segmentation, and autonomous driving.\n\n"
    },
    {
      "id": "660850",
      "title": "Visualization (graphics)",
      "url": "https://en.wikipedia.org/wiki/Visualization_(graphics)",
      "summary": "Visualization or visualisation (see spelling differences) is any technique for creating images, diagrams, or animations to communicate a message. Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of humanity. from history include cave paintings, Egyptian hieroglyphs, Greek geometry, and Leonardo da Vinci's revolutionary methods of technical drawing for engineering and scientific purposes.\nVisualization today has ever-expanding applications in science, education, engineering (e.g., product visualization), interactive multimedia, medicine, etc. Typical of a visualization application is the field of computer graphics. The invention of computer graphics (and 3D computer graphics) may be the most important development in visualization since the invention of central perspective in the Renaissance period. The development of animation also helped advance visualization.\n\n"
    },
    {
      "id": "302949",
      "title": "Walter Pitts",
      "url": "https://en.wikipedia.org/wiki/Walter_Pitts",
      "summary": "Walter Harry Pitts, Jr. (23 April 1923 \u2013 14 May 1969) was an American logician who worked in the field of computational neuroscience.  He proposed landmark theoretical formulations of neural activity and generative processes that influenced diverse fields such as cognitive sciences and psychology, philosophy, neurosciences, computer science, artificial neural networks, cybernetics and artificial intelligence, together with what has come to be known as the generative sciences. He is best remembered for having written along with Warren McCulloch, a seminal paper in scientific history, titled \"A Logical Calculus of Ideas Immanent in Nervous Activity\" (1943). This paper proposed the first mathematical model of a neural network. The unit of this model, a simple formalized neuron, is still the standard of reference in the field of neural networks. It is often called a McCulloch\u2013Pitts neuron.  Prior to that paper, he formalized his ideas regarding the fundamental steps to building a Turing machine in \"The Bulletin of Mathematical Biophysics\" in an essay titled \"Some observations on the simple neuron circuit\".\n\n"
    },
    {
      "id": "44508",
      "title": "Warren Sturgis McCulloch",
      "url": "https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch",
      "summary": "Warren Sturgis McCulloch (November 16, 1898 \u2013 September 24, 1969) was an American neurophysiologist and cybernetician, known for his work on the foundation for certain brain theories and his contribution to the cybernetics movement. Along with Walter Pitts, McCulloch created computational models based on mathematical algorithms called threshold logic which split the inquiry into two distinct approaches, one approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence."
    },
    {
      "id": "33109",
      "title": "Wearable computer",
      "url": "https://en.wikipedia.org/wiki/Wearable_computer",
      "summary": "A wearable computer, also known as a body-borne computer, is a computing device worn on the body. The definition of 'wearable computer' may be narrow or broad, extending to smartphones or even ordinary wristwatches.Wearables may be for general use, in which case they are just a particularly small example of mobile computing. Alternatively, they may be for specialized purposes such as fitness trackers. They may incorporate special sensors such as accelerometers, heart rate monitors, or on the more advanced side, electrocardiogram (ECG) and blood oxygen saturation (SpO2) monitors. Under the definition of wearable computers, we also include novel user interfaces such as Google Glass, an optical head-mounted display controlled by gestures. It may be that specialized wearables will evolve into general all-in-one devices, as happened with the convergence of PDAs and mobile phones into smartphones.\nWearables are typically worn on the wrist (e.g. fitness trackers), hung from the neck (like a necklace), strapped to the arm or leg (smartphones when exercising), or on the head (as glasses or a helmet), though some have been located elsewhere (e.g. on a finger or in a shoe). Devices carried in a pocket or bag \u2013 such as smartphones and before them, pocket calculators and PDAs, may or may not be regarded as 'worn'.\nWearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management. Many wearable computers are active all the time, e.g. processing or recording data continuously.\n\n"
    },
    {
      "id": "3003284",
      "title": "Weighting",
      "url": "https://en.wikipedia.org/wiki/Weighting",
      "summary": "The process of weighting involves emphasizing the contribution of particular aspects of a phenomenon (or of a set of data) over others to an outcome or result; thereby highlighting those aspects in comparison to others in the analysis. That is, rather than each variable in the data set contributing equally to the final result, some of the data is adjusted to make a greater contribution than others. This is analogous to the practice of adding (extra) weight to one side of a pair of scales in order to favour either the buyer or seller.\nWhile weighting may be applied to a set of data, such as epidemiological data, it is more commonly applied to measurements of light, heat, sound, gamma radiation, and in fact any stimulus that is spread over a spectrum of frequencies.\n\n"
    },
    {
      "id": "3829034",
      "title": "Weka (software)",
      "url": "https://en.wikipedia.org/wiki/Weka_(software)",
      "summary": "Waikato Environment for Knowledge Analysis (Weka) is a collection of machine learning and data analysis free software licensed under the GNU General Public License. It was developed at the University of Waikato, New Zealand and is the companion software to the book \"Data Mining: Practical Machine Learning Tools and Techniques\".\n\n"
    },
    {
      "id": "74575461",
      "title": "Whisper (speech recognition system)",
      "url": "https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)",
      "summary": "Whisper is a machine learning model for speech recognition and transcription, created by OpenAI and first released as open-source software in September 2022.It is capable of transcribing speech in English and several other languages,  and is also capable of translating several non-English languages into English. OpenAI claims that the combination of different training data used in its development has led to improved recognition of accents, background noise and jargon compared to previous approaches.Whisper is a weakly-supervised deep learning acoustic model, made using an encoder-decoder transformer architecture.Whisper V2 was released on December 8, 2022. Whisper V3 was released in November 2023, on the OpenAI Dev Day."
    },
    {
      "id": "768799",
      "title": "White-box testing",
      "url": "https://en.wikipedia.org/wiki/White-box_testing",
      "summary": "White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing, an internal perspective of the system is used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).\nWhite-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system\u2013level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements. Where white-box testing is design-driven, that is, driven exclusively by agreed specifications of how each component of software is required to behave (as in DO-178C and ISO 26262 processes), white-box test techniques can accomplish assessment for unimplemented or missing requirements.\nWhite-box test design techniques include the following code coverage criteria:\n\nControl flow testing\nData flow testing\nBranch testing\nStatement coverage\nDecision coverage\nModified condition/decision coverage\nPrime path testing\nPath testing"
    },
    {
      "id": "47527969",
      "title": "Word2vec",
      "url": "https://en.wikipedia.org/wiki/Word2vec",
      "summary": "Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word and their usage in context. The word2vec algorithm estimates these representations by modeling text in a large corpus. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors.\n\n"
    },
    {
      "id": "57307464",
      "title": "Word processor",
      "url": "https://en.wikipedia.org/wiki/Word_processor",
      "summary": "A word processor (WP) is a device or computer program that provides for input, editing, formatting, and output of text, often with some additional features.\nEarly word processors were stand-alone devices dedicated to the function, but current word processors are word processor programs running on general purpose computers.\nThe functions of a word processor program fall somewhere between those of a simple text editor and a fully functioned desktop publishing program. However, the distinctions between these three have changed over time and were unclear after 2010.\n\n"
    },
    {
      "id": "33139",
      "title": "World Wide Web",
      "url": "https://en.wikipedia.org/wiki/World_Wide_Web",
      "summary": "The World Wide Web (WWW or simply the Web) is an information system that enables content sharing over the Internet through user-friendly ways meant to appeal to users beyond IT specialists and hobbyists. It allows documents and other web resources to be accessed over the Internet according to specific rules of the Hypertext Transfer Protocol (HTTP).The Web was invented by English computer scientist Tim Berners-Lee while at CERN in 1989 and opened to the public in 1991. It was conceived as a \"universal linked information system\". Documents and other media content are made available to the network through web servers and can be accessed by programs such as web browsers. Servers and resources on the World Wide Web are identified and located through character strings called uniform resource locators (URLs).\nThe original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text, images, embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs) which provide immediate access to other web resources. Web navigation, or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software. The information in the Web is transferred across the Internet using the Hypertext Transfer Protocol (HTTP). Multiple web resources with a common theme and usually a common domain name make up a website. A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and individual users; and comprises an enormous amount of educational, entertainment, commercial, and government information.\nThe Web has become the world's dominant information systems platform. It is the primary tool billions of people worldwide use to interact with the Internet."
    },
    {
      "id": "23534873",
      "title": "Yann LeCun",
      "url": "https://en.wikipedia.org/wiki/Yann_LeCun",
      "summary": "Yann Andr\u00e9 LeCun ( l\u0259-KUN, French: [l\u0259k\u0153\u0303]; originally spelled Le Cun; born 8 July 1960) is a  Turing Award winning French computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics and computational neuroscience. He is the Silver Professor of the Courant Institute of Mathematical Sciences at New York University and Vice-President, Chief AI Scientist at Meta.He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNN). He is also one of the main creators of the DjVu image compression technology (together with L\u00e9on Bottou and Patrick Haffner). He co-developed the Lush programming language with L\u00e9on Bottou.\nLeCun received the 2018 Turing Award (often referred to as the \"Nobel Prize of Computing\"), together with Yoshua Bengio and Geoffrey Hinton, for their work on deep learning.\nThe three are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\"."
    },
    {
      "id": "43375904",
      "title": "Yooreeka",
      "url": "https://en.wikipedia.org/wiki/Yooreeka",
      "summary": "Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book \"Algorithms of the Intelligent Web\". Although the term \"Web\" prevailed in the title, in essence, the algorithms are valuable in any software application.\nIt covers all major algorithms and provides many examples.\nYooreeka 2.x is licensed under the Apache License rather than the somewhat more restrictive LGPL (which was the license of v1.x).\nThe library is written 100% in the Java language."
    },
    {
      "id": "47749536",
      "title": "Yoshua Bengio",
      "url": "https://en.wikipedia.org/wiki/Yoshua_Bengio",
      "summary": "Yoshua Bengio  (born March 5, 1964) is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning. He is a professor at the Department of Computer Science and Operations Research at the Universit\u00e9 de Montr\u00e9al and scientific director of the Montreal Institute for Learning Algorithms (MILA).Bengio received the 2018 ACM A.M. Turing Award (often referred to as the \"Nobel Prize of Computing\"), together with Geoffrey Hinton and Yann LeCun, for their work on deep learning. Bengio, Hinton, and LeCun are sometimes referred to as the \"Godfathers of AI\" and \"Godfathers of Deep Learning\". As of May 2023, he has the highest h-index of any computer scientist."
    }
  ],
  "links": [
    {
      "source": "455220",
      "target": "2381958"
    },
    {
      "source": "455220",
      "target": "639009"
    },
    {
      "source": "455220",
      "target": "775"
    },
    {
      "source": "455220",
      "target": "90451"
    },
    {
      "source": "455220",
      "target": "61928892"
    },
    {
      "source": "455220",
      "target": "42164234"
    },
    {
      "source": "455220",
      "target": "1164"
    },
    {
      "source": "455220",
      "target": "27051151"
    },
    {
      "source": "455220",
      "target": "1364506"
    },
    {
      "source": "455220",
      "target": "2371482"
    },
    {
      "source": "455220",
      "target": "168387"
    },
    {
      "source": "455220",
      "target": "1040299"
    },
    {
      "source": "455220",
      "target": "19541494"
    },
    {
      "source": "455220",
      "target": "5300"
    },
    {
      "source": "455220",
      "target": "7077"
    },
    {
      "source": "455220",
      "target": "5323"
    },
    {
      "source": "455220",
      "target": "7398"
    },
    {
      "source": "455220",
      "target": "2720954"
    },
    {
      "source": "455220",
      "target": "34296790"
    },
    {
      "source": "455220",
      "target": "46626475"
    },
    {
      "source": "455220",
      "target": "82871"
    },
    {
      "source": "455220",
      "target": "759422"
    },
    {
      "source": "455220",
      "target": "41961"
    },
    {
      "source": "455220",
      "target": "237536"
    },
    {
      "source": "455220",
      "target": "35458904"
    },
    {
      "source": "455220",
      "target": "7990"
    },
    {
      "source": "455220",
      "target": "8078610"
    },
    {
      "source": "455220",
      "target": "1040387"
    },
    {
      "source": "455220",
      "target": "8377"
    },
    {
      "source": "455220",
      "target": "1079396"
    },
    {
      "source": "455220",
      "target": "32472154"
    },
    {
      "source": "455220",
      "target": "204002"
    },
    {
      "source": "455220",
      "target": "19721986"
    },
    {
      "source": "455220",
      "target": "422994"
    },
    {
      "source": "455220",
      "target": "239516"
    },
    {
      "source": "455220",
      "target": "239516"
    },
    {
      "source": "455220",
      "target": "5603080"
    },
    {
      "source": "455220",
      "target": "1010280"
    },
    {
      "source": "455220",
      "target": "13777"
    },
    {
      "source": "455220",
      "target": "185529"
    },
    {
      "source": "455220",
      "target": "56938"
    },
    {
      "source": "455220",
      "target": "20276016"
    },
    {
      "source": "455220",
      "target": "58644759"
    },
    {
      "source": "455220",
      "target": "36674345"
    },
    {
      "source": "455220",
      "target": "146738"
    },
    {
      "source": "455220",
      "target": "1716320"
    },
    {
      "source": "455220",
      "target": "59252"
    },
    {
      "source": "455220",
      "target": "18831"
    },
    {
      "source": "455220",
      "target": "18933632"
    },
    {
      "source": "455220",
      "target": "19001"
    },
    {
      "source": "455220",
      "target": "175537"
    },
    {
      "source": "455220",
      "target": "37256799"
    },
    {
      "source": "455220",
      "target": "23968131"
    },
    {
      "source": "455220",
      "target": "2063278"
    },
    {
      "source": "455220",
      "target": "40572678"
    },
    {
      "source": "455220",
      "target": "189239"
    },
    {
      "source": "455220",
      "target": "2329992"
    },
    {
      "source": "455220",
      "target": "23862"
    },
    {
      "source": "455220",
      "target": "25873"
    },
    {
      "source": "455220",
      "target": "29414838"
    },
    {
      "source": "455220",
      "target": "48455863"
    },
    {
      "source": "455220",
      "target": "1393991"
    },
    {
      "source": "455220",
      "target": "69894"
    },
    {
      "source": "455220",
      "target": "455220"
    },
    {
      "source": "455220",
      "target": "3254510"
    },
    {
      "source": "455220",
      "target": "22135297"
    },
    {
      "source": "455220",
      "target": "5309"
    },
    {
      "source": "455220",
      "target": "27010"
    },
    {
      "source": "455220",
      "target": "7366298"
    },
    {
      "source": "455220",
      "target": "26685"
    },
    {
      "source": "455220",
      "target": "82871"
    },
    {
      "source": "455220",
      "target": "8286675"
    },
    {
      "source": "455220",
      "target": "12098689"
    },
    {
      "source": "455220",
      "target": "21101351"
    },
    {
      "source": "455220",
      "target": "651800"
    },
    {
      "source": "455220",
      "target": "1079396"
    },
    {
      "source": "455220",
      "target": "45554839"
    },
    {
      "source": "455220",
      "target": "82871"
    },
    {
      "source": "455220",
      "target": "23538754"
    },
    {
      "source": "455220",
      "target": "12506378"
    },
    {
      "source": "35458904",
      "target": "14924067"
    },
    {
      "source": "35458904",
      "target": "775"
    },
    {
      "source": "35458904",
      "target": "341988"
    },
    {
      "source": "35458904",
      "target": "1134"
    },
    {
      "source": "35458904",
      "target": "25745941"
    },
    {
      "source": "35458904",
      "target": "38751"
    },
    {
      "source": "35458904",
      "target": "1673865"
    },
    {
      "source": "35458904",
      "target": "30863191"
    },
    {
      "source": "35458904",
      "target": "20877649"
    },
    {
      "source": "35458904",
      "target": "27051151"
    },
    {
      "source": "35458904",
      "target": "24437894"
    },
    {
      "source": "35458904",
      "target": "30068"
    },
    {
      "source": "35458904",
      "target": "39206"
    },
    {
      "source": "35458904",
      "target": "158442"
    },
    {
      "source": "35458904",
      "target": "26880450"
    },
    {
      "source": "35458904",
      "target": "6310"
    },
    {
      "source": "35458904",
      "target": "63552467"
    },
    {
      "source": "35458904",
      "target": "7671"
    },
    {
      "source": "35458904",
      "target": "5177"
    },
    {
      "source": "35458904",
      "target": "37438"
    },
    {
      "source": "35458904",
      "target": "1181008"
    },
    {
      "source": "35458904",
      "target": "5311"
    },
    {
      "source": "35458904",
      "target": "5323"
    },
    {
      "source": "35458904",
      "target": "45443335"
    },
    {
      "source": "35458904",
      "target": "2234333"
    },
    {
      "source": "35458904",
      "target": "2720954"
    },
    {
      "source": "35458904",
      "target": "2720954"
    },
    {
      "source": "35458904",
      "target": "2588620"
    },
    {
      "source": "35458904",
      "target": "51443362"
    },
    {
      "source": "35458904",
      "target": "3575651"
    },
    {
      "source": "35458904",
      "target": "3575651"
    },
    {
      "source": "35458904",
      "target": "11501746"
    },
    {
      "source": "35458904",
      "target": "8013"
    },
    {
      "source": "35458904",
      "target": "1040512"
    },
    {
      "source": "35458904",
      "target": "23943140"
    },
    {
      "source": "35458904",
      "target": "372381"
    },
    {
      "source": "35458904",
      "target": "42906439"
    },
    {
      "source": "35458904",
      "target": "455220"
    },
    {
      "source": "35458904",
      "target": "12097860"
    },
    {
      "source": "35458904",
      "target": "2690471"
    },
    {
      "source": "35458904",
      "target": "9328883"
    },
    {
      "source": "35458904",
      "target": "7176679"
    },
    {
      "source": "35458904",
      "target": "4780372"
    },
    {
      "source": "35458904",
      "target": "40995"
    },
    {
      "source": "35458904",
      "target": "24810701"
    },
    {
      "source": "35458904",
      "target": "44783487"
    },
    {
      "source": "35458904",
      "target": "239516"
    },
    {
      "source": "35458904",
      "target": "1381282"
    },
    {
      "source": "35458904",
      "target": "759312"
    },
    {
      "source": "35458904",
      "target": "1135408"
    },
    {
      "source": "35458904",
      "target": "49882988"
    },
    {
      "source": "35458904",
      "target": "12386904"
    },
    {
      "source": "35458904",
      "target": "12386904"
    },
    {
      "source": "35458904",
      "target": "55810614"
    },
    {
      "source": "35458904",
      "target": "41961"
    },
    {
      "source": "35458904",
      "target": "42813835"
    },
    {
      "source": "35458904",
      "target": "1609808"
    },
    {
      "source": "35458904",
      "target": "2160183"
    },
    {
      "source": "35458904",
      "target": "23789529"
    },
    {
      "source": "35458904",
      "target": "3095080"
    },
    {
      "source": "35458904",
      "target": "46471245"
    },
    {
      "source": "35458904",
      "target": "3575656"
    },
    {
      "source": "35458904",
      "target": "1157832"
    },
    {
      "source": "35458904",
      "target": "8495"
    },
    {
      "source": "35458904",
      "target": "60798295"
    },
    {
      "source": "35458904",
      "target": "6212365"
    },
    {
      "source": "35458904",
      "target": "28174"
    },
    {
      "source": "35458904",
      "target": "7360695"
    },
    {
      "source": "35458904",
      "target": "4080917"
    },
    {
      "source": "35458904",
      "target": "1705399"
    },
    {
      "source": "35458904",
      "target": "3461736"
    },
    {
      "source": "35458904",
      "target": "7990"
    },
    {
      "source": "35458904",
      "target": "12487489"
    },
    {
      "source": "35458904",
      "target": "8377"
    },
    {
      "source": "35458904",
      "target": "265752"
    },
    {
      "source": "35458904",
      "target": "8501"
    },
    {
      "source": "35458904",
      "target": "422994"
    },
    {
      "source": "35458904",
      "target": "750101"
    },
    {
      "source": "35458904",
      "target": "9545"
    },
    {
      "source": "35458904",
      "target": "72826783"
    },
    {
      "source": "35458904",
      "target": "46363781"
    },
    {
      "source": "35458904",
      "target": "239516"
    },
    {
      "source": "35458904",
      "target": "46207323"
    },
    {
      "source": "35458904",
      "target": "294894"
    },
    {
      "source": "35458904",
      "target": "12799"
    },
    {
      "source": "35458904",
      "target": "1822696"
    },
    {
      "source": "35458904",
      "target": "23534602"
    },
    {
      "source": "35458904",
      "target": "14919"
    },
    {
      "source": "35458904",
      "target": "234930"
    },
    {
      "source": "35458904",
      "target": "2862975"
    },
    {
      "source": "35458904",
      "target": "14774"
    },
    {
      "source": "35458904",
      "target": "237536"
    },
    {
      "source": "35458904",
      "target": "149354"
    },
    {
      "source": "35458904",
      "target": "36674345"
    },
    {
      "source": "35458904",
      "target": "3461736"
    },
    {
      "source": "35458904",
      "target": "15201"
    },
    {
      "source": "35458904",
      "target": "15201"
    },
    {
      "source": "35458904",
      "target": "15201"
    },
    {
      "source": "35458904",
      "target": "68833"
    },
    {
      "source": "35458904",
      "target": "48996671"
    },
    {
      "source": "35458904",
      "target": "308367"
    },
    {
      "source": "35458904",
      "target": "31074852"
    },
    {
      "source": "35458904",
      "target": "243391"
    },
    {
      "source": "35458904",
      "target": "31002435"
    },
    {
      "source": "35458904",
      "target": "233488"
    },
    {
      "source": "35458904",
      "target": "18831"
    },
    {
      "source": "35458904",
      "target": "3606435"
    },
    {
      "source": "35458904",
      "target": "62500799"
    },
    {
      "source": "35458904",
      "target": "3156699"
    },
    {
      "source": "35458904",
      "target": "883885"
    },
    {
      "source": "35458904",
      "target": "503009"
    },
    {
      "source": "35458904",
      "target": "58731"
    },
    {
      "source": "35458904",
      "target": "60931"
    },
    {
      "source": "35458904",
      "target": "1467948"
    },
    {
      "source": "35458904",
      "target": "23862"
    },
    {
      "source": "35458904",
      "target": "376707"
    },
    {
      "source": "35458904",
      "target": "48455863"
    },
    {
      "source": "35458904",
      "target": "46951032"
    },
    {
      "source": "35458904",
      "target": "1181008"
    },
    {
      "source": "35458904",
      "target": "26833"
    },
    {
      "source": "35458904",
      "target": "29006"
    },
    {
      "source": "35458904",
      "target": "26685"
    },
    {
      "source": "35458904",
      "target": "26685"
    },
    {
      "source": "35458904",
      "target": "82871"
    },
    {
      "source": "35458904",
      "target": "8151410"
    },
    {
      "source": "35458904",
      "target": "82871"
    },
    {
      "source": "35458904",
      "target": "35263738"
    },
    {
      "source": "35458904",
      "target": "6390647"
    },
    {
      "source": "35458904",
      "target": "46374359"
    },
    {
      "source": "35458904",
      "target": "70358320"
    },
    {
      "source": "2720954",
      "target": "189373"
    },
    {
      "source": "2720954",
      "target": "40744441"
    },
    {
      "source": "2720954",
      "target": "775"
    },
    {
      "source": "2720954",
      "target": "1134"
    },
    {
      "source": "2720954",
      "target": "11731170"
    },
    {
      "source": "2720954",
      "target": "393311"
    },
    {
      "source": "2720954",
      "target": "1833304"
    },
    {
      "source": "2720954",
      "target": "26161833"
    },
    {
      "source": "2720954",
      "target": "14092434"
    },
    {
      "source": "2720954",
      "target": "2326042"
    },
    {
      "source": "2720954",
      "target": "27051151"
    },
    {
      "source": "2720954",
      "target": "7838811"
    },
    {
      "source": "2720954",
      "target": "6885770"
    },
    {
      "source": "2720954",
      "target": "2370583"
    },
    {
      "source": "2720954",
      "target": "1678795"
    },
    {
      "source": "2720954",
      "target": "160960"
    },
    {
      "source": "2720954",
      "target": "21707946"
    },
    {
      "source": "2720954",
      "target": "15393674"
    },
    {
      "source": "2720954",
      "target": "168387"
    },
    {
      "source": "2720954",
      "target": "49749"
    },
    {
      "source": "2720954",
      "target": "938069"
    },
    {
      "source": "2720954",
      "target": "37196"
    },
    {
      "source": "2720954",
      "target": "11548952"
    },
    {
      "source": "2720954",
      "target": "743794"
    },
    {
      "source": "2720954",
      "target": "47278"
    },
    {
      "source": "2720954",
      "target": "1519880"
    },
    {
      "source": "2720954",
      "target": "34308675"
    },
    {
      "source": "2720954",
      "target": "3849994"
    },
    {
      "source": "2720954",
      "target": "305924"
    },
    {
      "source": "2720954",
      "target": "5205878"
    },
    {
      "source": "2720954",
      "target": "17156914"
    },
    {
      "source": "2720954",
      "target": "106418"
    },
    {
      "source": "2720954",
      "target": "1181008"
    },
    {
      "source": "2720954",
      "target": "53713224"
    },
    {
      "source": "2720954",
      "target": "375416"
    },
    {
      "source": "2720954",
      "target": "59160"
    },
    {
      "source": "2720954",
      "target": "1053718"
    },
    {
      "source": "2720954",
      "target": "36108052"
    },
    {
      "source": "2720954",
      "target": "435754"
    },
    {
      "source": "2720954",
      "target": "157057"
    },
    {
      "source": "2720954",
      "target": "379269"
    },
    {
      "source": "2720954",
      "target": "3144369"
    },
    {
      "source": "2720954",
      "target": "416612"
    },
    {
      "source": "2720954",
      "target": "201756"
    },
    {
      "source": "2720954",
      "target": "2234333"
    },
    {
      "source": "2720954",
      "target": "2720954"
    },
    {
      "source": "2720954",
      "target": "3461736"
    },
    {
      "source": "2720954",
      "target": "155319"
    },
    {
      "source": "2720954",
      "target": "3461736"
    },
    {
      "source": "2720954",
      "target": "2588620"
    },
    {
      "source": "2720954",
      "target": "51443362"
    },
    {
      "source": "2720954",
      "target": "54332547"
    },
    {
      "source": "2720954",
      "target": "3575651"
    },
    {
      "source": "2720954",
      "target": "11501746"
    },
    {
      "source": "2720954",
      "target": "8013"
    },
    {
      "source": "2720954",
      "target": "1040512"
    },
    {
      "source": "2720954",
      "target": "23943140"
    },
    {
      "source": "2720954",
      "target": "28192799"
    },
    {
      "source": "2720954",
      "target": "372381"
    },
    {
      "source": "2720954",
      "target": "42906439"
    },
    {
      "source": "2720954",
      "target": "12097860"
    },
    {
      "source": "2720954",
      "target": "2690471"
    },
    {
      "source": "2720954",
      "target": "9328883"
    },
    {
      "source": "2720954",
      "target": "7176679"
    },
    {
      "source": "2720954",
      "target": "6222875"
    },
    {
      "source": "2720954",
      "target": "4780372"
    },
    {
      "source": "2720954",
      "target": "40995"
    },
    {
      "source": "2720954",
      "target": "24810701"
    },
    {
      "source": "2720954",
      "target": "44783487"
    },
    {
      "source": "2720954",
      "target": "239516"
    },
    {
      "source": "2720954",
      "target": "1381282"
    },
    {
      "source": "2720954",
      "target": "759312"
    },
    {
      "source": "2720954",
      "target": "1135408"
    },
    {
      "source": "2720954",
      "target": "82871"
    },
    {
      "source": "2720954",
      "target": "759422"
    },
    {
      "source": "2720954",
      "target": "49882988"
    },
    {
      "source": "2720954",
      "target": "12386904"
    },
    {
      "source": "2720954",
      "target": "55810614"
    },
    {
      "source": "2720954",
      "target": "41961"
    },
    {
      "source": "2720954",
      "target": "42813835"
    },
    {
      "source": "2720954",
      "target": "1609808"
    },
    {
      "source": "2720954",
      "target": "2160183"
    },
    {
      "source": "2720954",
      "target": "23789529"
    },
    {
      "source": "2720954",
      "target": "3095080"
    },
    {
      "source": "2720954",
      "target": "35458904"
    },
    {
      "source": "2720954",
      "target": "46471245"
    },
    {
      "source": "2720954",
      "target": "3575656"
    },
    {
      "source": "2720954",
      "target": "1157832"
    },
    {
      "source": "2720954",
      "target": "6212365"
    },
    {
      "source": "2720954",
      "target": "28174"
    },
    {
      "source": "2720954",
      "target": "7360695"
    },
    {
      "source": "2720954",
      "target": "24770596"
    },
    {
      "source": "2720954",
      "target": "4080917"
    },
    {
      "source": "2720954",
      "target": "10056274"
    },
    {
      "source": "2720954",
      "target": "1705399"
    },
    {
      "source": "2720954",
      "target": "3461736"
    },
    {
      "source": "2720954",
      "target": "7990"
    },
    {
      "source": "2720954",
      "target": "12487489"
    },
    {
      "source": "2720954",
      "target": "8377"
    },
    {
      "source": "2720954",
      "target": "30666895"
    },
    {
      "source": "2720954",
      "target": "35276459"
    },
    {
      "source": "2720954",
      "target": "8187"
    },
    {
      "source": "2720954",
      "target": "23321884"
    },
    {
      "source": "2720954",
      "target": "416589"
    },
    {
      "source": "2720954",
      "target": "8525"
    },
    {
      "source": "2720954",
      "target": "579867"
    },
    {
      "source": "2720954",
      "target": "12578506"
    },
    {
      "source": "2720954",
      "target": "422994"
    },
    {
      "source": "2720954",
      "target": "10672320"
    },
    {
      "source": "2720954",
      "target": "1324565"
    },
    {
      "source": "2720954",
      "target": "20789898"
    },
    {
      "source": "2720954",
      "target": "61612443"
    },
    {
      "source": "2720954",
      "target": "9252"
    },
    {
      "source": "2720954",
      "target": "1706783"
    },
    {
      "source": "2720954",
      "target": "263977"
    },
    {
      "source": "2720954",
      "target": "416589"
    },
    {
      "source": "2720954",
      "target": "46363781"
    },
    {
      "source": "2720954",
      "target": "239516"
    },
    {
      "source": "2720954",
      "target": "1339400"
    },
    {
      "source": "2720954",
      "target": "758833"
    },
    {
      "source": "2720954",
      "target": "6184541"
    },
    {
      "source": "2720954",
      "target": "6054681"
    },
    {
      "source": "2720954",
      "target": "18233581"
    },
    {
      "source": "2720954",
      "target": "345017"
    },
    {
      "source": "2720954",
      "target": "509709"
    },
    {
      "source": "2720954",
      "target": "39414862"
    },
    {
      "source": "2720954",
      "target": "41916270"
    },
    {
      "source": "2720954",
      "target": "6116152"
    },
    {
      "source": "2720954",
      "target": "14349"
    },
    {
      "source": "2720954",
      "target": "22048289"
    },
    {
      "source": "2720954",
      "target": "40744441"
    },
    {
      "source": "2720954",
      "target": "13266"
    },
    {
      "source": "2720954",
      "target": "21073209"
    },
    {
      "source": "2720954",
      "target": "30284"
    },
    {
      "source": "2720954",
      "target": "14919"
    },
    {
      "source": "2720954",
      "target": "234930"
    },
    {
      "source": "2720954",
      "target": "1309220"
    },
    {
      "source": "2720954",
      "target": "27577"
    },
    {
      "source": "2720954",
      "target": "18985062"
    },
    {
      "source": "2720954",
      "target": "429063"
    },
    {
      "source": "2720954",
      "target": "234824"
    },
    {
      "source": "2720954",
      "target": "450703"
    },
    {
      "source": "2720954",
      "target": "237536"
    },
    {
      "source": "2720954",
      "target": "2008426"
    },
    {
      "source": "2720954",
      "target": "2336317"
    },
    {
      "source": "2720954",
      "target": "3461736"
    },
    {
      "source": "2720954",
      "target": "714366"
    },
    {
      "source": "2720954",
      "target": "216262"
    },
    {
      "source": "2720954",
      "target": "50146071"
    },
    {
      "source": "2720954",
      "target": "15942"
    },
    {
      "source": "2720954",
      "target": "34498283"
    },
    {
      "source": "2720954",
      "target": "38455554"
    },
    {
      "source": "2720954",
      "target": "73044681"
    },
    {
      "source": "2720954",
      "target": "24960689"
    },
    {
      "source": "2720954",
      "target": "396743"
    },
    {
      "source": "2720954",
      "target": "75971902"
    },
    {
      "source": "2720954",
      "target": "55704468"
    },
    {
      "source": "2720954",
      "target": "5074413"
    },
    {
      "source": "2720954",
      "target": "21850768"
    },
    {
      "source": "2720954",
      "target": "227686"
    },
    {
      "source": "2720954",
      "target": "1558201"
    },
    {
      "source": "2720954",
      "target": "51354460"
    },
    {
      "source": "2720954",
      "target": "49082762"
    },
    {
      "source": "2720954",
      "target": "1444165"
    },
    {
      "source": "2720954",
      "target": "23244381"
    },
    {
      "source": "2720954",
      "target": "642328"
    },
    {
      "source": "2720954",
      "target": "19022"
    },
    {
      "source": "2720954",
      "target": "47403"
    },
    {
      "source": "2720954",
      "target": "56107"
    },
    {
      "source": "2720954",
      "target": "47939883"
    },
    {
      "source": "2720954",
      "target": "36197584"
    },
    {
      "source": "2720954",
      "target": "7859676"
    },
    {
      "source": "2720954",
      "target": "198608"
    },
    {
      "source": "2720954",
      "target": "1112960"
    },
    {
      "source": "2720954",
      "target": "56098"
    },
    {
      "source": "2720954",
      "target": "47933469"
    },
    {
      "source": "2720954",
      "target": "41240510"
    },
    {
      "source": "2720954",
      "target": "762970"
    },
    {
      "source": "2720954",
      "target": "42637526"
    },
    {
      "source": "2720954",
      "target": "30928751"
    },
    {
      "source": "2720954",
      "target": "30909817"
    },
    {
      "source": "2720954",
      "target": "2789271"
    },
    {
      "source": "2720954",
      "target": "41221419"
    },
    {
      "source": "2720954",
      "target": "312648"
    },
    {
      "source": "2720954",
      "target": "4917686"
    },
    {
      "source": "2720954",
      "target": "63145468"
    },
    {
      "source": "2720954",
      "target": "7309022"
    },
    {
      "source": "2720954",
      "target": "71193445"
    },
    {
      "source": "2720954",
      "target": "146103"
    },
    {
      "source": "2720954",
      "target": "40158142"
    },
    {
      "source": "2720954",
      "target": "21462"
    },
    {
      "source": "2720954",
      "target": "397245"
    },
    {
      "source": "2720954",
      "target": "216827"
    },
    {
      "source": "2720954",
      "target": "883885"
    },
    {
      "source": "2720954",
      "target": "2539154"
    },
    {
      "source": "2720954",
      "target": "40935351"
    },
    {
      "source": "2720954",
      "target": "503009"
    },
    {
      "source": "2720954",
      "target": "38833779"
    },
    {
      "source": "2720954",
      "target": "2499813"
    },
    {
      "source": "2720954",
      "target": "4060171"
    },
    {
      "source": "2720954",
      "target": "596816"
    },
    {
      "source": "2720954",
      "target": "153390"
    },
    {
      "source": "2720954",
      "target": "394780"
    },
    {
      "source": "2720954",
      "target": "6879051"
    },
    {
      "source": "2720954",
      "target": "19774918"
    },
    {
      "source": "2720954",
      "target": "76340"
    },
    {
      "source": "2720954",
      "target": "23543"
    },
    {
      "source": "2720954",
      "target": "1467948"
    },
    {
      "source": "2720954",
      "target": "484764"
    },
    {
      "source": "2720954",
      "target": "16722927"
    },
    {
      "source": "2720954",
      "target": "389564"
    },
    {
      "source": "2720954",
      "target": "376707"
    },
    {
      "source": "2720954",
      "target": "65462"
    },
    {
      "source": "2720954",
      "target": "6895400"
    },
    {
      "source": "2720954",
      "target": "826997"
    },
    {
      "source": "2720954",
      "target": "239138"
    },
    {
      "source": "2720954",
      "target": "29915274"
    },
    {
      "source": "2720954",
      "target": "3095637"
    },
    {
      "source": "2720954",
      "target": "20733547"
    },
    {
      "source": "2720954",
      "target": "21511800"
    },
    {
      "source": "2720954",
      "target": "9461390"
    },
    {
      "source": "2720954",
      "target": "23814012"
    },
    {
      "source": "2720954",
      "target": "6392749"
    },
    {
      "source": "2720954",
      "target": "48455863"
    },
    {
      "source": "2720954",
      "target": "620083"
    },
    {
      "source": "2720954",
      "target": "5665800"
    },
    {
      "source": "2720954",
      "target": "67353201"
    },
    {
      "source": "2720954",
      "target": "531362"
    },
    {
      "source": "2720954",
      "target": "1266110"
    },
    {
      "source": "2720954",
      "target": "1743077"
    },
    {
      "source": "2720954",
      "target": "15934463"
    },
    {
      "source": "2720954",
      "target": "30284"
    },
    {
      "source": "2720954",
      "target": "27576"
    },
    {
      "source": "2720954",
      "target": "26502065"
    },
    {
      "source": "2720954",
      "target": "27580"
    },
    {
      "source": "2720954",
      "target": "26685"
    },
    {
      "source": "2720954",
      "target": "977649"
    },
    {
      "source": "2720954",
      "target": "17813659"
    },
    {
      "source": "2720954",
      "target": "6816305"
    },
    {
      "source": "2720954",
      "target": "552466"
    },
    {
      "source": "2720954",
      "target": "337862"
    },
    {
      "source": "2720954",
      "target": "47389113"
    },
    {
      "source": "2720954",
      "target": "10559845"
    },
    {
      "source": "2720954",
      "target": "473317"
    },
    {
      "source": "2720954",
      "target": "318439"
    },
    {
      "source": "2720954",
      "target": "1592887"
    },
    {
      "source": "2720954",
      "target": "3087410"
    },
    {
      "source": "2720954",
      "target": "5657877"
    },
    {
      "source": "2720954",
      "target": "5657877"
    },
    {
      "source": "2720954",
      "target": "11526222"
    },
    {
      "source": "2720954",
      "target": "82871"
    },
    {
      "source": "2720954",
      "target": "21280496"
    },
    {
      "source": "2720954",
      "target": "61159333"
    },
    {
      "source": "2720954",
      "target": "50903"
    },
    {
      "source": "2720954",
      "target": "23538754"
    },
    {
      "source": "2720954",
      "target": "502806"
    },
    {
      "source": "8078610",
      "target": "454746"
    },
    {
      "source": "8078610",
      "target": "3425793"
    },
    {
      "source": "8078610",
      "target": "63325563"
    },
    {
      "source": "8078610",
      "target": "336975"
    },
    {
      "source": "8078610",
      "target": "3074936"
    },
    {
      "source": "8078610",
      "target": "8237163"
    },
    {
      "source": "8078610",
      "target": "33354721"
    },
    {
      "source": "8078610",
      "target": "488211"
    },
    {
      "source": "8078610",
      "target": "6870132"
    },
    {
      "source": "8078610",
      "target": "1041167"
    },
    {
      "source": "8078610",
      "target": "14410703"
    },
    {
      "source": "8078610",
      "target": "49259327"
    },
    {
      "source": "8078610",
      "target": "12912603"
    },
    {
      "source": "8078610",
      "target": "1569036"
    },
    {
      "source": "8078610",
      "target": "217356"
    },
    {
      "source": "8078610",
      "target": "287180"
    },
    {
      "source": "8078610",
      "target": "2104836"
    },
    {
      "source": "8078610",
      "target": "6968"
    },
    {
      "source": "8078610",
      "target": "452018"
    },
    {
      "source": "8078610",
      "target": "3361242"
    },
    {
      "source": "8078610",
      "target": "911354"
    },
    {
      "source": "8078610",
      "target": "452018"
    },
    {
      "source": "8078610",
      "target": "645139"
    },
    {
      "source": "8078610",
      "target": "975347"
    },
    {
      "source": "8078610",
      "target": "1135408"
    },
    {
      "source": "8078610",
      "target": "82871"
    },
    {
      "source": "8078610",
      "target": "7990"
    },
    {
      "source": "8078610",
      "target": "8377"
    },
    {
      "source": "8078610",
      "target": "13783336"
    },
    {
      "source": "8078610",
      "target": "1075035"
    },
    {
      "source": "8078610",
      "target": "25386818"
    },
    {
      "source": "8078610",
      "target": "8078610"
    },
    {
      "source": "8078610",
      "target": "30055981"
    },
    {
      "source": "8078610",
      "target": "1989841"
    },
    {
      "source": "8078610",
      "target": "14158342"
    },
    {
      "source": "8078610",
      "target": "8423925"
    },
    {
      "source": "8078610",
      "target": "1040387"
    },
    {
      "source": "8078610",
      "target": "4675212"
    },
    {
      "source": "8078610",
      "target": "1711076"
    },
    {
      "source": "8078610",
      "target": "40995"
    },
    {
      "source": "8078610",
      "target": "4916098"
    },
    {
      "source": "8078610",
      "target": "8377"
    },
    {
      "source": "8078610",
      "target": "8640"
    },
    {
      "source": "8078610",
      "target": "23454460"
    },
    {
      "source": "8078610",
      "target": "1500119"
    },
    {
      "source": "8078610",
      "target": "7514525"
    },
    {
      "source": "8078610",
      "target": "345937"
    },
    {
      "source": "8078610",
      "target": "4689778"
    },
    {
      "source": "8078610",
      "target": "815760"
    },
    {
      "source": "8078610",
      "target": "10983365"
    },
    {
      "source": "8078610",
      "target": "1093623"
    },
    {
      "source": "8078610",
      "target": "233953"
    },
    {
      "source": "8078610",
      "target": "1384316"
    },
    {
      "source": "8078610",
      "target": "4452966"
    },
    {
      "source": "8078610",
      "target": "21852501"
    },
    {
      "source": "8078610",
      "target": "21463262"
    },
    {
      "source": "8078610",
      "target": "334641"
    },
    {
      "source": "8078610",
      "target": "41054"
    },
    {
      "source": "8078610",
      "target": "8377"
    },
    {
      "source": "8078610",
      "target": "40149201"
    },
    {
      "source": "8078610",
      "target": "15002414"
    },
    {
      "source": "8078610",
      "target": "422994"
    },
    {
      "source": "8078610",
      "target": "14554451"
    },
    {
      "source": "8078610",
      "target": "752991"
    },
    {
      "source": "8078610",
      "target": "9310"
    },
    {
      "source": "8078610",
      "target": "752991"
    },
    {
      "source": "8078610",
      "target": "1270246"
    },
    {
      "source": "8078610",
      "target": "259065"
    },
    {
      "source": "8078610",
      "target": "24799509"
    },
    {
      "source": "8078610",
      "target": "10210385"
    },
    {
      "source": "8078610",
      "target": "18825039"
    },
    {
      "source": "8078610",
      "target": "247075"
    },
    {
      "source": "8078610",
      "target": "142983"
    },
    {
      "source": "8078610",
      "target": "1942477"
    },
    {
      "source": "8078610",
      "target": "494528"
    },
    {
      "source": "8078610",
      "target": "36674345"
    },
    {
      "source": "8078610",
      "target": "8514669"
    },
    {
      "source": "8078610",
      "target": "26257672"
    },
    {
      "source": "8078610",
      "target": "37704882"
    },
    {
      "source": "8078610",
      "target": "43291963"
    },
    {
      "source": "8078610",
      "target": "1005923"
    },
    {
      "source": "8078610",
      "target": "6331838"
    },
    {
      "source": "8078610",
      "target": "29717355"
    },
    {
      "source": "8078610",
      "target": "42500102"
    },
    {
      "source": "8078610",
      "target": "50833317"
    },
    {
      "source": "8078610",
      "target": "42447955"
    },
    {
      "source": "8078610",
      "target": "8621397"
    },
    {
      "source": "8078610",
      "target": "3967073"
    },
    {
      "source": "8078610",
      "target": "52714158"
    },
    {
      "source": "8078610",
      "target": "1568820"
    },
    {
      "source": "8078610",
      "target": "67986706"
    },
    {
      "source": "8078610",
      "target": "15219872"
    },
    {
      "source": "8078610",
      "target": "689470"
    },
    {
      "source": "8078610",
      "target": "21064035"
    },
    {
      "source": "8078610",
      "target": "20722131"
    },
    {
      "source": "8078610",
      "target": "19001"
    },
    {
      "source": "8078610",
      "target": "19961416"
    },
    {
      "source": "8078610",
      "target": "13989994"
    },
    {
      "source": "8078610",
      "target": "18984"
    },
    {
      "source": "8078610",
      "target": "44971098"
    },
    {
      "source": "8078610",
      "target": "622805"
    },
    {
      "source": "8078610",
      "target": "12024861"
    },
    {
      "source": "8078610",
      "target": "185098"
    },
    {
      "source": "8078610",
      "target": "37256799"
    },
    {
      "source": "8078610",
      "target": "23968131"
    },
    {
      "source": "8078610",
      "target": "2002540"
    },
    {
      "source": "8078610",
      "target": "22826"
    },
    {
      "source": "8078610",
      "target": "5646487"
    },
    {
      "source": "8078610",
      "target": "68960"
    },
    {
      "source": "8078610",
      "target": "59202"
    },
    {
      "source": "8078610",
      "target": "168701"
    },
    {
      "source": "8078610",
      "target": "2504464"
    },
    {
      "source": "8078610",
      "target": "22591"
    },
    {
      "source": "8078610",
      "target": "323725"
    },
    {
      "source": "8078610",
      "target": "35855099"
    },
    {
      "source": "8078610",
      "target": "1983101"
    },
    {
      "source": "8078610",
      "target": "154864"
    },
    {
      "source": "8078610",
      "target": "23824"
    },
    {
      "source": "8078610",
      "target": "15395806"
    },
    {
      "source": "8078610",
      "target": "43435003"
    },
    {
      "source": "8078610",
      "target": "494528"
    },
    {
      "source": "8078610",
      "target": "3480761"
    },
    {
      "source": "8078610",
      "target": "1710792"
    },
    {
      "source": "8078610",
      "target": "42437"
    },
    {
      "source": "8078610",
      "target": "445718"
    },
    {
      "source": "8078610",
      "target": "23720058"
    },
    {
      "source": "8078610",
      "target": "175285"
    },
    {
      "source": "8078610",
      "target": "175769"
    },
    {
      "source": "8078610",
      "target": "25873"
    },
    {
      "source": "8078610",
      "target": "26220"
    },
    {
      "source": "8078610",
      "target": "2722905"
    },
    {
      "source": "8078610",
      "target": "1041142"
    },
    {
      "source": "8078610",
      "target": "2688402"
    },
    {
      "source": "8078610",
      "target": "48455863"
    },
    {
      "source": "8078610",
      "target": "2728460"
    },
    {
      "source": "8078610",
      "target": "19467605"
    },
    {
      "source": "8078610",
      "target": "2816674"
    },
    {
      "source": "8078610",
      "target": "14109784"
    },
    {
      "source": "8078610",
      "target": "277251"
    },
    {
      "source": "8078610",
      "target": "12796220"
    },
    {
      "source": "8078610",
      "target": "1137952"
    },
    {
      "source": "8078610",
      "target": "472950"
    },
    {
      "source": "8078610",
      "target": "1701163"
    },
    {
      "source": "8078610",
      "target": "32399"
    },
    {
      "source": "8078610",
      "target": "245974"
    },
    {
      "source": "8078610",
      "target": "212409"
    },
    {
      "source": "8078610",
      "target": "244602"
    },
    {
      "source": "8078610",
      "target": "6850099"
    },
    {
      "source": "8078610",
      "target": "1960226"
    },
    {
      "source": "8078610",
      "target": "47393845"
    },
    {
      "source": "8078610",
      "target": "32238027"
    },
    {
      "source": "233488",
      "target": "24714635"
    },
    {
      "source": "233488",
      "target": "3192516"
    },
    {
      "source": "233488",
      "target": "24559562"
    },
    {
      "source": "233488",
      "target": "50785023"
    },
    {
      "source": "233488",
      "target": "75934668"
    },
    {
      "source": "233488",
      "target": "72360809"
    },
    {
      "source": "233488",
      "target": "813176"
    },
    {
      "source": "233488",
      "target": "3548574"
    },
    {
      "source": "233488",
      "target": "3874825"
    },
    {
      "source": "233488",
      "target": "5033373"
    },
    {
      "source": "233488",
      "target": "14179835"
    },
    {
      "source": "233488",
      "target": "28801798"
    },
    {
      "source": "233488",
      "target": "23924341"
    },
    {
      "source": "233488",
      "target": "45049676"
    },
    {
      "source": "233488",
      "target": "233942"
    },
    {
      "source": "233488",
      "target": "627"
    },
    {
      "source": "233488",
      "target": "5906926"
    },
    {
      "source": "233488",
      "target": "50568903"
    },
    {
      "source": "233488",
      "target": "775"
    },
    {
      "source": "233488",
      "target": "775"
    },
    {
      "source": "233488",
      "target": "55817338"
    },
    {
      "source": "233488",
      "target": "145128"
    },
    {
      "source": "233488",
      "target": "52773150"
    },
    {
      "source": "233488",
      "target": "59766171"
    },
    {
      "source": "233488",
      "target": "55981499"
    },
    {
      "source": "233488",
      "target": "1691376"
    },
    {
      "source": "233488",
      "target": "2230"
    },
    {
      "source": "233488",
      "target": "15345868"
    },
    {
      "source": "233488",
      "target": "8190902"
    },
    {
      "source": "233488",
      "target": "6206236"
    },
    {
      "source": "233488",
      "target": "18706674"
    },
    {
      "source": "233488",
      "target": "42164234"
    },
    {
      "source": "233488",
      "target": "49338480"
    },
    {
      "source": "233488",
      "target": "2571015"
    },
    {
      "source": "233488",
      "target": "15893057"
    },
    {
      "source": "233488",
      "target": "19463198"
    },
    {
      "source": "233488",
      "target": "49277634"
    },
    {
      "source": "233488",
      "target": "38751"
    },
    {
      "source": "233488",
      "target": "38751"
    },
    {
      "source": "233488",
      "target": "2052"
    },
    {
      "source": "233488",
      "target": "10809677"
    },
    {
      "source": "233488",
      "target": "566680"
    },
    {
      "source": "233488",
      "target": "586357"
    },
    {
      "source": "233488",
      "target": "1589987"
    },
    {
      "source": "233488",
      "target": "1164"
    },
    {
      "source": "233488",
      "target": "60639760"
    },
    {
      "source": "233488",
      "target": "59539440"
    },
    {
      "source": "233488",
      "target": "52588198"
    },
    {
      "source": "233488",
      "target": "54008163"
    },
    {
      "source": "233488",
      "target": "72771661"
    },
    {
      "source": "233488",
      "target": "7872324"
    },
    {
      "source": "233488",
      "target": "21523"
    },
    {
      "source": "233488",
      "target": "349771"
    },
    {
      "source": "233488",
      "target": "644058"
    },
    {
      "source": "233488",
      "target": "577053"
    },
    {
      "source": "233488",
      "target": "28326718"
    },
    {
      "source": "233488",
      "target": "66001552"
    },
    {
      "source": "233488",
      "target": "6836612"
    },
    {
      "source": "233488",
      "target": "103356"
    },
    {
      "source": "233488",
      "target": "55843837"
    },
    {
      "source": "233488",
      "target": "9034035"
    },
    {
      "source": "233488",
      "target": "1505641"
    },
    {
      "source": "233488",
      "target": "2546"
    },
    {
      "source": "233488",
      "target": "734787"
    },
    {
      "source": "233488",
      "target": "245926"
    },
    {
      "source": "233488",
      "target": "1434444"
    },
    {
      "source": "233488",
      "target": "19961416"
    },
    {
      "source": "233488",
      "target": "62026514"
    },
    {
      "source": "233488",
      "target": "83443"
    },
    {
      "source": "233488",
      "target": "71889610"
    },
    {
      "source": "233488",
      "target": "360788"
    },
    {
      "source": "233488",
      "target": "1360091"
    },
    {
      "source": "233488",
      "target": "486777"
    },
    {
      "source": "233488",
      "target": "73363598"
    },
    {
      "source": "233488",
      "target": "1274090"
    },
    {
      "source": "233488",
      "target": "50909"
    },
    {
      "source": "233488",
      "target": "19892153"
    },
    {
      "source": "233488",
      "target": "57222123"
    },
    {
      "source": "233488",
      "target": "49571"
    },
    {
      "source": "233488",
      "target": "203996"
    },
    {
      "source": "233488",
      "target": "40973765"
    },
    {
      "source": "233488",
      "target": "540801"
    },
    {
      "source": "233488",
      "target": "40678189"
    },
    {
      "source": "233488",
      "target": "40678189"
    },
    {
      "source": "233488",
      "target": "14092434"
    },
    {
      "source": "233488",
      "target": "27051151"
    },
    {
      "source": "233488",
      "target": "205393"
    },
    {
      "source": "233488",
      "target": "4214"
    },
    {
      "source": "233488",
      "target": "1729542"
    },
    {
      "source": "233488",
      "target": "12207"
    },
    {
      "source": "233488",
      "target": "90500"
    },
    {
      "source": "233488",
      "target": "1307911"
    },
    {
      "source": "233488",
      "target": "6885770"
    },
    {
      "source": "233488",
      "target": "19009110"
    },
    {
      "source": "233488",
      "target": "623686"
    },
    {
      "source": "233488",
      "target": "22643107"
    },
    {
      "source": "233488",
      "target": "53631046"
    },
    {
      "source": "233488",
      "target": "363900"
    },
    {
      "source": "233488",
      "target": "1012548"
    },
    {
      "source": "233488",
      "target": "27809"
    },
    {
      "source": "233488",
      "target": "575697"
    },
    {
      "source": "233488",
      "target": "72752788"
    },
    {
      "source": "233488",
      "target": "6216"
    },
    {
      "source": "233488",
      "target": "8760516"
    },
    {
      "source": "233488",
      "target": "8760516"
    },
    {
      "source": "233488",
      "target": "555480"
    },
    {
      "source": "233488",
      "target": "158949"
    },
    {
      "source": "233488",
      "target": "2155752"
    },
    {
      "source": "233488",
      "target": "142440"
    },
    {
      "source": "233488",
      "target": "669675"
    },
    {
      "source": "233488",
      "target": "1500869"
    },
    {
      "source": "233488",
      "target": "42581062"
    },
    {
      "source": "233488",
      "target": "5739"
    },
    {
      "source": "233488",
      "target": "155414"
    },
    {
      "source": "233488",
      "target": "48520204"
    },
    {
      "source": "233488",
      "target": "149353"
    },
    {
      "source": "233488",
      "target": "6019"
    },
    {
      "source": "233488",
      "target": "7543"
    },
    {
      "source": "233488",
      "target": "1420447"
    },
    {
      "source": "233488",
      "target": "7948184"
    },
    {
      "source": "233488",
      "target": "176927"
    },
    {
      "source": "233488",
      "target": "387537"
    },
    {
      "source": "233488",
      "target": "5561"
    },
    {
      "source": "233488",
      "target": "10433833"
    },
    {
      "source": "233488",
      "target": "106418"
    },
    {
      "source": "233488",
      "target": "1181008"
    },
    {
      "source": "233488",
      "target": "40077102"
    },
    {
      "source": "233488",
      "target": "15832717"
    },
    {
      "source": "233488",
      "target": "411964"
    },
    {
      "source": "233488",
      "target": "6777"
    },
    {
      "source": "233488",
      "target": "25652303"
    },
    {
      "source": "233488",
      "target": "5300"
    },
    {
      "source": "233488",
      "target": "1336512"
    },
    {
      "source": "233488",
      "target": "18567210"
    },
    {
      "source": "233488",
      "target": "21808348"
    },
    {
      "source": "233488",
      "target": "4122592"
    },
    {
      "source": "233488",
      "target": "5323"
    },
    {
      "source": "233488",
      "target": "7398"
    },
    {
      "source": "233488",
      "target": "404048"
    },
    {
      "source": "233488",
      "target": "81196"
    },
    {
      "source": "233488",
      "target": "2581605"
    },
    {
      "source": "233488",
      "target": "801135"
    },
    {
      "source": "233488",
      "target": "4118276"
    },
    {
      "source": "233488",
      "target": "5856528"
    },
    {
      "source": "233488",
      "target": "1175156"
    },
    {
      "source": "233488",
      "target": "1642843"
    },
    {
      "source": "233488",
      "target": "7039"
    },
    {
      "source": "233488",
      "target": "22284121"
    },
    {
      "source": "233488",
      "target": "7519"
    },
    {
      "source": "233488",
      "target": "40409788"
    },
    {
      "source": "233488",
      "target": "33737411"
    },
    {
      "source": "233488",
      "target": "53887"
    },
    {
      "source": "233488",
      "target": "9793263"
    },
    {
      "source": "233488",
      "target": "15905419"
    },
    {
      "source": "233488",
      "target": "416612"
    },
    {
      "source": "233488",
      "target": "554546"
    },
    {
      "source": "233488",
      "target": "5292585"
    },
    {
      "source": "233488",
      "target": "18934432"
    },
    {
      "source": "233488",
      "target": "3369375"
    },
    {
      "source": "233488",
      "target": "331535"
    },
    {
      "source": "233488",
      "target": "1124646"
    },
    {
      "source": "233488",
      "target": "3575651"
    },
    {
      "source": "233488",
      "target": "51443362"
    },
    {
      "source": "233488",
      "target": "11501746"
    },
    {
      "source": "233488",
      "target": "1609808"
    },
    {
      "source": "233488",
      "target": "35458904"
    },
    {
      "source": "233488",
      "target": "8377"
    },
    {
      "source": "233488",
      "target": "2679315"
    },
    {
      "source": "233488",
      "target": "2823113"
    },
    {
      "source": "233488",
      "target": "50568835"
    },
    {
      "source": "233488",
      "target": "2714979"
    },
    {
      "source": "233488",
      "target": "265752"
    },
    {
      "source": "233488",
      "target": "469578"
    },
    {
      "source": "233488",
      "target": "577003"
    },
    {
      "source": "233488",
      "target": "47332350"
    },
    {
      "source": "233488",
      "target": "64396232"
    },
    {
      "source": "233488",
      "target": "32472154"
    },
    {
      "source": "233488",
      "target": "69432561"
    },
    {
      "source": "233488",
      "target": "32472154"
    },
    {
      "source": "233488",
      "target": "43169442"
    },
    {
      "source": "233488",
      "target": "3259263"
    },
    {
      "source": "233488",
      "target": "226660"
    },
    {
      "source": "233488",
      "target": "554671"
    },
    {
      "source": "233488",
      "target": "661384"
    },
    {
      "source": "233488",
      "target": "48813654"
    },
    {
      "source": "233488",
      "target": "330206"
    },
    {
      "source": "233488",
      "target": "52036598"
    },
    {
      "source": "233488",
      "target": "59939845"
    },
    {
      "source": "233488",
      "target": "71912239"
    },
    {
      "source": "233488",
      "target": "1667059"
    },
    {
      "source": "233488",
      "target": "148113"
    },
    {
      "source": "233488",
      "target": "18934863"
    },
    {
      "source": "233488",
      "target": "9933471"
    },
    {
      "source": "233488",
      "target": "579867"
    },
    {
      "source": "233488",
      "target": "204002"
    },
    {
      "source": "233488",
      "target": "14924067"
    },
    {
      "source": "233488",
      "target": "53231"
    },
    {
      "source": "233488",
      "target": "8492"
    },
    {
      "source": "233488",
      "target": "237629"
    },
    {
      "source": "233488",
      "target": "8501"
    },
    {
      "source": "233488",
      "target": "71608"
    },
    {
      "source": "233488",
      "target": "422994"
    },
    {
      "source": "233488",
      "target": "519239"
    },
    {
      "source": "233488",
      "target": "323121"
    },
    {
      "source": "233488",
      "target": "1242713"
    },
    {
      "source": "233488",
      "target": "125297"
    },
    {
      "source": "233488",
      "target": "39208"
    },
    {
      "source": "233488",
      "target": "19834151"
    },
    {
      "source": "233488",
      "target": "8887731"
    },
    {
      "source": "233488",
      "target": "3670866"
    },
    {
      "source": "233488",
      "target": "1944675"
    },
    {
      "source": "233488",
      "target": "891719"
    },
    {
      "source": "233488",
      "target": "64305316"
    },
    {
      "source": "233488",
      "target": "216881"
    },
    {
      "source": "233488",
      "target": "190831"
    },
    {
      "source": "233488",
      "target": "371301"
    },
    {
      "source": "233488",
      "target": "73176768"
    },
    {
      "source": "233488",
      "target": "1336001"
    },
    {
      "source": "233488",
      "target": "46630"
    },
    {
      "source": "233488",
      "target": "46630"
    },
    {
      "source": "233488",
      "target": "1455062"
    },
    {
      "source": "233488",
      "target": "22212276"
    },
    {
      "source": "233488",
      "target": "22212276"
    },
    {
      "source": "233488",
      "target": "22212276"
    },
    {
      "source": "233488",
      "target": "37531624"
    },
    {
      "source": "233488",
      "target": "1010494"
    },
    {
      "source": "233488",
      "target": "2302514"
    },
    {
      "source": "233488",
      "target": "461509"
    },
    {
      "source": "233488",
      "target": "13659583"
    },
    {
      "source": "233488",
      "target": "28801798"
    },
    {
      "source": "233488",
      "target": "190837"
    },
    {
      "source": "233488",
      "target": "46583121"
    },
    {
      "source": "233488",
      "target": "470752"
    },
    {
      "source": "233488",
      "target": "10136"
    },
    {
      "source": "233488",
      "target": "54575571"
    },
    {
      "source": "233488",
      "target": "416589"
    },
    {
      "source": "233488",
      "target": "602401"
    },
    {
      "source": "233488",
      "target": "253492"
    },
    {
      "source": "233488",
      "target": "62683332"
    },
    {
      "source": "233488",
      "target": "43286898"
    },
    {
      "source": "233488",
      "target": "1299404"
    },
    {
      "source": "233488",
      "target": "46207323"
    },
    {
      "source": "233488",
      "target": "46207323"
    },
    {
      "source": "233488",
      "target": "38870173"
    },
    {
      "source": "233488",
      "target": "1299404"
    },
    {
      "source": "233488",
      "target": "60992857"
    },
    {
      "source": "233488",
      "target": "1706332"
    },
    {
      "source": "233488",
      "target": "44359594"
    },
    {
      "source": "233488",
      "target": "14924067"
    },
    {
      "source": "233488",
      "target": "41916"
    },
    {
      "source": "233488",
      "target": "60929882"
    },
    {
      "source": "233488",
      "target": "74609356"
    },
    {
      "source": "233488",
      "target": "161883"
    },
    {
      "source": "233488",
      "target": "351887"
    },
    {
      "source": "233488",
      "target": "2422496"
    },
    {
      "source": "233488",
      "target": "49180"
    },
    {
      "source": "233488",
      "target": "5218"
    },
    {
      "source": "233488",
      "target": "50569499"
    },
    {
      "source": "233488",
      "target": "302944"
    },
    {
      "source": "233488",
      "target": "50523514"
    },
    {
      "source": "233488",
      "target": "74637995"
    },
    {
      "source": "233488",
      "target": "7408685"
    },
    {
      "source": "233488",
      "target": "39156893"
    },
    {
      "source": "233488",
      "target": "12746"
    },
    {
      "source": "233488",
      "target": "747122"
    },
    {
      "source": "233488",
      "target": "50073184"
    },
    {
      "source": "233488",
      "target": "1222578"
    },
    {
      "source": "233488",
      "target": "40254"
    },
    {
      "source": "233488",
      "target": "507174"
    },
    {
      "source": "233488",
      "target": "507174"
    },
    {
      "source": "233488",
      "target": "12398"
    },
    {
      "source": "233488",
      "target": "60898046"
    },
    {
      "source": "233488",
      "target": "50336055"
    },
    {
      "source": "233488",
      "target": "72798"
    },
    {
      "source": "233488",
      "target": "38651188"
    },
    {
      "source": "233488",
      "target": "42411494"
    },
    {
      "source": "233488",
      "target": "41755648"
    },
    {
      "source": "233488",
      "target": "71066378"
    },
    {
      "source": "233488",
      "target": "2671720"
    },
    {
      "source": "233488",
      "target": "201489"
    },
    {
      "source": "233488",
      "target": "4375576"
    },
    {
      "source": "233488",
      "target": "68162942"
    },
    {
      "source": "233488",
      "target": "447298"
    },
    {
      "source": "233488",
      "target": "390214"
    },
    {
      "source": "233488",
      "target": "72607666"
    },
    {
      "source": "233488",
      "target": "203619"
    },
    {
      "source": "233488",
      "target": "607285"
    },
    {
      "source": "233488",
      "target": "2031045"
    },
    {
      "source": "233488",
      "target": "54213333"
    },
    {
      "source": "233488",
      "target": "18426501"
    },
    {
      "source": "233488",
      "target": "22048289"
    },
    {
      "source": "233488",
      "target": "351581"
    },
    {
      "source": "233488",
      "target": "404084"
    },
    {
      "source": "233488",
      "target": "63452"
    },
    {
      "source": "233488",
      "target": "14220429"
    },
    {
      "source": "233488",
      "target": "98770"
    },
    {
      "source": "233488",
      "target": "477573"
    },
    {
      "source": "233488",
      "target": "2894560"
    },
    {
      "source": "233488",
      "target": "22904524"
    },
    {
      "source": "233488",
      "target": "490620"
    },
    {
      "source": "233488",
      "target": "4287389"
    },
    {
      "source": "233488",
      "target": "23534602"
    },
    {
      "source": "233488",
      "target": "2932246"
    },
    {
      "source": "233488",
      "target": "54361643"
    },
    {
      "source": "233488",
      "target": "22584291"
    },
    {
      "source": "233488",
      "target": "55207134"
    },
    {
      "source": "233488",
      "target": "1951226"
    },
    {
      "source": "233488",
      "target": "12953325"
    },
    {
      "source": "233488",
      "target": "14919"
    },
    {
      "source": "233488",
      "target": "234930"
    },
    {
      "source": "233488",
      "target": "29261811"
    },
    {
      "source": "233488",
      "target": "51213339"
    },
    {
      "source": "233488",
      "target": "51097862"
    },
    {
      "source": "233488",
      "target": "46469"
    },
    {
      "source": "233488",
      "target": "476836"
    },
    {
      "source": "233488",
      "target": "3255074"
    },
    {
      "source": "233488",
      "target": "14617"
    },
    {
      "source": "233488",
      "target": "598031"
    },
    {
      "source": "233488",
      "target": "173926"
    },
    {
      "source": "233488",
      "target": "54069"
    },
    {
      "source": "233488",
      "target": "41644056"
    },
    {
      "source": "233488",
      "target": "393736"
    },
    {
      "source": "233488",
      "target": "14539"
    },
    {
      "source": "233488",
      "target": "1194259"
    },
    {
      "source": "233488",
      "target": "487312"
    },
    {
      "source": "233488",
      "target": "15036"
    },
    {
      "source": "233488",
      "target": "237495"
    },
    {
      "source": "233488",
      "target": "14773"
    },
    {
      "source": "233488",
      "target": "30876141"
    },
    {
      "source": "233488",
      "target": "15150"
    },
    {
      "source": "233488",
      "target": "15305"
    },
    {
      "source": "233488",
      "target": "527943"
    },
    {
      "source": "233488",
      "target": "39758073"
    },
    {
      "source": "233488",
      "target": "27855504"
    },
    {
      "source": "233488",
      "target": "59466481"
    },
    {
      "source": "233488",
      "target": "18586449"
    },
    {
      "source": "233488",
      "target": "2614944"
    },
    {
      "source": "233488",
      "target": "805626"
    },
    {
      "source": "233488",
      "target": "59868"
    },
    {
      "source": "233488",
      "target": "113021"
    },
    {
      "source": "233488",
      "target": "113021"
    },
    {
      "source": "233488",
      "target": "309754"
    },
    {
      "source": "233488",
      "target": "61890679"
    },
    {
      "source": "233488",
      "target": "216262"
    },
    {
      "source": "233488",
      "target": "40671069"
    },
    {
      "source": "233488",
      "target": "649572"
    },
    {
      "source": "233488",
      "target": "5721283"
    },
    {
      "source": "233488",
      "target": "405484"
    },
    {
      "source": "233488",
      "target": "39799215"
    },
    {
      "source": "233488",
      "target": "1860407"
    },
    {
      "source": "233488",
      "target": "1775388"
    },
    {
      "source": "233488",
      "target": "24960689"
    },
    {
      "source": "233488",
      "target": "12185719"
    },
    {
      "source": "233488",
      "target": "957422"
    },
    {
      "source": "233488",
      "target": "3424576"
    },
    {
      "source": "233488",
      "target": "8771825"
    },
    {
      "source": "233488",
      "target": "3424576"
    },
    {
      "source": "233488",
      "target": "42253"
    },
    {
      "source": "233488",
      "target": "67944487"
    },
    {
      "source": "233488",
      "target": "16920"
    },
    {
      "source": "233488",
      "target": "61071091"
    },
    {
      "source": "233488",
      "target": "34913689"
    },
    {
      "source": "233488",
      "target": "7914867"
    },
    {
      "source": "233488",
      "target": "17851"
    },
    {
      "source": "233488",
      "target": "1911810"
    },
    {
      "source": "233488",
      "target": "73248112"
    },
    {
      "source": "233488",
      "target": "30806"
    },
    {
      "source": "233488",
      "target": "854461"
    },
    {
      "source": "233488",
      "target": "233488"
    },
    {
      "source": "233488",
      "target": "25050663"
    },
    {
      "source": "233488",
      "target": "4909283"
    },
    {
      "source": "233488",
      "target": "106421"
    },
    {
      "source": "233488",
      "target": "64785522"
    },
    {
      "source": "233488",
      "target": "98974"
    },
    {
      "source": "233488",
      "target": "1470657"
    },
    {
      "source": "233488",
      "target": "48758386"
    },
    {
      "source": "233488",
      "target": "2142"
    },
    {
      "source": "233488",
      "target": "49082762"
    },
    {
      "source": "233488",
      "target": "73733460"
    },
    {
      "source": "233488",
      "target": "454351"
    },
    {
      "source": "233488",
      "target": "27321681"
    },
    {
      "source": "233488",
      "target": "1151991"
    },
    {
      "source": "233488",
      "target": "17927"
    },
    {
      "source": "233488",
      "target": "18152"
    },
    {
      "source": "233488",
      "target": "226631"
    },
    {
      "source": "233488",
      "target": "10711453"
    },
    {
      "source": "233488",
      "target": "442137"
    },
    {
      "source": "233488",
      "target": "44578554"
    },
    {
      "source": "233488",
      "target": "20412"
    },
    {
      "source": "233488",
      "target": "434274"
    },
    {
      "source": "233488",
      "target": "57388949"
    },
    {
      "source": "233488",
      "target": "39227709"
    },
    {
      "source": "233488",
      "target": "52513310"
    },
    {
      "source": "233488",
      "target": "233488"
    },
    {
      "source": "233488",
      "target": "32237314"
    },
    {
      "source": "233488",
      "target": "53802271"
    },
    {
      "source": "233488",
      "target": "53970843"
    },
    {
      "source": "233488",
      "target": "68735447"
    },
    {
      "source": "233488",
      "target": "61373032"
    },
    {
      "source": "233488",
      "target": "11920671"
    },
    {
      "source": "233488",
      "target": "19980"
    },
    {
      "source": "233488",
      "target": "5371104"
    },
    {
      "source": "233488",
      "target": "75795581"
    },
    {
      "source": "233488",
      "target": "2073470"
    },
    {
      "source": "233488",
      "target": "68581881"
    },
    {
      "source": "233488",
      "target": "309261"
    },
    {
      "source": "233488",
      "target": "48777199"
    },
    {
      "source": "233488",
      "target": "516931"
    },
    {
      "source": "233488",
      "target": "350872"
    },
    {
      "source": "233488",
      "target": "59252"
    },
    {
      "source": "233488",
      "target": "1125883"
    },
    {
      "source": "233488",
      "target": "18831"
    },
    {
      "source": "233488",
      "target": "48396"
    },
    {
      "source": "233488",
      "target": "18881"
    },
    {
      "source": "233488",
      "target": "52033"
    },
    {
      "source": "233488",
      "target": "1727027"
    },
    {
      "source": "233488",
      "target": "20556859"
    },
    {
      "source": "233488",
      "target": "253873"
    },
    {
      "source": "233488",
      "target": "10999922"
    },
    {
      "source": "233488",
      "target": "19013767"
    },
    {
      "source": "233488",
      "target": "19013767"
    },
    {
      "source": "233488",
      "target": "20976642"
    },
    {
      "source": "233488",
      "target": "14246162"
    },
    {
      "source": "233488",
      "target": "56906363"
    },
    {
      "source": "233488",
      "target": "4615464"
    },
    {
      "source": "233488",
      "target": "20455"
    },
    {
      "source": "233488",
      "target": "69902039"
    },
    {
      "source": "233488",
      "target": "21017"
    },
    {
      "source": "233488",
      "target": "49830146"
    },
    {
      "source": "233488",
      "target": "61289371"
    },
    {
      "source": "233488",
      "target": "877295"
    },
    {
      "source": "233488",
      "target": "41585002"
    },
    {
      "source": "233488",
      "target": "1773278"
    },
    {
      "source": "233488",
      "target": "346781"
    },
    {
      "source": "233488",
      "target": "62285602"
    },
    {
      "source": "233488",
      "target": "938833"
    },
    {
      "source": "233488",
      "target": "938663"
    },
    {
      "source": "233488",
      "target": "30909817"
    },
    {
      "source": "233488",
      "target": "42171777"
    },
    {
      "source": "233488",
      "target": "46975535"
    },
    {
      "source": "233488",
      "target": "64020"
    },
    {
      "source": "233488",
      "target": "10520679"
    },
    {
      "source": "233488",
      "target": "50347"
    },
    {
      "source": "233488",
      "target": "21659435"
    },
    {
      "source": "233488",
      "target": "555213"
    },
    {
      "source": "233488",
      "target": "87339"
    },
    {
      "source": "233488",
      "target": "55825672"
    },
    {
      "source": "233488",
      "target": "98778"
    },
    {
      "source": "233488",
      "target": "21652"
    },
    {
      "source": "233488",
      "target": "59973182"
    },
    {
      "source": "233488",
      "target": "23348504"
    },
    {
      "source": "233488",
      "target": "21120"
    },
    {
      "source": "233488",
      "target": "175537"
    },
    {
      "source": "233488",
      "target": "9399111"
    },
    {
      "source": "233488",
      "target": "41406"
    },
    {
      "source": "233488",
      "target": "1522954"
    },
    {
      "source": "233488",
      "target": "28030850"
    },
    {
      "source": "233488",
      "target": "38050347"
    },
    {
      "source": "233488",
      "target": "592687"
    },
    {
      "source": "233488",
      "target": "788676"
    },
    {
      "source": "233488",
      "target": "1446517"
    },
    {
      "source": "233488",
      "target": "1699254"
    },
    {
      "source": "233488",
      "target": "17422480"
    },
    {
      "source": "233488",
      "target": "47012074"
    },
    {
      "source": "233488",
      "target": "50568884"
    },
    {
      "source": "233488",
      "target": "47961606"
    },
    {
      "source": "233488",
      "target": "4390806"
    },
    {
      "source": "233488",
      "target": "453086"
    },
    {
      "source": "233488",
      "target": "21120"
    },
    {
      "source": "233488",
      "target": "21120"
    },
    {
      "source": "233488",
      "target": "486492"
    },
    {
      "source": "233488",
      "target": "8016113"
    },
    {
      "source": "233488",
      "target": "3681279"
    },
    {
      "source": "233488",
      "target": "21506"
    },
    {
      "source": "233488",
      "target": "883885"
    },
    {
      "source": "233488",
      "target": "22509799"
    },
    {
      "source": "233488",
      "target": "44577560"
    },
    {
      "source": "233488",
      "target": "1555671"
    },
    {
      "source": "233488",
      "target": "19892153"
    },
    {
      "source": "233488",
      "target": "11053817"
    },
    {
      "source": "233488",
      "target": "277663"
    },
    {
      "source": "233488",
      "target": "48795986"
    },
    {
      "source": "233488",
      "target": "58714104"
    },
    {
      "source": "233488",
      "target": "42129549"
    },
    {
      "source": "233488",
      "target": "22194"
    },
    {
      "source": "233488",
      "target": "65692"
    },
    {
      "source": "233488",
      "target": "43476"
    },
    {
      "source": "233488",
      "target": "49091"
    },
    {
      "source": "233488",
      "target": "51687009"
    },
    {
      "source": "233488",
      "target": "8740192"
    },
    {
      "source": "233488",
      "target": "2539154"
    },
    {
      "source": "233488",
      "target": "1651906"
    },
    {
      "source": "233488",
      "target": "1088262"
    },
    {
      "source": "233488",
      "target": "169633"
    },
    {
      "source": "233488",
      "target": "53587467"
    },
    {
      "source": "233488",
      "target": "173332"
    },
    {
      "source": "233488",
      "target": "503009"
    },
    {
      "source": "233488",
      "target": "22989201"
    },
    {
      "source": "233488",
      "target": "38833779"
    },
    {
      "source": "233488",
      "target": "145162"
    },
    {
      "source": "233488",
      "target": "707411"
    },
    {
      "source": "233488",
      "target": "126706"
    },
    {
      "source": "233488",
      "target": "47790413"
    },
    {
      "source": "233488",
      "target": "24511"
    },
    {
      "source": "233488",
      "target": "172777"
    },
    {
      "source": "233488",
      "target": "60644"
    },
    {
      "source": "233488",
      "target": "11897790"
    },
    {
      "source": "233488",
      "target": "566666"
    },
    {
      "source": "233488",
      "target": "2958015"
    },
    {
      "source": "233488",
      "target": "577742"
    },
    {
      "source": "233488",
      "target": "24833746"
    },
    {
      "source": "233488",
      "target": "42167618"
    },
    {
      "source": "233488",
      "target": "21893202"
    },
    {
      "source": "233488",
      "target": "1504425"
    },
    {
      "source": "233488",
      "target": "144068"
    },
    {
      "source": "233488",
      "target": "2538775"
    },
    {
      "source": "233488",
      "target": "239887"
    },
    {
      "source": "233488",
      "target": "76340"
    },
    {
      "source": "233488",
      "target": "65910"
    },
    {
      "source": "233488",
      "target": "42378348"
    },
    {
      "source": "233488",
      "target": "5017608"
    },
    {
      "source": "233488",
      "target": "22934"
    },
    {
      "source": "233488",
      "target": "154725"
    },
    {
      "source": "233488",
      "target": "380008"
    },
    {
      "source": "233488",
      "target": "658183"
    },
    {
      "source": "233488",
      "target": "276598"
    },
    {
      "source": "233488",
      "target": "23015"
    },
    {
      "source": "233488",
      "target": "3882218"
    },
    {
      "source": "233488",
      "target": "189897"
    },
    {
      "source": "233488",
      "target": "26615065"
    },
    {
      "source": "233488",
      "target": "370882"
    },
    {
      "source": "233488",
      "target": "16598232"
    },
    {
      "source": "233488",
      "target": "57719367"
    },
    {
      "source": "233488",
      "target": "69071767"
    },
    {
      "source": "233488",
      "target": "55330205"
    },
    {
      "source": "233488",
      "target": "49761"
    },
    {
      "source": "233488",
      "target": "30051"
    },
    {
      "source": "233488",
      "target": "1281850"
    },
    {
      "source": "233488",
      "target": "25220"
    },
    {
      "source": "233488",
      "target": "44108758"
    },
    {
      "source": "233488",
      "target": "44300158"
    },
    {
      "source": "233488",
      "target": "33283283"
    },
    {
      "source": "233488",
      "target": "1363880"
    },
    {
      "source": "233488",
      "target": "1089270"
    },
    {
      "source": "233488",
      "target": "25685"
    },
    {
      "source": "233488",
      "target": "495383"
    },
    {
      "source": "233488",
      "target": "5100865"
    },
    {
      "source": "233488",
      "target": "402673"
    },
    {
      "source": "233488",
      "target": "248454"
    },
    {
      "source": "233488",
      "target": "25767"
    },
    {
      "source": "233488",
      "target": "20646438"
    },
    {
      "source": "233488",
      "target": "20646438"
    },
    {
      "source": "233488",
      "target": "922505"
    },
    {
      "source": "233488",
      "target": "596646"
    },
    {
      "source": "233488",
      "target": "37862937"
    },
    {
      "source": "233488",
      "target": "1706303"
    },
    {
      "source": "233488",
      "target": "1167455"
    },
    {
      "source": "233488",
      "target": "826997"
    },
    {
      "source": "233488",
      "target": "2009061"
    },
    {
      "source": "233488",
      "target": "63451675"
    },
    {
      "source": "233488",
      "target": "66294"
    },
    {
      "source": "233488",
      "target": "4195092"
    },
    {
      "source": "233488",
      "target": "6604"
    },
    {
      "source": "233488",
      "target": "522449"
    },
    {
      "source": "233488",
      "target": "10667750"
    },
    {
      "source": "233488",
      "target": "55867424"
    },
    {
      "source": "233488",
      "target": "33742232"
    },
    {
      "source": "233488",
      "target": "21280907"
    },
    {
      "source": "233488",
      "target": "954328"
    },
    {
      "source": "233488",
      "target": "31404681"
    },
    {
      "source": "233488",
      "target": "175885"
    },
    {
      "source": "233488",
      "target": "3290880"
    },
    {
      "source": "233488",
      "target": "1167036"
    },
    {
      "source": "233488",
      "target": "20903754"
    },
    {
      "source": "233488",
      "target": "51997474"
    },
    {
      "source": "233488",
      "target": "48455863"
    },
    {
      "source": "233488",
      "target": "990677"
    },
    {
      "source": "233488",
      "target": "20588127"
    },
    {
      "source": "233488",
      "target": "2079997"
    },
    {
      "source": "233488",
      "target": "160361"
    },
    {
      "source": "233488",
      "target": "33490859"
    },
    {
      "source": "233488",
      "target": "28249"
    },
    {
      "source": "233488",
      "target": "4059023"
    },
    {
      "source": "233488",
      "target": "2471540"
    },
    {
      "source": "233488",
      "target": "29015809"
    },
    {
      "source": "233488",
      "target": "245926"
    },
    {
      "source": "233488",
      "target": "76996"
    },
    {
      "source": "233488",
      "target": "70721875"
    },
    {
      "source": "233488",
      "target": "67902375"
    },
    {
      "source": "233488",
      "target": "14271782"
    },
    {
      "source": "233488",
      "target": "397608"
    },
    {
      "source": "233488",
      "target": "60968880"
    },
    {
      "source": "233488",
      "target": "5599330"
    },
    {
      "source": "233488",
      "target": "143394"
    },
    {
      "source": "233488",
      "target": "6435232"
    },
    {
      "source": "233488",
      "target": "62607005"
    },
    {
      "source": "233488",
      "target": "37895661"
    },
    {
      "source": "233488",
      "target": "4986804"
    },
    {
      "source": "233488",
      "target": "9517150"
    },
    {
      "source": "233488",
      "target": "87210"
    },
    {
      "source": "233488",
      "target": "38059657"
    },
    {
      "source": "233488",
      "target": "49648894"
    },
    {
      "source": "233488",
      "target": "25765920"
    },
    {
      "source": "233488",
      "target": "1416993"
    },
    {
      "source": "233488",
      "target": "34327569"
    },
    {
      "source": "233488",
      "target": "222034"
    },
    {
      "source": "233488",
      "target": "6152185"
    },
    {
      "source": "233488",
      "target": "430106"
    },
    {
      "source": "233488",
      "target": "165180"
    },
    {
      "source": "233488",
      "target": "33408418"
    },
    {
      "source": "233488",
      "target": "1688759"
    },
    {
      "source": "233488",
      "target": "223325"
    },
    {
      "source": "233488",
      "target": "248932"
    },
    {
      "source": "233488",
      "target": "23407868"
    },
    {
      "source": "233488",
      "target": "27010"
    },
    {
      "source": "233488",
      "target": "768211"
    },
    {
      "source": "233488",
      "target": "780960"
    },
    {
      "source": "233488",
      "target": "1301906"
    },
    {
      "source": "233488",
      "target": "9570763"
    },
    {
      "source": "233488",
      "target": "442656"
    },
    {
      "source": "233488",
      "target": "457579"
    },
    {
      "source": "233488",
      "target": "26817320"
    },
    {
      "source": "233488",
      "target": "6147487"
    },
    {
      "source": "233488",
      "target": "48813654"
    },
    {
      "source": "233488",
      "target": "341015"
    },
    {
      "source": "233488",
      "target": "29468"
    },
    {
      "source": "233488",
      "target": "35891416"
    },
    {
      "source": "233488",
      "target": "10159567"
    },
    {
      "source": "233488",
      "target": "19135734"
    },
    {
      "source": "233488",
      "target": "47538581"
    },
    {
      "source": "233488",
      "target": "1418949"
    },
    {
      "source": "233488",
      "target": "71642695"
    },
    {
      "source": "233488",
      "target": "10584297"
    },
    {
      "source": "233488",
      "target": "1579244"
    },
    {
      "source": "233488",
      "target": "233488"
    },
    {
      "source": "233488",
      "target": "38523090"
    },
    {
      "source": "233488",
      "target": "1053303"
    },
    {
      "source": "233488",
      "target": "36408395"
    },
    {
      "source": "233488",
      "target": "27576"
    },
    {
      "source": "233488",
      "target": "26685"
    },
    {
      "source": "233488",
      "target": "2593441"
    },
    {
      "source": "233488",
      "target": "179426"
    },
    {
      "source": "233488",
      "target": "1180641"
    },
    {
      "source": "233488",
      "target": "47895"
    },
    {
      "source": "233488",
      "target": "6319351"
    },
    {
      "source": "233488",
      "target": "3069503"
    },
    {
      "source": "233488",
      "target": "27260435"
    },
    {
      "source": "233488",
      "target": "566689"
    },
    {
      "source": "233488",
      "target": "26980"
    },
    {
      "source": "233488",
      "target": "20926"
    },
    {
      "source": "233488",
      "target": "65309"
    },
    {
      "source": "233488",
      "target": "65309"
    },
    {
      "source": "233488",
      "target": "762988"
    },
    {
      "source": "233488",
      "target": "339417"
    },
    {
      "source": "233488",
      "target": "339417"
    },
    {
      "source": "233488",
      "target": "681654"
    },
    {
      "source": "233488",
      "target": "6069850"
    },
    {
      "source": "233488",
      "target": "100563"
    },
    {
      "source": "233488",
      "target": "39758474"
    },
    {
      "source": "233488",
      "target": "33094374"
    },
    {
      "source": "233488",
      "target": "1209759"
    },
    {
      "source": "233488",
      "target": "486876"
    },
    {
      "source": "233488",
      "target": "50673241"
    },
    {
      "source": "233488",
      "target": "499010"
    },
    {
      "source": "233488",
      "target": "1514392"
    },
    {
      "source": "233488",
      "target": "53887"
    },
    {
      "source": "233488",
      "target": "47937215"
    },
    {
      "source": "233488",
      "target": "47089"
    },
    {
      "source": "233488",
      "target": "173070"
    },
    {
      "source": "233488",
      "target": "33520809"
    },
    {
      "source": "233488",
      "target": "323392"
    },
    {
      "source": "233488",
      "target": "30402"
    },
    {
      "source": "233488",
      "target": "31600"
    },
    {
      "source": "233488",
      "target": "405944"
    },
    {
      "source": "233488",
      "target": "406624"
    },
    {
      "source": "233488",
      "target": "12413470"
    },
    {
      "source": "233488",
      "target": "50828755"
    },
    {
      "source": "233488",
      "target": "33275304"
    },
    {
      "source": "233488",
      "target": "25757480"
    },
    {
      "source": "233488",
      "target": "995908"
    },
    {
      "source": "233488",
      "target": "28934119"
    },
    {
      "source": "233488",
      "target": "42571226"
    },
    {
      "source": "233488",
      "target": "68639524"
    },
    {
      "source": "233488",
      "target": "922505"
    },
    {
      "source": "233488",
      "target": "1514392"
    },
    {
      "source": "233488",
      "target": "171623"
    },
    {
      "source": "233488",
      "target": "61603971"
    },
    {
      "source": "233488",
      "target": "35425965"
    },
    {
      "source": "233488",
      "target": "57179040"
    },
    {
      "source": "233488",
      "target": "31871"
    },
    {
      "source": "233488",
      "target": "5987648"
    },
    {
      "source": "233488",
      "target": "20926"
    },
    {
      "source": "233488",
      "target": "305843"
    },
    {
      "source": "233488",
      "target": "62078649"
    },
    {
      "source": "233488",
      "target": "32823"
    },
    {
      "source": "233488",
      "target": "5363"
    },
    {
      "source": "233488",
      "target": "226779"
    },
    {
      "source": "233488",
      "target": "50228744"
    },
    {
      "source": "233488",
      "target": "68212199"
    },
    {
      "source": "233488",
      "target": "660850"
    },
    {
      "source": "233488",
      "target": "302949"
    },
    {
      "source": "233488",
      "target": "44508"
    },
    {
      "source": "233488",
      "target": "22584291"
    },
    {
      "source": "233488",
      "target": "50903"
    },
    {
      "source": "233488",
      "target": "23538754"
    },
    {
      "source": "233488",
      "target": "60968880"
    },
    {
      "source": "233488",
      "target": "33109"
    },
    {
      "source": "233488",
      "target": "42253"
    },
    {
      "source": "233488",
      "target": "3003284"
    },
    {
      "source": "233488",
      "target": "3829034"
    },
    {
      "source": "233488",
      "target": "74575461"
    },
    {
      "source": "233488",
      "target": "768799"
    },
    {
      "source": "233488",
      "target": "47527969"
    },
    {
      "source": "233488",
      "target": "57307464"
    },
    {
      "source": "233488",
      "target": "33139"
    },
    {
      "source": "233488",
      "target": "23534873"
    },
    {
      "source": "233488",
      "target": "43375904"
    },
    {
      "source": "233488",
      "target": "47749536"
    }
  ]
}