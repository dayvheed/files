{
  "nodes": [
    {
      "id": "455220",
      "title": "Data engineering",
      "url": "https://en.wikipedia.org/wiki/Data_engineering",
      "summary": "Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science; which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing.\n\n"
    },
    {
      "id": "2381958",
      "title": "Conceptual model",
      "url": "https://en.wikipedia.org/wiki/Conceptual_model",
      "summary": "The term conceptual model refers to any model that is formed after a conceptualization or generalization process. Conceptual models are often abstractions of things in the real world, whether physical or social. Semantic studies are relevant to various stages of concept formation. Semantics is fundamentally a study of concepts, the meaning that thinking beings give to various elements of their experience.\n\n"
    },
    {
      "id": "639009",
      "title": "Agile software development",
      "url": "https://en.wikipedia.org/wiki/Agile_software_development",
      "summary": "In software development, agile practices (sometimes written \"Agile\") include requirements, discovery and solutions improvement through the collaborative effort of self-organizing and cross-functional teams with their customer(s)/end user(s). Popularized in the 2001 Manifesto for Agile Software Development, these values and principles were derived from, and underpin, a broad range of software development frameworks, including Scrum and Kanban.While there is much anecdotal evidence that adopting agile practices and values improves the effectiveness of software professionals, teams and organizations, the empirical evidence is mixed and hard to find."
    },
    {
      "id": "775",
      "title": "Algorithm",
      "url": "https://en.wikipedia.org/wiki/Algorithm",
      "summary": "In mathematics and computer science, an algorithm ( ) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as \"memory\", \"search\" and \"stimulus\".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.  For example, social media recommender systems rely on heuristics in such a way that, although widely characterized as \"algorithms\" in 21st century popular media, cannot deliver correct results due to the nature of the problem.\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n"
    },
    {
      "id": "90451",
      "title": "Amazon (company)",
      "url": "https://en.wikipedia.org/wiki/Amazon_(company)",
      "summary": "Amazon.com, Inc., doing business as Amazon ( AM-\u0259-zon, UK also  AM-\u0259-z\u0259n), is an American multinational technology company focusing on e-commerce, cloud computing, online advertising, digital streaming, and artificial intelligence. It is considered one of the Big Five American technology companies; the other four are Alphabet (parent company of Google), Apple, Meta (parent company of Facebook), and Microsoft.\nAmazon was founded on July 5, 1994 by Jeff Bezos from his garage in Bellevue, Washington. The company initially was an online marketplace for books, but incrementally expanded into a multitude of product categories, a strategy that has earned it the moniker \"The Everything Store\".The company has multiple subsidiaries, including Amazon Web Services, providing cloud computing, Zoox, a self-driving car division, Kuiper Systems, a satellite Internet provider, and Amazon Lab126, a computer hardware R&D provider. Other subsidiaries include Ring, Twitch, IMDb, and Whole Foods Market. Its acquisition of Whole Foods in August 2017 for US$13.4 billion substantially increased its market share and presence as a physical retailer.Amazon has a reputation as a disruptor of industries through technological innovation and aggressive reinvestment of profits into capital expenditures. As of 2023, it is the world's largest online retailer and marketplace, smart speaker provider, cloud computing service through AWS, live-streaming service through Twitch, and Internet company as measured by revenue and market share. In 2021, it surpassed Walmart as the world's largest retailer outside of China, driven in large part by its paid subscription plan, Amazon Prime, which has close to 200 million subscribers worldwide. It is the second-largest private employer in the United States.As of October 2023, Amazon is the 12th-most visited website in the world with 82% of its traffic coming from the United States.Amazon also distributes a variety of downloadable and streaming content through its Amazon Prime Video, MGM+, Amazon Music, Twitch, and Audible units. It publishes books through its publishing arm, Amazon Publishing, film and television content through Amazon MGM Studios, including the Metro-Goldwyn-Mayer studio which acquired in March 2022. It also produces consumer electronics\u2014most notably, Kindle e-readers, Echo devices, Fire tablets, and Fire TVs.\nAmazon has been criticized for customer data collection practices, a toxic work culture, censorship, tax avoidance, and anti-competitive behavior.\n\n"
    },
    {
      "id": "61928892",
      "title": "Apache Airflow",
      "url": "https://en.wikipedia.org/wiki/Apache_Airflow",
      "summary": "Apache Airflow is an open-source workflow management platform for data engineering pipelines. It started at Airbnb in October 2014 as a solution to manage the company's increasingly complex workflows. Creating Airflow allowed Airbnb to programmatically author and schedule their workflows and monitor them via the built-in Airflow user interface. From the beginning, the project was made open source, becoming an Apache Incubator project in March 2016 and a top-level Apache Software Foundation project in January 2019.\nAirflow is written in Python, and workflows are created via Python scripts. Airflow is designed under the principle of \"configuration as code\". While other \"configuration as code\" workflow platforms exist using markup languages like XML, using Python allows developers to import libraries and classes to help them create their workflows."
    },
    {
      "id": "42164234",
      "title": "Apache Spark",
      "url": "https://en.wikipedia.org/wiki/Apache_Spark",
      "summary": "Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since."
    },
    {
      "id": "1164",
      "title": "Artificial intelligence",
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
      "summary": "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of other living beings, primarily of humans. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\nAI technology is widely used throughout industry, government, and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), interacting via human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).Alan Turing was the first person to conduct substantial research in the field that he called Machine Intelligence. Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism followed by disappointment and loss of funding. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI spring of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\n\n"
    },
    {
      "id": "27051151",
      "title": "Big data",
      "url": "https://en.wikipedia.org/wiki/Big_data",
      "summary": "Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.\"\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17\u00d7260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than \u20ac100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\""
    },
    {
      "id": "1364506",
      "title": "Binary data",
      "url": "https://en.wikipedia.org/wiki/Binary_data",
      "summary": "Binary data is data whose unit can take on only two possible states. These are often labelled as 0 and 1 in accordance with the binary numeral system and Boolean algebra.\nBinary data occurs in many different technical and scientific fields, where it can be called by different names including bit (binary digit) in computer science, truth value in mathematical logic and related domains and binary variable in statistics.\n\n"
    },
    {
      "id": "2371482",
      "title": "Business analysis",
      "url": "https://en.wikipedia.org/wiki/Business_analysis",
      "summary": "Business analysis is a professional discipline focused on identifying business needs and determining solutions to business problems. Solutions may include a software-systems development component, process improvements, or organizational changes, and may involve extensive analysis, strategic planning and policy development. A person dedicated to carrying out these tasks within an organization is called a business analyst or BA.Business analysts are not found solely within projects for developing software systems. They may also work across the organisation, solving business problems in consultation with business stakeholders. Whilst most of the work that business analysts do today relates to software development / solutions, this is due to the ongoing massive changes businesses all over the world are experiencing in their attempts to digitise.Although there are different role definitions, depending upon the organization, there does seem to be an area of common ground where most\nbusiness analysts work. The responsibilities appear to be:\n\nTo investigate business systems, taking a holistic view of the situation. This may include examining elements of the organisation structures and staff development issues as well as current processes and IT systems.\nTo evaluate actions to improve the operation of a business system. Again, this may require an examination of organisational structure and staff development needs, to ensure that they are in line with any proposed process redesign and IT system development.\nTo document the business requirements for the IT system support using appropriate documentation standards.In line with this, the core business analyst role could be defined as an internal consultancy role that has the responsibility for investigating business situations, identifying and evaluating options for improving business systems, defining requirements and ensuring the effective use of information systems in meeting the needs of the business."
    },
    {
      "id": "168387",
      "title": "Business intelligence",
      "url": "https://en.wikipedia.org/wiki/Business_intelligence",
      "summary": "Business intelligence comprises the strategies and technologies used by enterprises for the data analysis and management of business information. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics. \nBI tools can handle large amounts of structured and sometimes unstructured data to help identify, develop, and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability, and help them take strategic decisions.Business intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals, and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an \"intelligence\" that cannot be derived from any singular set of data.Among myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments, and to gauge the impact of marketing efforts.BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as \"BI/DW\"\nor as \"BIDW\". A data warehouse contains a copy of analytical data that facilitates decision support.\n\n"
    },
    {
      "id": "1040299",
      "title": "Clive Finkelstein",
      "url": "https://en.wikipedia.org/wiki/Clive_Finkelstein",
      "summary": "Clive Finkelstein (born ca. 1939 died 9/12/2021) is an Australian computer scientist, known as the \"Father\" of information engineering methodology."
    },
    {
      "id": "19541494",
      "title": "Cloud computing",
      "url": "https://en.wikipedia.org/wiki/Cloud_computing",
      "summary": "Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each of which is a data center. Cloud computing relies on sharing of resources to achieve coherence and typically uses a pay-as-you-go model, which can help in reducing capital expenses but may also lead to unexpected operating expenses for users."
    },
    {
      "id": "5300",
      "title": "Computer data storage",
      "url": "https://en.wikipedia.org/wiki/Computer_data_storage",
      "summary": "Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.:\u200a15\u201316\u200aThe central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy,:\u200a468\u2013473\u200a which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally, the fast technologies are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".\nEven the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.\n\n"
    },
    {
      "id": "7077",
      "title": "Computer file",
      "url": "https://en.wikipedia.org/wiki/Computer_file",
      "summary": "In computing, a computer file is a resource for recording data on a computer storage device, primarily identified by its filename. Just as words can be written on paper, so can data be written to a computer file. Files can be shared with and transferred between computers and mobile devices via removable media, networks, or the Internet.\nDifferent types of computer files are designed for different purposes. A file may be designed to store an image, a written message, a video, a program, or any wide variety of other kinds of data. Certain files can store multiple data types at once.\nBy using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times.\nFiles are typically organized in a file system, which tracks file locations on the disk and enables user access.\n\n"
    },
    {
      "id": "5323",
      "title": "Computer science",
      "url": "https://en.wikipedia.org/wiki/Computer_science",
      "summary": "Computer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). Though more often considered an academic discipline, computer science is closely related to computer programming.Algorithms and data structures are central to computer science.\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human\u2013computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\n\n"
    },
    {
      "id": "7398",
      "title": "Computer security",
      "url": "https://en.wikipedia.org/wiki/Computer_security",
      "summary": "Computer security, cybersecurity, digital security or information technology security (IT security) is the protection of computer systems and networks from attacks by malicious actors that may result in unauthorized information disclosure, theft of, or damage to hardware, software, or data, as well as from the disruption or misdirection of the services they provide.The field is significant due to the expanded reliance on computer systems, the Internet, and wireless network standards such as Bluetooth and Wi-Fi. Also, due to the growth of smart devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is one of the most significant challenges of the contemporary world, due to both the complexity of information systems and the societies they support. Security is of especially high importance for systems that govern large-scale systems with far-reaching physical effects, such as power distribution, elections, and finance.\n\n"
    },
    {
      "id": "2720954",
      "title": "Data analysis",
      "url": "https://en.wikipedia.org/wiki/Data_analysis",
      "summary": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.Data mining is a particular data analysis technique that focuses on statistical modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.\n\n"
    },
    {
      "id": "34296790",
      "title": "Data infrastructure",
      "url": "https://en.wikipedia.org/wiki/Data_infrastructure",
      "summary": "A data infrastructure is a digital infrastructure promoting data sharing and consumption. \nSimilarly to other infrastructures, it is a structure needed for the operation of a society as well as the services and facilities necessary for an economy to function, the data economy in this case.\n\n"
    },
    {
      "id": "46626475",
      "title": "Data lake",
      "url": "https://en.wikipedia.org/wiki/Data_lake",
      "summary": "A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and binary data (images, audio, video). A data lake can be established \"on premises\" (within an organization's data centers) or \"in the cloud\" (using cloud services from vendors such as Amazon, Microsoft, Oracle Cloud, or Google).\n\n"
    },
    {
      "id": "82871",
      "title": "Data model",
      "url": "https://en.wikipedia.org/wiki/Data_model",
      "summary": "A data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner.\nThe corresponding professional activity is called generally data modeling or, more specifically, database design.\nData models are typically specified by a data expert, data specialist, data scientist, data librarian, or a data scholar. \nA data modeling language and notation are often represented in graphical form as diagrams.A data model can sometimes be referred to as a data structure, especially in the context of programming languages. Data models are often complemented by function models, especially in the context of enterprise models.\nA data model explicitly determines the structure of data; conversely, structured data is data organized according to an explicit data model or data structure. Structured data is in contrast to unstructured data and semi-structured data.\n\n"
    },
    {
      "id": "759422",
      "title": "Data modeling",
      "url": "https://en.wikipedia.org/wiki/Data_modeling",
      "summary": "Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. It may be applied as part of broader Model-driven engineering (MDD) concept.\n\n"
    },
    {
      "id": "41961",
      "title": "Data processing",
      "url": "https://en.wikipedia.org/wiki/Data_processing",
      "summary": "Data processing is the collection and manipulation of digital data to produce meaningful information. Data processing is a form of information processing, which is the modification (processing) of information in any manner detectable by an observer.\n\n"
    },
    {
      "id": "237536",
      "title": "Information privacy",
      "url": "https://en.wikipedia.org/wiki/Information_privacy",
      "summary": "Information privacy is the relationship between the collection and dissemination of data, technology, the public expectation of privacy, contextual information norms, and the legal and political issues surrounding them.  It is also known as data privacy or data protection.\n\n"
    },
    {
      "id": "35458904",
      "title": "Data science",
      "url": "https://en.wikipedia.org/wiki/Data_science",
      "summary": "Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine). Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.A  data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.\n\n"
    },
    {
      "id": "7990",
      "title": "Data warehouse",
      "url": "https://en.wikipedia.org/wiki/Data_warehouse",
      "summary": "In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis and is considered a core component of business intelligence. Data warehouses are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise. This is beneficial for companies as it enables them to interrogate and draw insights from their data and make decisions.\nThe data stored in the warehouse is uploaded from the operational systems (such as marketing or sales). The data may pass through an operational data store and may require data cleansing for additional operations to ensure data quality before it is used in the data warehouse for reporting. \nExtract, transform, load (ETL) and extract, load, transform (ELT) are the two main approaches used to build a data warehouse system.\n\n"
    },
    {
      "id": "8078610",
      "title": "Database administration",
      "url": "https://en.wikipedia.org/wiki/Database_administration",
      "summary": "Database administration is the function of managing and maintaining database management systems (DBMS) software. Mainstream DBMS software such as Oracle, IBM Db2 and Microsoft SQL Server need ongoing management. As such, corporations that use DBMS software often hire specialized information technology personnel called database administrators or DBAs.\n\n"
    },
    {
      "id": "1040387",
      "title": "Database design",
      "url": "https://en.wikipedia.org/wiki/Database_design",
      "summary": "Database design is the organization of data according to a database model. The designer determines what data must be stored and how the data elements interrelate. With this information, they can begin to fit the data to the database model.\nA database management system manages the data accordingly.\nDatabase design involves classifying data and identifying interrelationships. This theoretical representation of the data is called an ontology.\n\n"
    },
    {
      "id": "8377",
      "title": "Database",
      "url": "https://en.wikipedia.org/wiki/Database",
      "summary": "In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\nSmall databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.\nComputer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.\n\n"
    },
    {
      "id": "1079396",
      "title": "Dataflow programming",
      "url": "https://en.wikipedia.org/wiki/Dataflow_programming",
      "summary": "In computer programming, dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. Dataflow programming languages share some features of functional languages, and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing. Some authors use the term datastream instead of dataflow to avoid confusion with dataflow computing or dataflow architecture, based on an indeterministic machine paradigm. Dataflow programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.\n\n"
    },
    {
      "id": "32472154",
      "title": "Deep learning",
      "url": "https://en.wikipedia.org/wiki/Deep_learning",
      "summary": "Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog. ANNs are generally seen as low quality models for brain function."
    },
    {
      "id": "204002",
      "title": "Directed acyclic graph",
      "url": "https://en.wikipedia.org/wiki/Directed_acyclic_graph",
      "summary": "In mathematics, particularly graph theory, and computer science, a directed acyclic graph (DAG) is a directed graph with no directed cycles. That is, it consists of vertices and edges (also called arcs), with each edge directed from one vertex to another, such that following those directions will never form a closed loop. A directed graph is a DAG if and only if it can be topologically ordered, by arranging the vertices as a linear ordering that is consistent with all edge directions. DAGs have numerous scientific and computational applications, ranging from biology (evolution, family trees, epidemiology) to information science (citation networks) to computation (scheduling).\nDirected acyclic graphs are sometimes instead called acyclic directed graphs or acyclic digraphs."
    },
    {
      "id": "19721986",
      "title": "Directed graph",
      "url": "https://en.wikipedia.org/wiki/Directed_graph",
      "summary": "In mathematics, and more specifically in graph theory, a directed graph (or digraph) is a graph that is made up of a set of vertices connected by directed edges, often called arcs.\n\n"
    },
    {
      "id": "422994",
      "title": "Digital object identifier",
      "url": "https://en.wikipedia.org/wiki/Digital_object_identifier",
      "summary": "A digital object identifier (DOI) is a persistent identifier or handle used to uniquely identify various objects, standardized by the International Organization for Standardization (ISO). DOIs are an implementation of the Handle System; they also fit within the URI system (Uniform Resource Identifier). They are widely used to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. \nA DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL where the object is located. Thus, by being actionable and interoperable, a DOI differs from ISBNs or ISRCs which are identifiers only. The DOI system uses the indecs Content Model for representing metadata.\nThe DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL. It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link, leaving the DOI useless.The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. The DOI system is implemented through a federation of registration agencies coordinated by the IDF. By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations, and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations."
    },
    {
      "id": "239516",
      "title": "Extract, transform, load",
      "url": "https://en.wikipedia.org/wiki/Extract,_transform,_load",
      "summary": "In computing, extract, transform, load (ETL) is a three-phase process where data is extracted, transformed (cleaned, sanitized, scrubbed) and loaded into an output data container. The data can be collated from one or more sources and it can also be output to one or more destinations. ETL processing is typically executed using software applications but it can also be done manually by system operators. ETL software typically automates the entire process and can be run manually or on reccurring schedules either as single jobs or aggregated into a batch of jobs.\n\nA properly designed ETL system extracts data from source systems and enforces data type and data validity standards and ensures it conforms structurally to the requirements of the output. Some ETL systems can also deliver data in a presentation-ready format so that application developers can build applications and end users can make decisions.The ETL process is often used in data warehousing. ETL systems commonly integrate data from multiple applications (systems), typically developed and supported by different vendors or hosted on separate computer hardware. The separate systems containing the original data are frequently managed and operated by different stakeholders. For example, a cost accounting system may combine data from payroll, sales, and purchasing.\nData extraction involves extracting data from homogeneous or heterogeneous sources; data transformation processes data by data cleaning and transforming it into a proper storage format/structure for the purposes of querying and analysis; finally, data loading describes the insertion of data into the final target database such as an operational data store, a data mart, data lake or a data warehouse.\n\n"
    },
    {
      "id": "5603080",
      "title": "Face book",
      "url": "https://en.wikipedia.org/wiki/Face_book",
      "summary": "A face book or facebook is a paper or online directory of individuals' photographs and names published by some American universities. In particular, the term denotes publications of this type distributed by university administrations at the start of the academic year, with the intention of helping students to get to know each other."
    },
    {
      "id": "1010280",
      "title": "File system",
      "url": "https://en.wikipedia.org/wiki/File_system",
      "summary": "In computing, a file system or filesystem (often abbreviated to fs) is a method and data structure that the operating system uses to control how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stopped and the next began, or where any piece of data was located when it was time to retrieve it. By separating the data into pieces and giving each piece a name, the data are easily isolated and identified. Taking its name from the way a paper-based data management system is named, each group of data is called a \"file\". The structure and logic rules used to manage the groups of data and their names is called a \"file system.\"\nThere are many kinds of file systems, each with unique structure and logic, properties of speed, flexibility, security, size and more. Some file systems have been designed to be used for specific applications. For example, the ISO 9660 and UDF file systems are designed specifically for optical discs.\nFile systems can be used on many types of storage devices using various media. Introduced by IBM in 1956, HDDs (hard disk drives) beginning in the early 1960s were, and still are, the dominant secondary storage device for general-purpose computers and are projected to remain so for the foreseeable future. Other kinds of media that are used include SSDs, magnetic tapes, and optical discs. In some cases, such as with tmpfs, the computer's main memory (random-access memory, RAM) is used to create a temporary file system for short-term use.\n\nSome file systems are used on local data storage devices; others provide file access via a network protocol (for example, NFS, SMB, or 9P clients).  Some file systems are \"virtual\", meaning that the supplied \"files\" (called virtual files) are computed on request (such as procfs and sysfs) or are merely a mapping into a different file system used as a backing store.  The file system manages access to both the content of files and the metadata about those files.  It is responsible for arranging storage space; reliability, efficiency, and tuning with regard to the physical storage medium are important design considerations."
    },
    {
      "id": "13777",
      "title": "Hard disk drive",
      "url": "https://en.wikipedia.org/wiki/Hard_disk_drive",
      "summary": "A hard disk drive (HDD), hard disk, hard drive, or fixed disk, is an electro-mechanical data storage device that stores and retrieves digital data using magnetic storage with one or more rigid rapidly rotating platters coated with magnetic material. The platters are paired with magnetic heads, usually arranged on a moving actuator arm, which read and write data to the platter surfaces. Data is accessed in a random-access manner, meaning that individual blocks of data can be stored and retrieved in any order. HDDs are a type of non-volatile storage, retaining stored data when powered off. Modern HDDs are typically in the form of a small rectangular box.\nIntroduced by IBM in 1956, HDDs were the dominant secondary storage device for general-purpose computers beginning in the early 1960s. HDDs maintained this position into the modern era of servers and personal computers, though personal computing devices produced in large volume, like mobile phones and tablets, rely on flash memory storage devices. More than 224 companies have produced HDDs historically, though after extensive industry consolidation, most units are manufactured by Seagate, Toshiba, and Western Digital. HDDs dominate the volume of storage produced (exabytes per year) for servers. Though production is growing slowly (by exabytes shipped), sales revenues and unit shipments are declining, because solid-state drives (SSDs) have higher data-transfer rates, higher areal storage density, somewhat better reliability, and much lower latency and access times.The revenues for SSDs, most of which use NAND flash memory, slightly exceeded those for HDDs in 2018. Flash storage products had more than twice the revenue of hard disk drives as of 2017. Though SSDs have four to nine times higher cost per bit, they are replacing HDDs in applications where speed, power consumption, small size, high capacity and durability are important. As of 2019, the cost per bit of SSDs is falling, and the price premium over HDDs has narrowed.The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of 1000: a 1-terabyte (TB) drive has a capacity of 1,000 gigabytes (GB; where 1 gigabyte = 1 000 megabytes = 1 000 000 kilobytes (1 million) = 1 000 000 000 bytes (1 billion)). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. There can be confusion regarding storage capacity, since capacities are stated in decimal gigabytes (powers of 1000) by HDD manufacturers, whereas the most commonly used operating systems report capacities in powers of 1024, which results in a smaller number than advertised. Performance is specified as the time required to move the heads to a track or cylinder (average access time), the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally, the speed at which the data is transmitted (data rate).\nThe two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as PATA (Parallel ATA), SATA (Serial ATA), USB or SAS (Serial Attached SCSI) cables.\n\n"
    },
    {
      "id": "185529",
      "title": "Scalability",
      "url": "https://en.wikipedia.org/wiki/Scalability",
      "summary": "Scalability is the property of a system to handle a growing amount of work.  One definition for software systems specifies that this may be done by adding resources to the system.In an economic context, a scalable business model implies that a company can increase sales given increased resources. For example, a package delivery system is scalable because more packages can be delivered by adding more delivery vehicles. However, if all packages had to first pass through a single warehouse for sorting, the system would not be as scalable, because one warehouse can handle only a limited number of packages.In computing, scalability is a characteristic of computers, networks, algorithms, networking protocols, programs and applications. An example is a search engine, which must support increasing numbers of users, and the number of topics it indexes. Webscale is a computer architectural approach that brings the capabilities of large-scale cloud computing companies into enterprise data centers.In distributed systems, there are several definitions according to the authors, some considering the concepts of scalability a sub-part of elasticity, others as being distinct.\nIn mathematics, scalability mostly refers to closure under scalar multiplication.\nIn industrial engineering and manufacturing, scalability refers to the capacity of a process, system, or organization to handle a growing workload, adapt to increasing demands, and maintain operational efficiency. A scalable system can effectively manage increased production volumes, new product lines, or expanding markets without compromising quality or performance. In this context, scalability is a vital consideration for businesses aiming to meet customer expectations, remain competitive, and achieve sustainable growth. Factors influencing scalability include the flexibility of the production process, the adaptability of the workforce, and the integration of advanced technologies. By implementing scalable solutions, companies can optimize resource utilization, reduce costs, and streamline their operations. Scalability in industrial engineering and manufacturing enables businesses to respond to fluctuating market conditions, capitalize on emerging opportunities, and thrive in an ever-evolving global landscape."
    },
    {
      "id": "56938",
      "title": "Institute of Electrical and Electronics Engineers",
      "url": "https://en.wikipedia.org/wiki/Institute_of_Electrical_and_Electronics_Engineers",
      "summary": "The Institute of Electrical and Electronics Engineers (IEEE) is a 501(c)(3) professional association for electronics engineering, electrical engineering, and other related disciplines.\nThe IEEE has a corporate office in New York City and an operations center in Piscataway, New Jersey. The IEEE was formed in 1963 as an amalgamation of the American Institute of Electrical Engineers and the Institute of Radio Engineers."
    },
    {
      "id": "20276016",
      "title": "Incremental computing",
      "url": "https://en.wikipedia.org/wiki/Incremental_computing",
      "summary": "Incremental computing, also known as incremental computation, is a software feature which, whenever a piece of data changes, attempts to save time by only recomputing those outputs which depend on the changed data. When incremental computing is successful, it can be significantly faster than computing new outputs naively. For example, a spreadsheet software package might use incremental computation in its recalculation feature, to update only those cells containing formulas which depend (directly or indirectly) on the changed cells.\nWhen incremental computing is implemented by a tool that can implement it for a variety of different pieces of code automatically, that tool is an example of a program analysis tool for optimization."
    },
    {
      "id": "58644759",
      "title": "Information engineering",
      "url": "https://en.wikipedia.org/wiki/Information_engineering",
      "summary": "Information engineering is the engineering discipline that deals with the generation, distribution, analysis, and use of information, data, and knowledge in systems. The field first became identifiable in the early 21st century.\n\nThe components of information engineering include more theoretical fields such as machine learning, artificial intelligence, control theory, signal processing, and information theory, and more applied fields such as computer vision, natural language processing, bioinformatics, medical image computing, cheminformatics, autonomous robotics, mobile robotics, and telecommunications. Many of these originate from computer science, as well as other branches of engineering such as computer engineering, electrical engineering, and bioengineering.\n\nThe field of information engineering is based heavily on mathematics, particularly probability, statistics, calculus, linear algebra, optimization, differential equations, variational calculus, and complex analysis.\nInformation engineers often hold a degree in information engineering or a related area, and are often part of a professional body such as the Institution of Engineering and Technology or Institute of Measurement and Control. They are employed in almost all industries due to the widespread use of information engineering.\n\n"
    },
    {
      "id": "36674345",
      "title": "Information technology",
      "url": "https://en.wikipedia.org/wiki/Information_technology",
      "summary": "Information technology (IT) is a set of related fields that encompass computer systems, software, programming languages and data and information processing and storage. IT forms part of information and communications technology (ICT). An information technology system (IT system) is generally an information system, a communications system, or, more specifically speaking, a computer system \u2014 including all hardware, software, and peripheral equipment \u2014 operated by a limited group of IT users, and an IT project usually refers to the commissioning and implementation of an IT system.Although humans have been storing, retrieving, manipulating, and communicating information since the earliest writing systems were developed, the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that \"the new technology does not yet have a single established name. We shall call it information technology (IT).\" Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several products or services within an economy are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, and e-commerce.Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC \u2014 1450 AD), mechanical (1450 \u2014 1840), electromechanical (1840 \u2014 1940), and electronic (1940 to present).Information technology is also a branch of computer science, which can be defined as the overall study of procedure, structure, and the processing of various types of data.  As this field continues to evolve across the world, its overall priority and importance has also grown, which is where we begin to see the introduction of computer science-related courses in K-12 education."
    },
    {
      "id": "146738",
      "title": "Interest",
      "url": "https://en.wikipedia.org/wiki/Interest",
      "summary": "In finance and economics, interest is payment from a borrower or deposit-taking financial institution to a lender or depositor of an amount above repayment of the principal sum (that is, the amount borrowed), at a particular rate. It is distinct from a fee which the borrower may pay to the lender or some third party. It is also distinct from dividend which is paid by a company to its shareholders (owners) from its profit or reserve, but not at a particular rate decided beforehand, rather on a pro rata basis as a share in the reward gained by risk taking entrepreneurs when the revenue earned exceeds the total costs.For example, a customer would usually pay interest to borrow from a bank, so they pay the bank an amount which is more than the amount they borrowed; or a customer may earn interest on their savings, and so they may withdraw more than they originally deposited. In the case of savings, the customer is the lender, and the bank plays the role of the borrower.\nInterest differs from profit, in that interest is received by a lender, whereas profit is received by the owner of an asset, investment or enterprise. (Interest may be part or the whole of the profit on an investment, but the two concepts are distinct from each other from an accounting perspective.)\nThe rate of interest is equal to the interest amount paid or received over a particular period divided by the principal sum borrowed or lent (usually expressed as a percentage).\nCompound interest means that interest is earned on prior interest in addition to the principal. Due to compounding, the total amount of debt grows exponentially, and its mathematical study led to the discovery of the number e. In practice, interest is most often calculated on a daily, monthly, or yearly basis, and its impact is influenced greatly by its compounding rate."
    },
    {
      "id": "1716320",
      "title": "James Martin (author)",
      "url": "https://en.wikipedia.org/wiki/James_Martin_(author)",
      "summary": "James Martin (19 October 1933 \u2013 24 June 2013) was an English information technology consultant and author, known for his work on information technology engineering.\n\n"
    },
    {
      "id": "59252",
      "title": "Marketing",
      "url": "https://en.wikipedia.org/wiki/Marketing",
      "summary": "Marketing is the act of satisfying and retaining customers. It is one of the primary components of business management and commerce.Marketing is typically conducted by the seller, typically a retailer or manufacturer. Products can be marketed to other businesses (B2B) or directly to consumers (B2C). Sometimes tasks are contracted to dedicated marketing firms, like a media, market research, or advertising agency. Sometimes, a trade association or government agency (such as the Agricultural Marketing Service) advertises on behalf of an entire industry or locality, often a specific type of food (e.g. Got Milk?), food from a specific area, or a city or region as a tourism destination.\nMarket orientations are philosophies concerning the factors that should go into market planning. The marketing mix, which outlines the specifics of the product and how it will be sold, including the channels that will be used to advertise the prodcut, is affected by the environment surrounding the product, the results of marketing research and market research, and the characteristics of the product's target market. Once these factors are determined, marketers must then decide what methods of promoting the product, including use of coupons and other price inducements."
    },
    {
      "id": "18831",
      "title": "Mathematics",
      "url": "https://en.wikipedia.org/wiki/Mathematics",
      "summary": "Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.\nMost mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or\u2014in modern mathematics\u2014entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and\u2014in case of abstraction from nature\u2014some basic properties that are considered true starting points of the theory under consideration.Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics was primarily divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.\n\n"
    },
    {
      "id": "18933632",
      "title": "Metadata",
      "url": "https://en.wikipedia.org/wiki/Metadata",
      "summary": "Metadata (or metainformation) is \"data that provides information about other data\", but not the content of the data itself, such as the text of a message or the image itself. There are many distinct types of metadata, including:\n\nDescriptive metadata \u2013 the descriptive information about a resource. It is used for discovery and identification. It includes elements such as title, abstract, author, and keywords.\nStructural metadata \u2013 metadata about containers of data and indicates how compound objects are put together, for example, how pages are ordered to form chapters. It describes the types, versions, relationships, and other characteristics of digital materials.\nAdministrative metadata \u2013 the information to help manage a resource, like resource type, permissions, and when and how it was created.\nReference metadata \u2013 the information about the contents and quality of statistical data.\nStatistical metadata \u2013 also called process data, may describe processes that collect, process, or produce statistical data.\nLegal metadata \u2013 provides information about the creator, copyright holder, and public licensing, if provided.Metadata is not strictly bound to one of these categories, as it can describe a piece of data in many other ways.\n\n"
    },
    {
      "id": "19001",
      "title": "Microsoft",
      "url": "https://en.wikipedia.org/wiki/Microsoft",
      "summary": "Microsoft Corporation is an American multinational technology corporation headquartered in Redmond, Washington, United States. Microsoft's best-known software products are the Windows line of operating systems, the Microsoft 365 suite of productivity applications, and the Edge web browser. Its flagship hardware products are the Xbox video game consoles and the Microsoft Surface lineup of touchscreen personal computers. Microsoft ranked No. 14 in the 2022 Fortune 500 rankings of the largest United States corporations by total revenue; it was the world's largest software maker by revenue as of 2022. It is considered one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Apple, and Meta (parent company of Facebook).\nMicrosoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell BASIC interpreters for the Altair 8800. It rose to dominate the personal computer operating system market with MS-DOS in the mid-1980s, followed by Windows. The company's 1986 initial public offering (IPO) and subsequent rise in its share price created three billionaires and an estimated 12,000 millionaires among Microsoft employees. Since the 1990s, it has increasingly diversified from the operating system market and has made several corporate acquisitions, the largest being the acquisition of Activision Blizzard for $68.7 billion in October 2023, followed by its acquisition of LinkedIn for $26.2 billion in December 2016, and its acquisition of Skype Technologies for $8.5 billion in May 2011.As of 2015, Microsoft is market-dominant in the IBM PC compatible operating system market and the office software suite market, although it has lost the majority of the overall operating system market to Android. The company also produces a wide range of other consumer and enterprise software for desktops, laptops, tabs, gadgets, and servers, including Internet search (with Bing), the digital services market (through MSN), mixed reality (HoloLens), cloud computing (Azure), and software development (Visual Studio).\nSteve Ballmer replaced Gates as CEO in 2000 and later envisioned a \"devices and services\" strategy. This unfolded with Microsoft acquiring Danger Inc. in 2008, entering the personal computer production market for the first time in June 2012 with the launch of the Microsoft Surface line of tablet computers, and later forming Microsoft Mobile through the acquisition of Nokia's devices and services division. Since Satya Nadella took over as CEO in 2014, the company has scaled back on hardware and instead focused on cloud computing, a move that helped the company's shares reach their highest value since December 1999. Under Nadella's direction, the company has also heavily expanded its gaming business to support the Xbox brand, establishing the Microsoft Gaming division in 2022, dedicated to operating Xbox in addition to its three subsidiaries (publishers). Microsoft Gaming is the third-largest gaming company in the world by revenue as of 2023.Earlier dethroned by Apple in 2010, in 2018, Microsoft reclaimed its position as the most valuable publicly traded company in the world. In April 2019, Microsoft reached a trillion-dollar market cap, becoming the third U.S. public company to be valued at over $1 trillion after Apple and Amazon, respectively. As of 2023, Microsoft has the third-highest global brand valuation.\nMicrosoft has been criticized for its monopolistic practices and the company's software has been criticized for problems with ease of use, robustness, and security."
    },
    {
      "id": "175537",
      "title": "Netflix",
      "url": "https://en.wikipedia.org/wiki/Netflix",
      "summary": "Netflix is an American subscription video on-demand over-the-top streaming service. The service primarily distributes original and acquired films and television shows from various genres, and it is available internationally in multiple languages.Launched on January 16, 2007, nearly a decade after Netflix, Inc. began its pioneering DVD\u2011by\u2011mail movie rental service, Netflix is the most-subscribed video on demand streaming media service, with 260.28 million paid memberships in more than 190 countries as of January 2024. By 2022, \"Netflix Original\" productions accounted for half of its library in the United States and the namesake company had ventured into other categories, such as video game publishing of mobile games via its flagship service. As of October 2023, Netflix is the 24th most-visited website in the world with 23.66% of its traffic coming from the United States, followed by the United Kingdom at 5.84% and Brazil at 5.64%."
    },
    {
      "id": "37256799",
      "title": "NewSQL",
      "url": "https://en.wikipedia.org/wiki/NewSQL",
      "summary": "NewSQL is a class of relational database management systems that seek to provide the scalability of NoSQL systems for online transaction processing (OLTP) workloads while maintaining the ACID guarantees of a traditional database system.Many enterprise systems that handle high-profile data (e.g., financial and order processing systems) are too large for conventional relational databases, but have transactional and consistency requirements that are not practical for NoSQL systems. The only options previously available for these organizations were to either purchase more powerful computers or to develop custom middleware that distributes requests over conventional DBMS. Both approaches feature high infrastructure costs and/or development costs. NewSQL systems attempt to reconcile the conflicts.\n\n"
    },
    {
      "id": "23968131",
      "title": "NoSQL",
      "url": "https://en.wikipedia.org/wiki/NoSQL",
      "summary": "NoSQL (originally referring to \"non-SQL\" or \"non-relational\") is an approach to database design that focuses on providing a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Instead of the typical tabular structure of a relational database, NoSQL databases house data within one data structure. Since this non-relational database design does not require a\u202fschema, it offers rapid\u202fscalability\u202fto manage\u202flarge and typically unstructured data sets.  NoSQL systems are also sometimes called \"Not only SQL\" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.Non-relational databases have existed since the late 1960s, but the name \"NoSQL\" was only coined in the early 2000s, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications.Motivations for this approach include simplicity of design, simpler \"horizontal\" scaling to clusters of machines (which is a problem for relational databases), finer control over availability, and limiting the object-relational impedance mismatch. The data structures used by NoSQL databases (e.g. key\u2013value pair, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as \"more flexible\" than relational database tables.Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance), lack of ability to perform ad hoc joins across tables, lack of standardized interfaces, and huge previous investments in existing relational databases. Most NoSQL stores lack true ACID transactions, although a few databases have made them central to their designs.\nInstead, most NoSQL databases offer a concept of \"eventual consistency\", in which database changes are propagated to all nodes \"eventually\" (typically within milliseconds), so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale read. Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss. Some NoSQL systems provide concepts such as write-ahead logging to avoid data loss. For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Relational databases \"do not allow referential integrity constraints to span databases\". Few systems maintain both ACID transactions and X/Open XA standards for distributed transaction processing. Interactive relational databases share conformational relay analysis techniques as a common feature. Limitations within the interface environment are overcome using semantic virtualization protocols, such that NoSQL services are accessible to most operating systems.\n\n"
    },
    {
      "id": "2063278",
      "title": "Object\u2013relational impedance mismatch",
      "url": "https://en.wikipedia.org/wiki/Object%E2%80%93relational_impedance_mismatch",
      "summary": "Object\u2013relational impedance mismatch creates difficulties going from data in relational data stores (relational database management system [\u201cRDBMS\u201d]) to usage in domain-driven object models. Object-orientation (OO) is the default method for business-centric design in programming languages. The problem lies in neither relational nor OO, but in the conceptual difficulty mapping between the two logic models. Both are logical models implementable differently on database servers, programming languages, design patterns, or other technologies. Issues range from application to enterprise scale, whenever stored relational data is used in domain-driven object models, and vice versa. Object-oriented data stores can trade this problem for other implementation difficulties.\nThe term impedance mismatch comes from impedance matching in electrical engineering .\n\n"
    },
    {
      "id": "40572678",
      "title": "Object storage",
      "url": "https://en.wikipedia.org/wiki/Object_storage",
      "summary": "Object storage (also known as object-based storage or blob storage) is a computer data storage approach that manages data as \"blobs\" or \"objects\", as opposed to other storage architectures like file systems which manages data as a file hierarchy, and block storage which manages data as blocks within sectors and tracks. Each object is typically associated with a variable amount of metadata, and a globally unique identifier. Object storage can be implemented at multiple levels, including the device level (object-storage device), the system level, and the interface level. In each case, object storage seeks to enable capabilities not addressed by other storage architectures, like interfaces that are directly programmable by the application, a namespace that can span multiple instances of physical hardware, and data-management functions like data replication and data distribution at object-level granularity.\nObject storage systems allow retention of massive amounts of unstructured data in which data is written once and read once (or many times). Object storage is used for purposes such as storing objects like videos and photos on Facebook, songs on Spotify, or files in online collaboration services, such as Dropbox. One of the limitations with object storage is that it is not intended for transactional data, as object storage was not designed to replace NAS file access and sharing; it does not support the locking and sharing mechanisms needed to maintain a single, accurately updated version of a file.\n\n"
    },
    {
      "id": "189239",
      "title": "Online analytical processing",
      "url": "https://en.wikipedia.org/wiki/Online_analytical_processing",
      "summary": "Online analytical processing, or OLAP (), is an approach to answer multi-dimensional analytical (MDA) queries swiftly in computing. OLAP is part of the broader category of business intelligence, which also encompasses relational databases, report writing and data mining.  Typical applications of OLAP include business reporting for sales, marketing, management reporting, business process management (BPM), budgeting and forecasting, financial reporting and similar areas, with new applications emerging, such as agriculture.The term OLAP was created as a slight modification of the traditional database term online transaction processing (OLTP).OLAP tools enable users to analyse multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.:\u200a402\u2013403\u200a Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region's sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP cube and view (dicing) the slices from different viewpoints.  These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson, or by date, or by customer, or by product, or by region, etc.).\nDatabases configured for OLAP use a multidimensional data model, allowing for complex analytical and ad hoc queries with a rapid execution time.  They borrow aspects of navigational databases, hierarchical databases and relational databases.\nOLAP is typically contrasted to OLTP (online transaction processing), which is generally characterized by much less complex queries, in a larger volume, to process transactions rather than for the purpose of business intelligence or reporting. Whereas OLAP systems are mostly optimized for read, OLTP has to process all kinds of queries (read, insert, update and delete)."
    },
    {
      "id": "2329992",
      "title": "Online transaction processing",
      "url": "https://en.wikipedia.org/wiki/Online_transaction_processing",
      "summary": "Online transaction processing (OLTP) is a type of database system used in transaction-oriented applications, such as many operational systems. \"Online\" refers to that such systems are expected to respond to user requests and process them in real-time (process transactions). The term is contrasted with online analytical processing (OLAP) which instead focuses on data analysis (for example planning and management systems).\n\n"
    },
    {
      "id": "23862",
      "title": "Python (programming language)",
      "url": "https://en.wikipedia.org/wiki/Python_(programming_language)",
      "summary": "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community."
    },
    {
      "id": "25873",
      "title": "Relational database",
      "url": "https://en.wikipedia.org/wiki/Relational_database",
      "summary": "A relational database is a (most commonly digital) database  based on the relational model of data, as proposed by E. F. Codd in 1970. A database management system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems are equipped with the option of using SQL (Structured Query Language) for querying and updating the database.\n\n"
    },
    {
      "id": "29414838",
      "title": "Rust (programming language)",
      "url": "https://en.wikipedia.org/wiki/Rust_(programming_language)",
      "summary": "Rust is a multi-paradigm, general-purpose programming language that emphasizes performance, type safety, and concurrency. It enforces memory safety\u2014meaning that all references point to valid memory\u2014without a garbage collector. To simultaneously enforce memory safety and prevent data races, its \"borrow checker\" tracks the object lifetime of all references in a program during compilation. Rust was influenced by ideas from functional programming, including immutability, higher-order functions, and algebraic data types. It is popular for systems programming.Software developer Graydon Hoare created Rust as a personal project while working at Mozilla Research in 2006. Mozilla officially sponsored the project in 2009. In the years following the first stable release in May 2015, Rust was adopted by companies including Amazon, Discord, Dropbox, Google (Alphabet), Meta, and Microsoft. In December 2022, it became the first language other than C and assembly to be supported in the development of the Linux kernel.\nRust has been noted for its rapid adoption, and has been studied in programming language theory research."
    },
    {
      "id": "48455863",
      "title": "Semantic Scholar",
      "url": "https://en.wikipedia.org/wiki/Semantic_Scholar",
      "summary": "Semantic Scholar is a research tool for scientific literature powered by artificial intelligence. It is developed at the Allen Institute for AI and was publicly released in November 2015. Semantic Scholar uses modern techniques in natural language processing to support the research process, for example by providing automatically generated summaries of scholarly papers. The Semantic Scholar team is actively researching the use of artificial intelligence in natural language processing, machine learning, human\u2013computer interaction, and information retrieval.Semantic Scholar began as a database for the topics of computer science, geoscience, and neuroscience. In 2017, the system began including biomedical literature in its corpus. As of September 2022, it includes over 200 million publications from all fields of science.\n\n"
    },
    {
      "id": "1393991",
      "title": "Social Science Research Network",
      "url": "https://en.wikipedia.org/wiki/Social_Science_Research_Network",
      "summary": "The Social Science Research Network (SSRN) is a repository for preprints devoted to the rapid dissemination of scholarly research in the social sciences, humanities, life sciences, and health sciences, among others. Elsevier bought SSRN from Social Science Electronic Publishing Inc. in May 2016. It is not an electronic journal, but rather an eLibrary and search engine."
    },
    {
      "id": "69894",
      "title": "Wales",
      "url": "https://en.wikipedia.org/wiki/Wales",
      "summary": "Wales (Welsh: Cymru [\u02c8k\u0259m.r\u0268] ) is a country that is part of the United Kingdom. It is bordered by the Irish Sea to the north and west, England to the east, the Bristol Channel to the south, and the Celtic Sea to the south-west. As of the 2021 census, it had a population of 3,107,494. It has a total area of 21,218 square kilometres (8,192 sq mi) and over 2,700 kilometres (1,680 mi) of coastline. It is largely mountainous with its higher peaks in the north and central areas, including Snowdon (Yr Wyddfa), its highest summit. The country lies within the north temperate zone and has a changeable, maritime climate. The capital and largest city is Cardiff.\nA distinct Welsh culture emerged among the Celtic Britons after the Roman withdrawal from Britain in the 5th century, and Wales was briefly united under Gruffydd ap Llywelyn in 1055. After over 200 years of war, the conquest of Wales by King Edward I of England was completed by 1283, though Owain Glynd\u0175r led the Welsh Revolt against English rule in the early 15th century, and briefly re-established an independent Welsh state with its own national parliament (Welsh: senedd). In the 16th century the whole of Wales was annexed by England and incorporated within the English legal system under the Laws in Wales Acts 1535 and 1542. Distinctive Welsh politics developed in the 19th century. Welsh Liberalism, exemplified in the early 20th century by David Lloyd George, was displaced by the growth of socialism and the Labour Party. Welsh national feeling grew over the century: a nationalist party, Plaid Cymru, was formed in 1925, and the Welsh Language Society in 1962. A governing system of Welsh devolution is employed in Wales, of which the most major step was the formation of the Senedd (Welsh Parliament, formerly the National Assembly for Wales) in 1998, responsible for a range of devolved policy matters.\nAt the dawn of the Industrial Revolution, development of the mining and metallurgical industries transformed the country from an agricultural society into an industrial one; the South Wales Coalfield's exploitation caused a rapid expansion of Wales's population. Two-thirds of the population live in South Wales, including Cardiff, Swansea, Newport and the nearby valleys. The eastern region of North Wales has about a sixth of the overall population, with Wrexham being the largest northern city. The remaining parts of Wales are sparsely populated. Now that the country's traditional extractive and heavy industries have gone or are in decline, the economy is based on the public sector, light and service industries, and tourism. Agriculture in Wales is largely livestock based, making Wales a net exporter of animal produce, contributing towards national agricultural self-sufficiency.\nThe country has a distinct national and cultural identity and from the late 19th century onwards Wales acquired its popular image as the \"land of song\", in part due to the eisteddfod tradition and rousing choir singing. Both Welsh and English are official languages. A majority of the population in most areas speaks English whilst the majority of the population in parts of the north and west speak Welsh, with a total of 538,300 Welsh speakers across the entire country."
    },
    {
      "id": "3254510",
      "title": "Scala (programming language)",
      "url": "https://en.wikipedia.org/wiki/Scala_(programming_language)",
      "summary": "Scala ( SKAH-lah) is a strong statically typed high-level general-purpose programming language that supports both object-oriented programming and functional programming. Designed to be concise, many of Scala's design decisions are intended to address criticisms of Java.Scala source code can be compiled to Java bytecode and run on a Java virtual machine (JVM). Scala can also be compiled to JavaScript to run in a browser, or directly to a native executable. On the JVM Scala provides language interoperability with Java so that libraries written in either language may be referenced directly in Scala or Java code. Like Java, Scala is object-oriented, and uses a syntax termed curly-brace which is similar to the language C. Since Scala 3, there is also an option to use the off-side rule (indenting) to structure blocks, and its use is advised. Martin Odersky has said that this turned out to be the most productive change introduced in Scala 3.Unlike Java, Scala has many features of functional programming languages (like Scheme, Standard ML, and Haskell), including currying, immutability, lazy evaluation, and pattern matching. It also has an advanced type system supporting algebraic data types, covariance and contravariance, higher-order types (but not higher-rank types), anonymous types, operator overloading, optional parameters, named parameters, raw strings, and an experimental exception-only version of algebraic effects that can be seen as a more powerful version of Java's checked exceptions.The name Scala is a portmanteau of scalable and language, signifying that it is designed to grow with the demands of its users."
    },
    {
      "id": "22135297",
      "title": "Semi-structured data",
      "url": "https://en.wikipedia.org/wiki/Semi-structured_data",
      "summary": "Semi-structured data is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Therefore, it is also known as self-describing structure.\nIn semi-structured data, the entities belonging to the same class may have different attributes even though they are grouped together, and the attributes' order is not important.\nSemi-structured data are increasingly occurring since the advent of the Internet where full-text documents and databases are not the only forms of data anymore, and different applications need a medium for exchanging information. In object-oriented databases, one often finds semi-structured data.\n\n"
    },
    {
      "id": "5309",
      "title": "Software",
      "url": "https://en.wikipedia.org/wiki/Software",
      "summary": "Software is a collection of programs and data that tell a computer how to perform specific tasks. Software often includes associated software documentation. This is in contrast to hardware, from which the system is built and which actually performs the work.\nAt the lowest programming level, executable code consists of machine language instructions supported by an individual processor\u2014typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer\u2014an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction or is interrupted by the operating system. As of 2024, most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.\nThe majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler, an interpreter, or a combination of the two. Software may also be written in a low-level assembly language that has a strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler."
    },
    {
      "id": "27010",
      "title": "Software engineering",
      "url": "https://en.wikipedia.org/wiki/Software_engineering",
      "summary": "Software engineering is an engineering-based approach to software development.\nA software engineer is a person who applies the engineering design process to design, develop, test, maintain, and evaluate computer software. The term programmer is sometimes used as a synonym, but may emphasize software implementation over design and can also lack connotations of engineering education or skills.Engineering techniques are used to inform the software development process, which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management, which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.\n\n"
    },
    {
      "id": "7366298",
      "title": "Solid-state drive",
      "url": "https://en.wikipedia.org/wiki/Solid-state_drive",
      "summary": "A solid-state drive (SSD) is a solid-state storage device that uses integrated circuit assemblies to store data persistently, typically using flash memory, and functions as secondary storage in the hierarchy of computer storage. It is also sometimes called a semiconductor storage device, a solid-state device, or a solid-state disk, even though SSDs lack the physical spinning disks and movable read-write heads used in hard disk drives (HDDs) and floppy disks. SSD also has rich internal parallelism for data processing.In comparison to hard disk drives and similar electromechanical media which use moving parts, SSDs are typically more resistant to physical shock, run silently, and have higher input/output rates and lower latency. SSDs store data in semiconductor cells. As of 2019, cells can contain between 1 and 4 bits of data. SSD storage devices vary in their properties according to the number of bits stored in each cell, with single-bit cells (\"Single Level Cells\" or \"SLC\") being generally the most reliable, durable, fast, and expensive type, compared with 2- and 3-bit cells (\"Multi-Level Cells/MLC\" and \"Triple-Level Cells/TLC\"), and finally, quad-bit cells (\"QLC\") being used for consumer devices that do not require such extreme properties and are the cheapest per gigabyte (GB) of the four. In addition, 3D XPoint memory (sold by Intel under the Optane brand) stores data by changing the electrical resistance of cells instead of storing electrical charges in cells, and SSDs made from RAM can be used for high speed, when data persistence after power loss is not required, or may use battery power to retain data when its usual power source is unavailable. Hybrid drives or solid-state hybrid drives (SSHDs), such as Intel's Hystor and Apple's Fusion Drive, combine features of SSDs and HDDs in the same unit using both flash memory and spinning magnetic disks in order to improve the performance of frequently-accessed data. Bcache achieves a similar effect purely in software, using combinations of dedicated regular SSDs and HDDs.\nSSDs based on NAND flash will slowly leak charge over time if left for long periods without power. This causes worn-out drives (that have exceeded their endurance rating) to start losing data typically after one year (if stored at 30 \u00b0C) to two years (at 25 \u00b0C) in storage; for new drives it takes longer. Therefore, SSDs are not suitable for archival storage. 3D XPoint is a possible exception to this rule; it is a relatively new technology with unknown long-term data-retention characteristics.\nSSDs can use traditional HDD interfaces and form factors, or newer interfaces and form factors that exploit specific advantages of the flash memory in SSDs. Traditional interfaces (e.g. SATA and SAS) and standard HDD form factors allow such SSDs to be used as drop-in replacements for HDDs in computers and other devices. Newer form factors such as mSATA, M.2, U.2, NF1/M.3/NGSFF, XFM Express (Crossover Flash Memory, form factor XT2) and EDSFF (formerly known as Ruler SSD) and higher speed interfaces such as NVM Express (NVMe) over PCI Express (PCIe) can further increase performance over HDD performance. SSDs have a limited lifetime number of writes, and also slow down as they reach their full storage capacity."
    },
    {
      "id": "26685",
      "title": "Statistics",
      "url": "https://en.wikipedia.org/wiki/Statistics",
      "summary": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when an it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems."
    },
    {
      "id": "8286675",
      "title": "System",
      "url": "https://en.wikipedia.org/wiki/System",
      "summary": "A system is a group of interacting or interrelated elements that act according to a set of rules to form a unified whole. A system, surrounded and influenced by its environment, is described by its boundaries, structure /system/\nand purpose and is expressed in its functioning. Systems are the subjects of study of systems theory and other systems sciences.\nSystems have several common properties and characteristics, including structure, function(s), behavior and interconnectivity."
    },
    {
      "id": "12098689",
      "title": "Systems analyst",
      "url": "https://en.wikipedia.org/wiki/Systems_analyst",
      "summary": "A systems analyst, also known as business technology analyst, is an information technology (IT) professional who specializes in analyzing, designing and implementing information systems. Systems analysts assess the suitability of information systems in terms of their intended outcomes and liaise with end users, software vendors and programmers in order to achieve these outcomes. A systems analyst is a person who uses analysis and design techniques to solve business problems using information technology. Systems analysts may serve as change agents who identify the organizational improvements needed, design systems to implement those changes, and train and motivate others to use the systems.\n\n"
    },
    {
      "id": "21101351",
      "title": "T. William Olle",
      "url": "https://en.wikipedia.org/wiki/T._William_Olle",
      "summary": "T. William (Bill) Olle (born 1933 and died March 2019) was a British computer scientist and consultant and President of T. William Olle Associates, England.\n\n"
    },
    {
      "id": "651800",
      "title": "Terry Halpin",
      "url": "https://en.wikipedia.org/wiki/Terry_Halpin",
      "summary": "Terence Aidan (Terry) Halpin (born 1950s) is an Australian computer scientist who is known for his formalization of the Object Role Modeling notation."
    },
    {
      "id": "45554839",
      "title": "Tony Morgan (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Tony_Morgan_(computer_scientist)",
      "summary": "Antony J. (Tony) Morgan (born c. 1944) is a British computer scientist, data modeling consultant, and Professor in computer science at INTI International University. He is known for his work on (2002) \"Business rules and information systems,\" and the 2010 \"Information modeling and relational databases,\" co-authored with Terry Halpin."
    },
    {
      "id": "23538754",
      "title": "Wayback Machine",
      "url": "https://en.wikipedia.org/wiki/Wayback_Machine",
      "summary": "The Wayback Machine is a digital archive of the World Wide Web founded by the Internet Archive, an American nonprofit organization based in San Francisco, California. Created in 1996 and launched to the public in 2001, it allows the user to go \"back in time\" to see how websites looked in the past. Its founders, Brewster Kahle and Bruce Gilliat, developed the Wayback Machine to provide \"universal access to all knowledge\" by preserving archived copies of defunct web pages.Launched on May 10, 1996, the Wayback Machine had saved more than 38.2 billion web pages at the end of 2009. As of January 3, 2024, the Wayback Machine has archived more than 860 billion web pages and well over 99 petabytes of data."
    },
    {
      "id": "12506378",
      "title": "Workflow management system",
      "url": "https://en.wikipedia.org/wiki/Workflow_management_system",
      "summary": "A workflow management system (WfMS or WFMS) provides an infrastructure for the set-up, performance, and monitoring of a defined sequence of tasks arranged as a workflow application."
    },
    {
      "id": "14924067",
      "title": "Academic discipline",
      "url": "https://en.wikipedia.org/wiki/Academic_discipline",
      "summary": "An academic discipline or academic field is a subdivision of knowledge that is taught and researched at the college or university level. Disciplines are defined (in part) and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties within colleges and universities to which their practitioners belong. Academic disciplines are conventionally divided into the humanities, including language, art and cultural studies, and the scientific disciplines, such as physics, chemistry, and biology; the social sciences are sometimes considered a third category.\nIndividuals associated with academic disciplines are commonly referred to as experts or specialists. Others, who may have studied liberal arts or systems theory rather than concentrating in a specific academic discipline, are classified as generalists.\nWhile academic disciplines in and of themselves are more or less focused practices, scholarly approaches such as multidisciplinarity/interdisciplinarity, transdisciplinarity, and cross-disciplinarity integrate aspects from multiple academic disciplines, therefore addressing any problems that may arise from narrow concentration within specialized fields of study. For example, professionals may encounter trouble communicating across academic disciplines because of differences in language, specified concepts, or methodology.\nSome researchers believe that academic disciplines may, in the future, be replaced by what is known as Mode 2 or \"post-academic science\", which involves the acquisition of cross-disciplinary knowledge through the collaboration of specialists from various academic disciplines.\nIt is also known as a field of study, field of inquiry, research field and branch of knowledge. The different terms are used in different countries and fields.\n\n"
    },
    {
      "id": "341988",
      "title": "American Statistical Association",
      "url": "https://en.wikipedia.org/wiki/American_Statistical_Association",
      "summary": "The American Statistical Association (ASA) is the main professional organization for statisticians and related professionals in the United States. It was founded in Boston, Massachusetts on November 27, 1839, and is the second-oldest continuously operating professional society in the U.S. behind the Massachusetts Medical Society, founded in 1781). ASA services statisticians, quantitative scientists, and users of statistics across many academic areas and applications. The association publishes a variety of journals and sponsors several international conferences every year.\n\n"
    },
    {
      "id": "1134",
      "title": "Analysis",
      "url": "https://en.wikipedia.org/wiki/Analysis",
      "summary": "Analysis (pl.: analyses) is the process of breaking a complex topic or substance into smaller parts in order to gain a better understanding of it. The technique has been applied in the study of mathematics and logic since before Aristotle (384\u2013322 B.C.), though analysis as a formal concept is a relatively recent development.The word comes from the Ancient Greek \u1f00\u03bd\u03ac\u03bb\u03c5\u03c3\u03b9\u03c2 (analysis, \"a breaking-up\" or \"an untying;\" from ana- \"up, throughout\" and lysis \"a loosening\"). From it also comes the word's plural, analyses.\nAs a formal concept, the method has variously been ascribed to Alhazen, Ren\u00e9 Descartes (Discourse on the Method), and Galileo Galilei. It has also been ascribed to Isaac Newton, in the form of a practical method of physical discovery (which he did not name).\nThe converse of analysis is synthesis: putting the pieces back together again in a new or different whole."
    },
    {
      "id": "25745941",
      "title": "Andrew Gelman",
      "url": "https://en.wikipedia.org/wiki/Andrew_Gelman",
      "summary": "Andrew Eric Gelman (born February 11, 1965) is an American statistician and professor of statistics and political science at Columbia University.\nGelman received bachelor of science degrees in mathematics and in physics from MIT, where he was a National Merit Scholar, in 1986. He then received a master of science in 1987 and a doctor of philosophy in 1990, both in statistics from Harvard University, under the supervision of Donald Rubin."
    },
    {
      "id": "38751",
      "title": "ArXiv",
      "url": "https://en.wikipedia.org/wiki/ArXiv",
      "summary": "arXiv (pronounced as \"archive\"\u2014the X represents the Greek letter chi \u27e8\u03c7\u27e9) is an open-access repository of electronic preprints and postprints (known as e-prints) approved for posting after moderation, but not peer review. It consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, mathematical finance and economics, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository before publication in a peer-reviewed journal. Some publishers also grant permission for authors to archive the peer-reviewed postprint. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, had hit a million by the end of 2014 and two million by the end of 2021. As of April 2021, the submission rate is about 16,000 articles per month."
    },
    {
      "id": "1673865",
      "title": "Astronomical survey",
      "url": "https://en.wikipedia.org/wiki/Astronomical_survey",
      "summary": "An astronomical survey is a general map or image of a region of the sky (or of the whole sky) that lacks a specific observational target.  Alternatively, an astronomical survey may comprise a set of images, spectra, or other observations of objects that share a common type or feature. Surveys are often restricted to one band of the electromagnetic spectrum due to instrumental limitations, although multiwavelength surveys can be made by using multiple detectors, each sensitive to a different bandwidth.Surveys have generally been performed as part of the production of an astronomical catalog. They may also search for transient astronomical events. They often use wide-field astrographs.\n\n"
    },
    {
      "id": "30863191",
      "title": "Basic research",
      "url": "https://en.wikipedia.org/wiki/Basic_research",
      "summary": "Basic research, also called pure research, fundamental research, basic science, or pure science, is a type of scientific research with the aim of improving scientific theories for better understanding and prediction of natural or other phenomena.  In contrast, applied research uses scientific theories to develop technology or techniques which can be used to intervene and alter natural or other phenomena. Though often driven simply by curiosity, basic research often fuels the technological innovations of applied science.  The two aims are often practiced simultaneously in coordinated research and development.\nIn addition to innovations, basic research also serves to provide insight into nature around us and allows us to respect its innate value. The development of this respect is what drives conservation efforts. Through learning about the environment, conservation efforts can be strengthened using research as a basis. Technological innovations can unintentionally be created through this as well, as seen with examples such as kingfishers' beaks affecting the design for high speed bullet train in Japan.\n\n"
    },
    {
      "id": "20877649",
      "title": "Ben Fry",
      "url": "https://en.wikipedia.org/wiki/Ben_Fry",
      "summary": "Benjamin Fry is an American designer who has expertise in data visualization.\n\n"
    },
    {
      "id": "24437894",
      "title": "Boston",
      "url": "https://en.wikipedia.org/wiki/Boston",
      "summary": "Boston (US: ), officially the City of Boston, is the capital and most populous city in Massachusetts, a state in the United States. The city serves as the cultural and financial center of the New England region of the Northeastern United States. It has an area of 48.4 sq mi (125 km2) and a population of 675,647 as of 2020. The Greater Boston metropolitan statistical area, surrounding the city, is the eleventh-largest in the country.Boston is one of the United States' oldest cities. It was founded on the Shawmut Peninsula in 1630 by Puritan settlers. The city was named after Boston, Lincolnshire, England. During the American Revolution, Boston was home to several key events. These included the Boston Massacre, the Boston Tea Party, the hanging of Paul Revere's lantern signal in Old North Church, the Battle of Bunker Hill, and the siege of Boston. Following American independence from Great Britain, the city continued to play an important role as a port, manufacturing hub, and center for education and culture.The city expanded significantly beyond the original peninsula through filling in land and annexing neighboring towns. It now attracts many tourists, with Faneuil Hall alone drawing more than 20 million visitors per year. Boston's many firsts include the United States' first public park (Boston Common, 1634), the first public school (Boston Latin School, 1635), and the first subway system (Tremont Street subway, 1897).In the 21st century, Boston emerged as a global leader in higher education and academic research. Some of the city's colleges and universities include Boston University and Northeastern University in the city proper. Furthermore, Greater Boston's many colleges and universities include globally-ranked Harvard and MIT in neighboring Cambridge.Boston has become the largest biotechnology hub in the world. The city is also a national leader in scientific research, law, medicine, engineering, and business. With nearly 5,000 startup companies, the city is considered a global pioneer in innovation and entrepreneurship, and more recently in artificial intelligence. Boston's economy also includes finance, professional and business services, information technology, and government activities. Households in the city claim the highest average rate of philanthropy in the United States. Furthermore, Boston's businesses and institutions rank among the top in the country overall for environmental sustainability and new investment."
    },
    {
      "id": "30068",
      "title": "The Boston Globe",
      "url": "https://en.wikipedia.org/wiki/The_Boston_Globe",
      "summary": "The Boston Globe is an American daily newspaper founded and based in Boston, Massachusetts. The newspaper has won a total of 27 Pulitzer Prizes.Its reported daily circulation had fallen to under 69,000 copies per day as of June 2022. It reported 300,000 print and digital subscribers in 2017. The Boston Globe is the oldest and largest daily newspaper in Boston.Founded in 1872, the paper was mainly controlled by Irish Catholic interests before being sold to Charles H. Taylor and his family. After being privately held until 1973, it was sold to The New York Times in 1993 for $1.1 billion, making it one of the most expensive print purchases in U.S. history. The newspaper was purchased in 2013 by Boston Red Sox and Liverpool F.C. owner John W. Henry for $70 million from The New York Times Company, having lost over 90% of its value in 20 years.\nThe newspaper has been noted as \"one of the nation's most prestigious papers.\" In 1967, The Boston Globe became the first major paper in the U.S. to come out against the Vietnam War. The paper's 2002 coverage of the Roman Catholic Church sex abuse scandal received international media attention and served as the basis for the 2015 American drama film Spotlight.The editor of The Boston Globe is Nancy Barnes, who took the helm in February 2023.The chief print rival of The Boston Globe is the Boston Herald, which has a smaller circulation that is declining more rapidly."
    },
    {
      "id": "39206",
      "title": "Business",
      "url": "https://en.wikipedia.org/wiki/Business",
      "summary": "Business is the practice of making one's living or making money by producing or buying and selling products (such as goods and services). It is also \"any activity or enterprise entered into for profit.\"A business entity is not necessarily separate from the owner and the creditors can hold the owner liable for debts the business has acquired. The taxation system for businesses is different from that of the corporates. A business structure does not allow for corporate tax rates. The proprietor is personally taxed on all income from the business.\nThe term is also often used colloquially (but not by lawyers or public officials) to refer to a company, such as a corporation or cooperative.\nCorporations, in contrast with sole proprietors and partnerships, are separate legal entities and provide limited liability for their owners/members, as well as being subject to corporate tax rates. A corporation is more complicated and expensive to set up, but offers more protection and benefits for the owners/members."
    },
    {
      "id": "158442",
      "title": "Buzzword",
      "url": "https://en.wikipedia.org/wiki/Buzzword",
      "summary": "A buzzword is a word or phrase, new or already existing, that becomes popular for a period of time. Buzzwords often derive from technical terms yet often have much of the original technical meaning removed through fashionable use, being simply used to impress others. Some buzzwords retain their true technical meaning when used in the correct contexts, for example artificial intelligence.\nBuzzwords often originate in jargon, acronyms, or neologisms. Examples of overworked business buzzwords include synergy, vertical, dynamic, cyber and strategy. \nIt has been stated that businesses could not operate without buzzwords, as they are the shorthands or internal shortcuts that make perfect sense to people informed of the context. However, a useful buzzword can become co-opted into general popular speech and lose its usefulness. According to management professor Robert Kreitner, \"Buzzwords are the literary equivalent of Gresham's law. They will drive out good ideas.\"\nBuzzwords, or buzz-phrases such as \"all on the same page\", can also be seen in business as a way to make people feel like there is a mutual understanding. As most workplaces use a specialized jargon, which could be argued is another form of buzzwords, it allows quicker communication. Indeed, many new hires feel more like \"part of the team\" the quicker they learn the buzzwords of their new workplace. Buzzwords permeate people's working lives so much that many don't realize that they are using them. The vice president of CSC Index, Rich DeVane, notes that buzzwords describe not only a trend, but also what can be considered a \"ticket of entry\" with regards to being considered as a successful organization \u2013 \"What people find tiresome is each consulting firm's attempt to put a different spin on it. That's what gives bad information.\"Buzzwords also feature prominently in politics, where they can result in a process which \"privileges rhetoric over reality, producing policies that are 'operationalized' first and only 'conceptualized' at a later date\". The resulting political speech is known for \"eschewing reasoned debate (as characterized by the use of evidence and structured argument), instead employing language exclusively for the purposes of control and manipulation\"."
    },
    {
      "id": "26880450",
      "title": "C. F. Jeff Wu",
      "url": "https://en.wikipedia.org/wiki/C._F._Jeff_Wu",
      "summary": "Chien-Fu Jeff Wu (born 1949) is the Coca-Cola Chair in Engineering Statistics and Professor in the H. Milton Stewart School of Industrial and Systems Engineering at the Georgia Institute of Technology.  He is known for his work on the convergence of the EM algorithm, resampling methods such as the bootstrap and jackknife, and industrial statistics, including design of experiments, and robust parameter design (Taguchi methods).\nBorn in Taiwan, Wu earned a B.Sc. in Mathematics from National Taiwan University in 1971, and a Ph.D. in Statistics from University of California, Berkeley in 1976.  He has been a faculty member at the University of Wisconsin, Madison (1977\u20131988), the University of Waterloo (1988\u20131993; GM-NSERC chair in quality and productivity), the University of Michigan (1995\u20132003; chair of Department of Statistics 1995\u201398; H.C. Carver professor of statistics, 1997\u20132003) and currently the Georgia Institute of Technology.  He has supervised 50 Ph.D. students and published around 185 peer-reviewed articles and two books. He has received several awards, including the COPSS Presidents' Award in 1987,\nthe Shewhart Medal in 2008,\nthe COPSS R. A. Fisher Lectureship in 2011,\nand the Deming Lecturer Award in 2012.  He gave the inaugural Akaike Memorial Lecture in 2016. He has been elected as a fellow of the American Statistical Association, the Institute of Mathematical Statistics, the American Society for Quality and the Institute for Operations Research and the Management Sciences.  In 2000 he was elected as a member of Academia Sinica. In 2004, he was elected as a member of the National Academy of Engineering.  He received the Shewhart Medal of the American Society for Quality and an honorary degree from the University of Waterloo in 2008. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, he used the term Data Science for the first time as an alternative name for statistics.  Later, in November 1997, he gave the inaugural lecture entitled \"Statistics = Data Science?\" for his appointment to the H. C. Carver Professorship at the University of Michigan.\nHe popularized the term \"data science\" and advocated that statistics be renamed data science and statisticians data scientists.\nHe also presented his lecture entitled \"Statistics = Data Science?\" as the first of his 1998 P.C. Mahalanobis Memorial Lectures. These lectures honor Prasanta Chandra Mahalanobis, an Indian scientist and statistician and founder of the Indian Statistical Institute.\nIn Mile, Yunnan, China, a conference was held in July 2014 celebrating Professor Wu's 65th birthday. In 2014 he gave the Bradley Lecture at the University of Georgia. In 2016 he was the inaugural recipient of the Akaike Memorial Lecture Award.  In 2017 Jeff Wu received the George Box Medal from ENBIS. In 2020, Jeff Wu received Georgia Institute of Technology\u2019s highest award given to a faculty member: the Class of 1934 Distinguished Professor Award.  In the same year, he also got the Sigma Xi\u2019s Monie A. Ferst Award which, since 1977, has honored science and engineering teachers who have inspired their students to significant research achievements. In 2020 he delivered the CANSSI/Fields Distinguished Lectures Series in Statistical Sciences."
    },
    {
      "id": "6310",
      "title": "Columbia University",
      "url": "https://en.wikipedia.org/wiki/Columbia_University",
      "summary": "Columbia University, officially Columbia University in the City of New York, is a private Ivy League research university in New York City. Established in 1754 as King's College on the grounds of Trinity Church in Manhattan, it is the oldest institution of higher education in New York and the fifth-oldest in the United States.\nColumbia was established as a colonial college by royal charter under George II of Great Britain. It was renamed Columbia College in 1784 following the American Revolution, and in 1787 was placed under a private board of trustees headed by former students Alexander Hamilton and John Jay. In 1896, the campus was moved to its current location in Morningside Heights and renamed Columbia University.\nColumbia is organized into twenty schools, including four undergraduate schools and 16 graduate schools. The university's research efforts include the Lamont\u2013Doherty Earth Observatory, the Goddard Institute for Space Studies, and accelerator laboratories with Big Tech firms such as Amazon and IBM. Columbia is a founding member of the Association of American Universities and was the first school in the United States to grant the MD degree. The university also administers and annually awards the Pulitzer Prize.\nColumbia scientists and scholars have played a pivotal role in scientific breakthroughs including brain-computer interface; the laser and maser; nuclear magnetic resonance; the first nuclear pile; the first nuclear fission reaction in the Americas; the first evidence for plate tectonics and continental drift; and much of the initial research and planning for the Manhattan Project during World War II.\nAs of December 2021, its alumni, faculty, and staff have included five of the Seven Founding Fathers of the United States of America; four U.S. presidents; 34 foreign heads of state; two secretaries-general of the United Nations; ten justices of the United States Supreme Court; 103 Nobel laureates; 125 National Academy of Sciences members; 53 living billionaires; 23 Olympic medalists; 33 Academy Award winners; and 125 Pulitzer Prize  recipients."
    },
    {
      "id": "63552467",
      "title": "Comet NEOWISE",
      "url": "https://en.wikipedia.org/wiki/Comet_NEOWISE",
      "summary": "C/2020 F3 (NEOWISE) or Comet NEOWISE is a long period comet with a near-parabolic orbit discovered on March 27, 2020, by astronomers during the NEOWISE mission of the Wide-field Infrared Survey Explorer (WISE) space telescope. At that time, it was an 18th-magnitude object, located 2 AU (300 million km; 190 million mi) away from the Sun and 1.7 AU (250 million km; 160 million mi) away from Earth.NEOWISE is known for being the brightest comet in the northern hemisphere since Comet Hale\u2013Bopp in 1997. It was widely photographed by professional and amateur observers and was even spotted by people living near city centers and areas with light pollution. While it was too close to the Sun to be observed at perihelion, it emerged from perihelion around magnitude 0.5 to 1, making it bright enough to be visible to the naked eye. Under dark skies, it could be seen with the naked eye and remained visible to the naked eye throughout July 2020. By July 30, the comet was about magnitude 5, when binoculars were required near urban areas to locate the comet. \nFor observers in the Northern Hemisphere, the comet could be seen on the northwestern horizon, below the Big Dipper. North of 45 degrees north, the comet was visible all night in mid-July 2020. On July 30, Comet NEOWISE entered the constellation of Coma Berenices, below the bright star Arcturus."
    },
    {
      "id": "7671",
      "title": "Committee on Data of the International Science Council",
      "url": "https://en.wikipedia.org/wiki/Committee_on_Data_of_the_International_Science_Council",
      "summary": "The Committee on Data of the International Science Council (CODATA) was established in 1966 as the Committee on Data for Science and Technology, originally part of the International Council of Scientific Unions, now part of the International Science Council (ISC). Since November 2023 its president is the Catalan researcher Merc\u00e8 Crosas.CODATA exists to promote global collaboration to advance open science and to improve the availability and usability of data for all areas of research. CODATA supports the principle that data produced by research and susceptible to being used for research should be as open as possible and as closed as necessary. CODATA works also to advance the interoperability and the usability of such data; research data should be FAIR (findable, accessible, interoperable and reusable). By promoting the policy, technological, and cultural changes that are essential to promote open science, CODATA helps advance ISC's vision and mission of advancing science as a global public good.\nThe CODATA Strategic Plan 2015 and Prospectus of Strategy and Achievement 2016 identify three priority areas:\n\npromoting principles, policies and practices for open data and open science;\nadvancing the frontiers of data science;\nbuilding capacity for open science by improving data skills and the functions of national science systems needed to support open data.CODATA achieves these objectives through a number of standing committees and strategic executive led initiatives, and through its task groups and working groups. CODATA also works closely with member unions and associations of ISC to promote the efforts on open data and open science."
    },
    {
      "id": "5177",
      "title": "Communication",
      "url": "https://en.wikipedia.org/wiki/Communication",
      "summary": "Communication is commonly defined as the transmission of information. Its precise definition is disputed and there are disagreements about whether unintentional or failed transmissions are included and whether communication not only transmits meaning but also creates it. Models of communication are simplified overviews of its main components and their interactions. Many models include the idea that a source uses a coding system to express information in the form of a message. The message is sent through a channel to a receiver who has to decode it to understand it. The main field of inquiry investigating communication is called communication studies.\nA common way to classify communication is by whether information is exchanged between humans, members of other species, or non-living entities such as computers. For human communication, a central contrast is between verbal and non-verbal communication. Verbal communication involves the exchange of messages in linguistic form, including spoken and written messages as well as sign language. Non-verbal communication happens without the use of a linguistic system, for example, using body language, touch, and facial expressions. Another distinction is between interpersonal communication, which happens between distinct persons, and intrapersonal communication, which is communication with oneself. Communicative competence is the ability to communicate well and applies to the skills of formulating messages and understanding them.\nNon-human forms of communication include animal and plant communication. Researchers in this field often refine their definition of communicative behavior by including the criteria that observable responses are present and that the participants benefit from the exchange. Animal communication is used in areas like courtship and mating, parent\u2013offspring relations, navigation, and self-defense. Communication through chemicals is particularly important for the relatively immobile plants. For example, maple trees release so-called volatile organic compounds into the air to warn other plants of a herbivore attack. Most communication takes place between members of the same species. The reason is that its purpose is usually some form of cooperation, which is not as common between different species. Interspecies communication happens mainly in cases of symbiotic relationships. For instance, many flowers use symmetrical shapes and distinctive colors to signal to insects where nectar is located. Humans engage in interspecies communication when interacting with pets and working animals.\nHuman communication has a long history and how people exchange information has changed over time. These changes were usually triggered by the development of new communication technologies. Examples are the invention of writing systems, the development of mass printing, the use of radio and television, and the invention of the internet. The technological advances also led to new forms of communication, such as the exchange of data between computers."
    },
    {
      "id": "37438",
      "title": "Complex system",
      "url": "https://en.wikipedia.org/wiki/Complex_system",
      "summary": "A complex system is a system composed of many components which may interact with each other. Examples of complex systems are Earth's global climate, organisms, the human brain, infrastructure such as power grid, transportation or communication systems, complex software and electronic systems, social and economic organizations (like cities), an ecosystem, a living cell, and ultimately the entire universe.Complex systems are systems whose behavior is intrinsically difficult to model due to the dependencies, competitions, relationships, or other types of interactions between their parts or between a given system and its environment. Systems that are \"complex\" have distinct properties that arise from these relationships, such as nonlinearity, emergence, spontaneous order, adaptation, and feedback loops, among others. Because such systems appear in a wide variety of fields, the commonalities among them have become the topic of their independent area of research. In many cases, it is useful to represent such a system as a network where the nodes represent the components and links to their interactions.\nThe term complex systems often refers to the study of complex systems, which is an approach to science that investigates how relationships between a system's parts give rise to its collective behaviors and how the system interacts and forms relationships with its environment. The study of complex systems regards collective, or system-wide, behaviors as the fundamental object of study; for this reason, complex systems can be understood as an alternative paradigm to reductionism, which attempts to explain systems in terms of their constituent parts and the individual interactions between them.\nAs an interdisciplinary domain, complex systems draws contributions from many different fields, such as the study of self-organization and critical phenomena from physics, that of spontaneous order from the social sciences, chaos from mathematics, adaptation from biology, and many others. Complex systems is therefore often used as a broad term encompassing a research approach to problems in many diverse disciplines, including statistical physics, information theory, nonlinear dynamics, anthropology, computer science, meteorology, sociology, economics, psychology, and biology.\n\n"
    },
    {
      "id": "1181008",
      "title": "Computational science",
      "url": "https://en.wikipedia.org/wiki/Computational_science",
      "summary": "Computational science, also known as scientific computing, technical computing or scientific computation (SC), is a division of science that uses advanced computing capabilities to understand and solve complex physical problems. This includes \n\nAlgorithms (numerical and non-numerical): mathematical models, computational models, and computer simulations developed to solve sciences (e.g, physical, biological, and social), engineering, and humanities problems\nComputer hardware that develops and optimizes the advanced system hardware, firmware, networking, and data management components needed to solve computationally demanding problems\nThe computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information scienceIn practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiments, which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs and application software that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.\n\n"
    },
    {
      "id": "5311",
      "title": "Computer programming",
      "url": "https://en.wikipedia.org/wiki/Computer_programming",
      "summary": "Computer programming or coding is the composition of sequences of instructions, called programs, that computers can follow to perform tasks. It involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code, which is directly executed by the central processing unit. Proficient programming usually requires expertise in several different subjects, including knowledge of the application domain, details of programming languages and generic code libraries, specialized algorithms, and formal logic.\nAuxiliary tasks accompanying and related to programming include analyzing requirements, testing, debugging (investigating and fixing problems), implementation of build systems, and management of derived artifacts, such as programs' machine code. While these are sometimes considered programming, often the term software development is used for this larger overall process \u2013 with the terms programming, implementation, and coding reserved for the writing and editing of code per se. Sometimes software development is known as software engineering, especially when it employs formal methods or follows an engineering design process.\n\n"
    },
    {
      "id": "45443335",
      "title": "DJ Patil",
      "url": "https://en.wikipedia.org/wiki/DJ_Patil",
      "summary": "Dhanurjay \"DJ\" Patil (born August 3, 1974) is an American mathematician and computer scientist who served as the Chief Data Scientist of the United States Office of Science and Technology Policy from 2015 to 2017. He is the Head of Technology for Devoted Health. \nHe previously served as the Vice President of Product at RelateIQ, which was acquired by Salesforce.com, as Chief Product Officer of Color Labs, and as Head of Data Products and Chief Scientist of LinkedIn. His father, Suhas Patil, is a venture capitalist and the founder of Cirrus Logic.\n\n"
    },
    {
      "id": "2234333",
      "title": "Data (computer science)",
      "url": "https://en.wikipedia.org/wiki/Data_(computer_science)",
      "summary": "In computer science, data (treated as singular, plural, or as a mass noun) is any sequence of one or more symbols; datum is a single symbol of data. Data requires interpretation to become information. Digital data is data that is represented using the binary number system of ones (1) and zeros (0), instead of analog representation. In modern (post-1960) computer systems, all data is digital. \nData exists in three states: data at rest, data in transit and data in use. Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. Data sourced from an analog device, such as a temperature sensor, may be converted to digital using an analog-to-digital converter. Data representing quantities, characters, or symbols on which operations are performed by a computer are stored and recorded on magnetic, optical, electronic, or mechanical recording media, and transmitted in the form of digital electrical or optical signals. Data pass in and out of computers via peripheral devices.\nPhysical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, including arrays, graphs, and objects. Data structures can store data of many different types, including numbers, strings and even other data structures.\n\n"
    },
    {
      "id": "2588620",
      "title": "Data archaeology",
      "url": "https://en.wikipedia.org/wiki/Data_archaeology",
      "summary": "There are two conceptualisations of data archaeology, the technical definition and the social science definition.\nData archaeology (also data archeology) in the technical sense refers to the art and science of recovering computer data encoded and/or encrypted in now obsolete media or formats. Data archaeology can also refer to recovering information from damaged electronic formats after natural disasters or human error.\nIt entails the rescue and recovery of old data trapped in outdated, archaic or obsolete storage formats such as floppy disks, magnetic tape, punch cards and transforming/transferring that data to more usable formats.\nData archaeology in the social sciences usually involves an investigation into the source and history of datasets and the construction of these datasets. It involves mapping out the entire lineage of data, its nature and characteristics, its quality and veracity and how these affect the analysis and interpretation of the dataset.\nThe findings of performing data archaeology affect the level to which the conclusions parsed from data analysis can be trusted.The term data archaeology originally appeared in 1993 as part of the Global Oceanographic Data Archaeology and Rescue Project (GODAR). The original impetus for data archaeology came from the need to recover computerised records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of climate change. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the Nimbus 2 satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.NASA also utilises the services of data archaeologists to recover information stored on 1960s-era vintage computer tape, as exemplified by the Lunar Orbiter Image Recovery Project (LOIRP)."
    },
    {
      "id": "51443362",
      "title": "Data augmentation",
      "url": "https://en.wikipedia.org/wiki/Data_augmentation",
      "summary": "Data augmentation is a technique in machine learning used to reduce overfitting when training a machine learning model, achieved by training models on several slightly-modified copies of existing data.\n\n"
    },
    {
      "id": "3575651",
      "title": "Data cleansing",
      "url": "https://en.wikipedia.org/wiki/Data_cleansing",
      "summary": "Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.\nAfter cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.\nThe actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of \"varying file formats, naming conventions, and columns\", and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (\"st, rd, etc.\" to \"street, road, etcetera\").\n\n"
    },
    {
      "id": "11501746",
      "title": "Data collection",
      "url": "https://en.wikipedia.org/wiki/Data_collection",
      "summary": "Data collection or data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities, and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture evidence that allows data analysis to lead to the formulation of credible answers to the questions that have been posed.\nRegardless of the field of or preference for defining data (quantitative or qualitative), accurate data collection is essential to maintain research integrity. The selection of appropriate data collection instruments (existing, modified, or newly developed) and delineated instructions for their correct use reduce the likelihood of errors.\n\n"
    },
    {
      "id": "8013",
      "title": "Data compression",
      "url": "https://en.wikipedia.org/wiki/Data_compression",
      "summary": "In information theory, data compression, source coding, or bit-rate reduction is the process of encoding information using fewer bits than the original representation. Any particular compression is either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by removing unnecessary or less important information. Typically, a device that performs data compression is referred to as an encoder, and one that performs the reversal of the process (decompression) as a decoder.\nThe process of reducing the size of a data file is often referred to as data compression. In the context of data transmission, it is called source coding: encoding is done at the source of the data before it is stored or transmitted. Source coding should not be confused with channel coding, for error detection and correction or line coding, the means for mapping data onto a signal.\nCompression is useful because it reduces the resources required to store and transmit data. Computational resources are consumed in the compression and decompression processes. Data compression is subject to a space-time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data."
    },
    {
      "id": "1040512",
      "title": "Data corruption",
      "url": "https://en.wikipedia.org/wiki/Data_corruption",
      "summary": "Data corruption refers to errors in computer data that occur during writing, reading, storage, transmission, or processing, which introduce unintended changes to the original data. Computer, transmission, and storage systems use a number of measures to provide end-to-end data integrity, or lack of errors.\nIn general, when data corruption occurs, a file containing that data will produce unexpected results when accessed by the system or the related application. Results could range from a minor loss of data to a system crash. For example, if a document file is corrupted, when a person tries to open that file with a document editor they may get an error message, thus the file might not be opened or might open with some of the data corrupted (or in some cases, completely corrupted, leaving the document unintelligible). The adjacent image is a corrupted image file in which most of the information has been lost.\nSome types of malware may intentionally corrupt files as part of their payloads, usually by overwriting them with inoperative or garbage code, while a non-malicious virus may also unintentionally corrupt files when it accesses them. If a virus or trojan with this payload method manages to alter files critical to the running of the computer's operating system software or physical hardware, the entire system may be rendered unusable.\nSome programs can give a suggestion to repair the file automatically (after the error), and some programs cannot repair it. It depends on the level of corruption, and the built-in functionality of the application to handle the error. There are various causes of the corruption."
    },
    {
      "id": "23943140",
      "title": "Data curation",
      "url": "https://en.wikipedia.org/wiki/Data_curation",
      "summary": "Data curation is the organization and integration of data collected from various sources. It involves annotation, publication and presentation of the data so that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes \"all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data\". In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database.In the modern era of big data, the curation of data has become more prominent, particularly for software processing high volume and complex data systems.  The term is also used in historical occasions and the humanities, where increasing cultural and scholarly data from digital humanities projects requires the expertise and analytical practices of data curation. In broad terms, curation means a range of activities and processes done to create, manage, maintain, and validate a component. Specifically, data curation is the attempt to determine what information is worth saving and for how long.\n\n"
    },
    {
      "id": "372381",
      "title": "Data degradation",
      "url": "https://en.wikipedia.org/wiki/Data_degradation",
      "summary": "Data degradation is the gradual corruption of computer data due to an accumulation of non-critical failures in a data storage device. The phenomenon is also known as data decay, data rot or bit rot. This process leads to the slow deterioration of data quality over time, even if the data is not actively being used or accessed."
    },
    {
      "id": "42906439",
      "title": "Data editing",
      "url": "https://en.wikipedia.org/wiki/Data_editing",
      "summary": "Data editing is defined as the process involving the review and adjustment of collected survey data. Data editing helps define guidelines that will reduce potential bias and ensure consistent estimates leading to a clear analysis of the data set by correct inconsistent data using the methods later in this article. The purpose is to control the quality of the collected data. Data editing can be performed manually, with the assistance of a computer or a combination of both.\n\n"
    },
    {
      "id": "12097860",
      "title": "Data extraction",
      "url": "https://en.wikipedia.org/wiki/Data_extraction",
      "summary": "Data extraction is the act or process of retrieving data out of (usually unstructured or poorly structured) data sources for further data processing or data storage (data migration). The import into the intermediate extracting system is thus usually followed by data transformation and possibly the addition of metadata prior to export to another stage in the data workflow.\nUsually, the term data extraction is applied when (experimental) data is first imported into a computer from primary sources, like measuring or recording devices. Today's electronic devices will usually present an electrical connector (e.g. USB) through which 'raw data' can be streamed into a personal computer.\n\n"
    },
    {
      "id": "2690471",
      "title": "Data farming",
      "url": "https://en.wikipedia.org/wiki/Data_farming",
      "summary": "Data farming  is the process of using designed computational experiments to \u201cgrow\u201d data, which can then be analyzed using statistical and visualization techniques to obtain insight into complex systems.  These methods can be applied to any computational model.\nData farming differs from Data mining, as the following metaphors indicate: \n\nMiners seek valuable nuggets of ore buried in the earth, but have no control over what is out there or how hard it is to extract the nuggets from their surroundings. ...  Similarly, data miners seek to uncover valuable nuggets of information buried within massive amounts of data.  Data-mining techniques use statistical and graphical measures to try to identify interesting correlations or clusters in the data set.\nFarmers cultivate the land to maximize their yield.  They manipulate the environment to their advantage using irrigation, pest control, crop rotation, fertilizer, and more.  Small-scale designed experiments let them determine whether these treatments are effective.  Similarly, data farmers manipulate simulation models to their advantage, using large-scale designed experimentation to grow data from their models in a manner that easily lets them extract useful information. ...the results can reveal root cause-and-effect relationships between the model input factors and the model responses, in addition to rich graphical and statistical views of these relationships.\n\nA NATO modeling and simulation task group has documented the data farming process in the Final Report of MSG-088.\nHere, data farming uses collaborative processes in combining rapid scenario prototyping, simulation modeling, design of experiments, high performance computing, and analysis and visualization in an iterative loop-of-loops Archived 2015-08-29 at the Wayback Machine.\n\n"
    },
    {
      "id": "9328883",
      "title": "Data format management",
      "url": "https://en.wikipedia.org/wiki/Data_format_management",
      "summary": "Data format management (DFM) is the application of a systematic approach to the selection and use of the data formats used to encode information for storage on a computer. \nIn practical terms, data format management is the analysis of data formats and their associated technical, legal or economic attributes which can either enhance or detract from the ability of a digital asset or a given information systems to meet specified objectives.\nData format management is necessary as the amount of information and number of people creating it grows. This is especially the case as the information with which users are working is difficult to generate, store, costly to acquire, or to be shared.\nData format management as an analytic tool or approach is data format neutral.    \nHistorically individuals, organization and businesses have been categorized by their type of computer or their operating system.  Today, however, it is primarily productivity software, such as spreadsheet or word processor programs, and the way these programs store information that also defines an entity. For instance, when browsing the web it is not important which kind of computer is responsible for hosting a site, only that the information it publishes is in a format that is readable by the viewing browser. In this instance the data format of the published information has more to do with defining compatibilities than the underlying hardware or operating system. \nSeveral initiatives have been established to record those data formats commonly used and the software available to read them, for example the Pronom project at the UK National Archives.\n\n"
    },
    {
      "id": "7176679",
      "title": "Data fusion",
      "url": "https://en.wikipedia.org/wiki/Data_fusion",
      "summary": "Data fusion is the process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source.\nData fusion processes are often categorized as low, intermediate, or high, depending on the processing stage at which fusion takes place. Low-level data fusion combines several sources of raw data to produce new raw data. The expectation is that fused data is more informative and synthetic than the original inputs.\nFor example, sensor fusion is also known as (multi-sensor) data fusion and is a subset of information fusion.\nThe concept of data fusion has origins in the evolved capacity of humans and animals to incorporate information from multiple senses to improve their ability to survive. For example, a combination of sight, touch, smell, and taste may indicate whether a substance is edible.\n\n"
    },
    {
      "id": "4780372",
      "title": "Data integration",
      "url": "https://en.wikipedia.org/wiki/Data_integration",
      "summary": "Data integration involves combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains.  Data integration appears with increasing frequency as the volume, complexity (that is, big data) and the need to share existing data explodes.  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved. Data integration encourages collaboration between internal as well as external users. The data being integrated must be received from a heterogeneous database system and transformed to a single coherent data store that provides synchronous data across a network of files for clients. A common use of data integration is in data mining when analyzing and extracting information from existing databases that can be useful for Business information.\n\n"
    },
    {
      "id": "40995",
      "title": "Data integrity",
      "url": "https://en.wikipedia.org/wiki/Data_integrity",
      "summary": "Data integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle. It is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation is a prerequisite for data integrity.\n\n"
    },
    {
      "id": "24810701",
      "title": "PANGAEA (data library)",
      "url": "https://en.wikipedia.org/wiki/PANGAEA_(data_library)",
      "summary": "PANGAEA - Data Publisher for Earth & Environmental Science is a digital data library and a data publisher for earth system science. Data can be georeferenced in time (date/time or geological age) and space (latitude, longitude, depth/height).\nScientific data are archived with related metainformation in a relational database (Sybase) through an editorial system. Data are in Open Access and are distributed through web services in standard formats on the Internet through various search engines and portals. Data set descriptions (metadata) are conform to the ISO 19115 standard and are served in various further formats (e.g. Directory Interchange Format, Dublin Core). They include a bibliographic citation and are persistently identified using Digital Object Identifiers (DOI). Identifier provision and long-term availability of data sets via library catalogs is ensured through a cooperation with the German National Library of Science and Technology (TIB). Retrieval of data sets is provided through a full text search engine (based on Apache Lucene / panFMP). For efficient data compilations a data warehouse is operated. Data descriptions are available through various protocols (OAI-PMH, Web Catalog Service).\nPANGAEA is hosted by the Alfred Wegener Institute, Helmholtz Centre for Polar and Marine Research (AWI), Bremerhaven and the MARUM \u2013 Center for Marine Environmental Sciences, Bremen in Germany. The system is used by various international research projects from public funding as data repository and by the World Data Center for Marine Environmental Sciences (WDC-MARE) as long-term archive. The system was initially developed since 1987 and is operational on the Internet since 1995.\nThe MediaWiki software is used to operate a wiki as PANGAEA manual and reference.\nPANGAEA is also listed in re3data.org.\n\n"
    },
    {
      "id": "44783487",
      "title": "Data lineage",
      "url": "https://en.wikipedia.org/wiki/Data_lineage",
      "summary": "Data lineage includes the data origin, what happens to it, and where it moves over time. Data lineage provides visibility and simplifies tracing errors back to the root cause in a data analytics process.It also enables replaying specific portions or inputs of the data flow for step-wise debugging or regenerating lost output. Database systems use such information, called data provenance, to address similar validation and debugging challenges. Data provenance refers to records of the inputs, entities, systems, and processes that influence data of interest, providing a historical record of the data and its origins. The generated evidence supports forensic activities such as data-dependency analysis, error/compromise detection and recovery, auditing, and compliance analysis. \"Lineage is a simple type of why provenance.\"Data lineage can be represented visually to discover the data flow/movement from its source to destination via various changes and hops on its way in the enterprise environment, how the data gets transformed along the way, how the representation and parameters change, and how the data splits or converges after each hop. A simple representation of the Data Lineage can be shown with dots and lines, where dot represents a data container for data points and lines connecting them represents the transformations the data point undergoes, between the data containers.\nRepresentation broadly depends on the scope of the metadata management and reference point of interest. Data lineage provides sources of the data and intermediate data flow hops from the reference point with backward data lineage, leading to the final destination's data points and its intermediate data flows with forward data lineage. These views can be combined with end-to-end lineage for a reference point that provides a complete audit trail of that data point of interest from sources to their final destinations. As the data points or hops increase, the complexity of such representation becomes incomprehensible. Thus, the best feature of the data lineage view would be to be able to simplify the view by temporarily masking unwanted peripheral data points. Tools that have the masking feature enable scalability of the view and enhance analysis with the best user experience for both technical and business users. Data lineage also enables companies to trace sources of specific business data for the purposes of tracking errors, implementing changes in processes, and implementing system migrations to save significant amounts of time and resources, thereby tremendously improving BI efficiency.The scope of the data lineage determines the volume of metadata required to represent its data lineage. Usually, data governance, and data management determines the scope of the data lineage based on their regulations, enterprise data management strategy, data impact, reporting attributes, and critical data elements of the organization.\nData lineage provides the audit trail of the data points at the highest granular level, but presentation of the lineage may be done at various zoom levels to simplify the vast information, similar to analytic web maps. Data Lineage can be visualized at various levels based on the granularity of the view. At a very high level data lineage provides what systems the data interacts before it reaches destination. As the granularity increases it goes up to the data point level where it can provide the details of the data point and its historical behavior, attribute properties, and trends and data quality of the data passed through that specific data point in the data lineage.\nData governance plays a key role in metadata management for guidelines, strategies, policies, implementation. Data quality, and master data management helps in enriching the data lineage with more business value. Even though the final representation of data lineage is provided in one interface but the way the metadata is harvested and exposed to the data lineage graphical user interface could be entirely different. Thus, data lineage can be broadly divided into three categories based on the way metadata is harvested: data lineage involving software packages for structured data, programming languages, and big data.\nData lineage information includes technical metadata involving data transformations. Enriched data lineage information may include data quality test results, reference data values, data models, business vocabulary, data stewards, program management information, and enterprise information systems linked to the data points and transformations. Masking feature in the data lineage visualization allows the tools to incorporate all the enrichments that matter for the specific use case. To represent disparate systems into one common view, \"metadata normalization\" or standardization may be necessary.\n\n"
    },
    {
      "id": "1381282",
      "title": "Data loss",
      "url": "https://en.wikipedia.org/wiki/Data_loss",
      "summary": "Data loss is an error condition in information systems in which information is destroyed by failures (like failed spindle motors or head crashes on hard drives) or neglect (like mishandling, careless handling or storage under unsuitable conditions) in storage, transmission, or processing. Information systems implement backup and disaster recovery equipment and processes to prevent data loss or restore lost data. Data loss can also occur if the physical medium containing the data is lost or stolen. \nData loss is distinguished from data unavailability, which may arise from a network outage. Although the two have substantially similar consequences for users, data unavailability is temporary, while data loss may be permanent. Data loss is also distinct from data breach, an incident where data falls into the wrong hands, although the term data loss has been used in those incidents.\n\n"
    },
    {
      "id": "759312",
      "title": "Data management",
      "url": "https://en.wikipedia.org/wiki/Data_management",
      "summary": "Data management comprises all disciplines related to handling data as a valuable resource, it is  the practice of managing an organization\u2019s data so it can be analyzed for decision making.\n\n"
    },
    {
      "id": "1135408",
      "title": "Data migration",
      "url": "https://en.wikipedia.org/wiki/Data_migration",
      "summary": "Data migration is the process of selecting, preparing, extracting, and transforming data and permanently transferring it from one computer storage system to another. Additionally, the validation of migrated data for completeness and the decommissioning of legacy data storage are considered part of the entire data migration process. Data migration is a key consideration for any system implementation, upgrade, or consolidation, and it is typically performed in such a way as to be as automated as possible, freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, application migration, website consolidation, disaster recovery, and data center relocation.\n\n"
    },
    {
      "id": "49882988",
      "title": "Data philanthropy",
      "url": "https://en.wikipedia.org/wiki/Data_philanthropy",
      "summary": "Data philanthropy describes a form of collaboration in which private sector companies share data for public benefit. There are multiple uses of data philanthropy being explored from humanitarian, corporate, human rights, and academic use. Since introducing the term in 2011, the United Nations Global Pulse has advocated for a global \"data philanthropy movement\".\n\n"
    },
    {
      "id": "12386904",
      "title": "Data Preprocessing",
      "url": "https://en.wikipedia.org/wiki/Data_Preprocessing",
      "summary": "Data preprocessing can refer to manipulation, filtration or augmentation of data before it is analyzed, and is often an important step in the data mining process. Data collection methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, amongst other issues.\nThe preprocessing pipeline used can often have large effects on the conclusions drawn from the downstream analysis. Thus, representation and quality of data is necessary before running any analysis. \nOften, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is a high proportion of irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase may be more difficult. Data preparation and filtering steps can take a considerable amount of processing time. Examples of methods used in data preprocessing include cleaning, instance selection, normalization, one-hot encoding, data transformation, feature extraction and feature selection.\n\n"
    },
    {
      "id": "55810614",
      "title": "Data preservation",
      "url": "https://en.wikipedia.org/wiki/Data_preservation",
      "summary": "Data preservation is the act of conserving and maintaining both the safety and integrity of data. Preservation is done through formal activities that are governed by policies, regulations and strategies directed towards protecting and prolonging the existence and authenticity of data and its metadata. Data can be described as the elements or units in which knowledge and information is created,\n and metadata are the summarizing subsets of the elements of data; or the data about the data. The main goal of data preservation is to protect data from being lost or destroyed and to contribute to the reuse and progression of the data.\n\n"
    },
    {
      "id": "42813835",
      "title": "Data publishing",
      "url": "https://en.wikipedia.org/wiki/Data_publishing",
      "summary": "Data publishing (also data publication) is the act of releasing research data in published form for use by others. It is a practice consisting in preparing certain data or data set(s) for public use thus to make them available to everyone to use as they wish. \nThis practice is an integral part of the open science movement. \nThere is a large and multidisciplinary consensus on the benefits resulting from this practice.The main goal is to elevate data to be first class research outputs. There are a number of initiatives underway as well as points of consensus and issues still in contention.There are several distinct ways to make research data available, including: \n\npublishing data as supplemental material associated with a research article, typically with the data files hosted by the publisher of the article\nhosting data on a publicly available website, with files available for download\nhosting data in a repository that has been developed to support data publication, e.g. figshare, Dryad, Dataverse, Zenodo. A large number of general and specialty (such as by research topic) data repositories exist. For example, the UK Data Service enables users to deposit data collections and re-share these for research purposes.\npublishing a data paper about the dataset, which may be published as a preprint, in a regular journal, or in a data journal that is dedicated to supporting data papers. The data may be hosted by the journal or hosted separately in a data repository.Publishing data allows researchers to both make their data available to others to use, and enables datasets to be cited similarly to other research publication types (such as articles or books), thereby enabling producers of datasets to gain academic credit for their work.\nThe motivations for publishing data may range for a desire to make research more accessible, to enable citability of datasets, or research funder or publisher mandates that require open data publishing. The UK Data Service is one key organisation working with others to raise the importance of citing data correctly and helping researchers to do so.\nSolutions to preserve privacy within data publishing has been proposed, including privacy protection algorithms, data \u201dmasking\u201d methods, and regional privacy level calculation algorithm.\n\n"
    },
    {
      "id": "1609808",
      "title": "Data quality",
      "url": "https://en.wikipedia.org/wiki/Data_quality",
      "summary": "Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is \"fit for [its] intended uses in operations, decision making and planning\". Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.\n\n"
    },
    {
      "id": "2160183",
      "title": "Data recovery",
      "url": "https://en.wikipedia.org/wiki/Data_recovery",
      "summary": "In computing, data recovery is a process of retrieving deleted, inaccessible, lost, corrupted, damaged, or formatted data from secondary storage, removable media or files, when the data stored in them cannot be accessed in a usual way.  The data is most often salvaged from storage media such as internal or external hard disk drives (HDDs), solid-state drives (SSDs), USB flash drives, magnetic tapes, CDs, DVDs, RAID subsystems, and other electronic devices. Recovery may be required due to physical damage to the storage devices or logical damage to the file system that prevents it from being mounted by the host operating system (OS).Logical failures occur when the hard drive devices are functional but the user or automated-OS cannot retrieve or access data stored on them. Logical failures can occur due to corruption of the engineering chip, lost partitions, firmware failure, or failures during formatting/re-installation.Data recovery can be a very simple or technical challenge. This is why there are specific software companies specialized in this field.\n\n"
    },
    {
      "id": "23789529",
      "title": "Data reduction",
      "url": "https://en.wikipedia.org/wiki/Data_reduction",
      "summary": "Data reduction is the transformation of numerical or alphabetical digital information derived empirically or experimentally into a corrected, ordered, and simplified form. The purpose of data reduction can be two-fold: reduce the number of data records by eliminating invalid data or produce summary data and statistics at different aggregation levels for various applications. Data reduction does not necessarily mean loss of information. For example, the body mass index reduces two dimensions (body and mass) into a single measure, without any information being lost in the process.\nWhen information is derived from instrument readings there may also be a transformation from analog to digital form. When the data are already in digital form the 'reduction' of the data typically involves some editing, scaling, encoding, sorting, collating, and producing tabular summaries. When the observations are discrete but the underlying phenomenon is continuous then smoothing and interpolation are often needed. The data reduction is often undertaken in the presence of reading or measurement errors. Some idea of the nature of these errors is needed before the most likely value may be determined.\nAn example in astronomy is the data reduction in the Kepler satellite. This satellite records 95-megapixel images once every six seconds, generating dozens of megabytes of data per second, which is orders-of-magnitudes more than the downlink bandwidth of 550 kB/s. The on-board data reduction encompasses co-adding the raw frames for thirty minutes, reducing the bandwidth by a factor of 300. Furthermore, interesting targets are pre-selected and only the relevant pixels are processed, which is 6% of the total. This reduced data is then sent to Earth where it is processed further.\nResearch has also been carried out on the use of data reduction in wearable (wireless) devices for health monitoring and diagnosis applications. For example, in the context of epilepsy diagnosis, data reduction has been used to increase the battery lifetime of a wearable EEG device by selecting and only transmitting EEG data that is relevant for diagnosis and discarding background activity.\n\n"
    },
    {
      "id": "3095080",
      "title": "Data retention",
      "url": "https://en.wikipedia.org/wiki/Data_retention",
      "summary": "Data retention defines the policies of persistent data and records management for meeting legal and business data archival requirements. Although sometimes interchangeable, it is not to be confused with the Data Protection Act 1998.\nThe different data retention policies weigh legal and privacy concerns economics and need-to-know concerns to determine the retention time, archival rules, data formats, and the permissible means of storage, access, and encryption."
    },
    {
      "id": "46471245",
      "title": "Data scraping",
      "url": "https://en.wikipedia.org/wiki/Data_scraping",
      "summary": "Data scraping is a technique where a computer program extracts data from human-readable output coming from another program."
    },
    {
      "id": "3575656",
      "title": "Data scrubbing",
      "url": "https://en.wikipedia.org/wiki/Data_scrubbing",
      "summary": "Data scrubbing is an error correction technique that uses a background task to periodically inspect main memory or storage for errors, then corrects detected errors using redundant data in the form of different checksums or copies of data. Data scrubbing reduces the likelihood that single correctable errors will accumulate, leading to reduced risks of uncorrectable errors.\nData integrity is a high-priority concern in writing, reading, storage, transmission, or processing of the computer data in computer operating systems and in computer storage and data transmission systems.  However, only a few of the currently existing and used file systems provide sufficient protection against data corruption.To address this issue, data scrubbing provides routine checks of all inconsistencies in data and, in general, prevention of hardware or software failure. This \"scrubbing\" feature occurs commonly in memory, disk arrays, file systems, or FPGAs as a mechanism of error detection and correction.\n\n"
    },
    {
      "id": "1157832",
      "title": "Data security",
      "url": "https://en.wikipedia.org/wiki/Data_security",
      "summary": "Data security  means protecting digital data, such as those in a database, from destructive forces and from the unwanted actions of unauthorized users, such as a cyberattack or a data breach.\n\n"
    },
    {
      "id": "8495",
      "title": "Data set",
      "url": "https://en.wikipedia.org/wiki/Data_set",
      "summary": "A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set.  Data sets can also consist of a collection of documents or files.In the open data discipline, data set is the unit to measure the information released in a public open data repository.  The European data.europa.eu portal aggregates more than a million data sets.\n\n"
    },
    {
      "id": "60798295",
      "title": "Data sonification",
      "url": "https://en.wikipedia.org/wiki/Data_sonification",
      "summary": "Data sonification is the presentation of data as sound using sonification. It is the auditory equivalent of the more established practice of data visualization.\nThe usual process for data sonification is directing digital media of a dataset through a software synthesizer and into a digital-to-analog converter to produce sound for humans to experience.Applications of data sonification include astronomy studies of star creation, interpreting cluster analysis, and geoscience.Various projects describe the production of sonifications as a collaboration between scientists and musicians.A target demographic for using data sonification is the blind community because of the inaccessibility of data visualizations.\n\n"
    },
    {
      "id": "6212365",
      "title": "Data steward",
      "url": "https://en.wikipedia.org/wiki/Data_steward",
      "summary": "A data steward is an oversight or data governance role within an organization, and is responsible for ensuring the quality and fitness for purpose of the organization's data assets, including the metadata for those data assets.  A data steward may share some responsibilities with a data custodian, such as the awareness, accessibility, release, appropriate use, security and management of data.  A data steward would also participate in the development and implementation of data assets.  A data steward may seek to improve the quality and fitness for purpose of other data assets their organization depends upon but is not responsible for.\nData stewards have a specialist role that utilizes an organization's data governance processes, policies, guidelines and responsibilities for administering an organizations' entire data in compliance with policy and/or regulatory obligations. The overall objective of a data steward is the data quality of the data assets, datasets, data records and data elements. This includes documenting metainformation for the data, such as definitions, related rules/governance, physical manifestation, and related data models (most of these properties being specific to an attribute/concept relationship), identifying owners/custodian's various responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules.\nData stewards begin the stewarding process with the identification of the data assets and elements which they will steward, with the ultimate result being standards, controls and data entry.  The steward works closely with business glossary standards analysts (for standards), with data architect/modelers (for standards), with  DQ analysts (for controls) and with operations team members (good-quality data going in per business rules) while entering data.\nData stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.  Master data management often makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.\n\n"
    },
    {
      "id": "28174",
      "title": "Data storage",
      "url": "https://en.wikipedia.org/wiki/Data_storage",
      "summary": "Data storage is the recording (storing) of information (data) in a storage medium. Handwriting, phonographic recording, magnetic tape, and optical discs are all examples of storage media. Biological molecules such as RNA and DNA are considered by some as data storage. Recording may be accomplished with virtually any form of energy. Electronic data storage requires electrical power to store and retrieve data. \nData storage in a digital, machine-readable medium is sometimes called digital data. Computer data storage is one of the core functions of a general-purpose computer. Electronic documents can be stored in much less space than paper documents. Barcodes and magnetic ink character recognition (MICR) are two ways of recording machine-readable data on paper.\n\n"
    },
    {
      "id": "7360695",
      "title": "Data synchronization",
      "url": "https://en.wikipedia.org/wiki/Data_synchronization",
      "summary": "Data synchronization is the process of establishing consistency between source and target data stores, and the continuous harmonization of the data over time. It is fundamental to a wide variety of applications, including file synchronization and mobile device synchronization.\nData synchronization can also be useful in encryption for synchronizing public key servers.\nData synchronization is needed to update and keep multiple copies of a set of data coherent with one another or to maintain data integrity, Figure 3. For example, database replication is used to keep multiple copies of data synchronized with database servers that store data in different locations.\n\n"
    },
    {
      "id": "4080917",
      "title": "Data transformation (computing)",
      "url": "https://en.wikipedia.org/wiki/Data_transformation_(computing)",
      "summary": "In computing, data transformation is the process of converting data from one format or structure into another format or structure. It is a fundamental aspect of most data integration and data management tasks such as data wrangling, data warehousing, data integration and application integration.\nData transformation can be simple or complex based on the required changes to the data between the source (initial) data and the target (final) data. Data transformation is typically performed via a mixture of manual and automated steps. Tools and technologies used for data transformation can vary widely based on the format, structure, complexity, and volume of the data being transformed.\nA master data recast is another form of data transformation where the entire database of data values is transformed or recast without extracting the data from the database.  All data in a well designed database is directly or indirectly related to a limited set of master database tables by a network of foreign key constraints.  Each foreign key constraint is dependent upon a unique database index from the parent database table.  Therefore, when the proper master database table is recast with a different unique index, the directly and indirectly related data are also recast or restated.  The directly and indirectly related data may also still be viewed in the original form since the original unique index still exists with the master data.  Also, the database recast must be done in such a way as to not impact the applications architecture software.\nWhen the data mapping is indirect via a mediating data model, the process is also called data mediation.\n\n"
    },
    {
      "id": "1705399",
      "title": "Data validation",
      "url": "https://en.wikipedia.org/wiki/Data_validation",
      "summary": "In computing, data validation is the process of ensuring data has undergone data cleansing to confirm they have data quality, that is, that they are both correct and useful. It uses routines, often called \"validation rules\", \"validation constraints\", or \"check routines\", that check for correctness, meaningfulness, and security of data that are input to the system. The rules may be implemented through the automated facilities of a data dictionary, or by the inclusion of explicit application program validation logic of the computer and its application.\nThis is distinct from formal verification, which attempts to prove or disprove the correctness of algorithms for implementing a specification or property.\n\n"
    },
    {
      "id": "3461736",
      "title": "Data and information visualization",
      "url": "https://en.wikipedia.org/wiki/Data_and_information_visualization",
      "summary": "Data and information visualization (data viz/vis or info viz/vis) is the practice of designing and creating easy-to-communicate and easy-to-understand graphic or visual representations of a large amount of complex quantitative and qualitative data and information with the help of static, dynamic or interactive visual items. Typically based on data and information collected from a certain domain of expertise, these visualizations are intended for a broader audience to help them visually explore and discover, quickly understand, interpret and gain important insights into otherwise difficult-to-identify structures, relationships, correlations, local and global patterns, trends, variations, constancy, clusters, outliers and unusual groupings within data (exploratory visualization). When intended for the general public (mass communication) to convey a concise version of known, specific information in a clear and engaging manner (presentational or explanatory visualization), it is typically called information graphics.  \nData visualization is concerned with visually presenting sets of primarily quantitative raw data in a schematic form. The visual formats used in data visualization include tables, charts and graphs (e.g. pie charts, bar charts, line charts, area charts, cone charts, pyramid charts, donut charts, histograms, spectrograms, cohort charts, waterfall charts, funnel charts, bullet graphs, etc.), diagrams, plots (e.g. scatter plots, distribution plots, box-and-whisker plots), geospatial maps (such as proportional symbol maps, choropleth maps, isopleth maps and heat maps), figures, correlation matrices, percentage gauges, etc., which sometimes can be combined in a dashboard. \nInformation visualization, on the other hand, deals with multiple, large-scale and complicated datasets which contain quantitative (numerical) data as well as qualitative (non-numerical, i.e. verbal or graphical) and primarily abstract information and its goal is to add value to raw data, improve the viewers' comprehension, reinforce their cognition and help them derive insights and make decisions as they navigate and interact with the computer-supported graphical display. Visual tools used in information visualization include maps (such as tree maps), animations, infographics, Sankey diagrams, flow charts, network diagrams, semantic networks, entity-relationship diagrams, venn diagrams, timelines, mind maps, etc.\nEmerging technologies like virtual, augmented and mixed reality have the potential to make information visualization more immersive, intuitive, interactive and easily manipulable and thus enhance the user's visual perception and cognition. In data and information visualization, the goal is to graphically present and explore abstract, non-physical and non-spatial data collected from databases, information systems, file systems, documents, business and financial data, etc. (presentational and exploratory visualization) which is different from the field of scientific visualization, where the goal is to render realistic images based on physical and spatial scientific data to confirm or reject hypotheses (confirmatory visualization).Effective data visualization is properly sourced, contextualized, simple and uncluttered. The underlying data is accurate and up-to-date to make sure that insights are reliable. Graphical items are well-chosen for the given datasets and aesthetically appealing, with shapes, colors and other visual elements used deliberately in a meaningful and non-distracting manner. The visuals are accompanied by supporting texts (labels and titles). These verbal and graphical components complement each other to ensure clear, quick and memorable understanding. Effective information visualization is aware of the needs and concerns and the level of expertise of the target audience, deliberately guiding them to the intended conclusion. Such effective visualization can be used not only for conveying specialized, complex, big data-driven ideas to a wider group of non-technical audience in a visually appealing, engaging and accessible manner, but also to domain experts and executives for making decisions, monitoring performance, generating new ideas and stimulating research. In addition, data scientists, data analysts and data mining specialists use data visualization to check the quality of data, find errors, unusual gaps and missing values in data, clean data, explore the structures and features of data and assess outputs of data-driven models. In business, data and information visualization can constitute a part of data storytelling, where they are paired with a coherent narrative structure or storyline to contextualize the analyzed data and communicate the insights gained from analyzing the data clearly and memorably with the goal of convincing the audience into making a decision or taking an action in order to create business value. This can be contrasted with the field of statistical graphics, where complex statistical data are communicated graphically in an accurate and precise manner among researchers and analysts with statistical expertise to help them perform exploratory data analysis or to convey the results of such analyses, where visual appeal, capturing attention to a certain issue and storytelling are not as important.The field of data and information visualization is of interdisciplinary nature as it incorporates principles found in the disciplines of descriptive statistics (as early as the 18th century), visual communication, graphic design, cognitive science and, more recently, interactive computer graphics and human-computer interaction. Since effective visualization requires design skills, statistical skills and computing skills, it is argued by authors such as Gershon and Page that it is both an art and a science. The neighboring field of visual analytics marries statistical data analysis, data and information visualization and human analytical reasoning through interactive visual interfaces to help human users reach conclusions, gain actionable insights and make informed decisions which are otherwise difficult for computers to do.\nResearch into how people read and misread various types of visualizations is helping to determine what types and features of visualizations are most understandable and effective in conveying information. On the other hand, unintentionally poor or intentionally misleading and deceptive visualizations (misinformative visualization) can function as powerful tools which disseminate misinformation, manipulate public perception and divert public opinion toward a certain agenda. Thus data visualization literacy has become an important component of data and information literacy in the information age akin to the roles played by textual, mathematical and visual literacy in the past.\n\n"
    },
    {
      "id": "12487489",
      "title": "Data wrangling",
      "url": "https://en.wikipedia.org/wiki/Data_wrangling",
      "summary": "Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. The goal of data wrangling is to assure quality and useful data. Data analysts typically spend the majority of their time in the process of data wrangling compared to the actual analysis of the data.\nThe process of data wrangling may include further munging, data visualization, data aggregation, training a statistical model, as well as many other potential uses. Data wrangling typically follows a set of general steps which begin with extracting the data in a raw form from the data source, \"munging\" the raw data (e.g. sorting) or parsing the data into predefined data structures, and finally depositing the resulting content into a data sink for storage and future use. It is closely aligned with the ETL process. \n\n"
    },
    {
      "id": "265752",
      "title": "Decision-making",
      "url": "https://en.wikipedia.org/wiki/Decision-making",
      "summary": "In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action.\nResearch about decision-making is also published under the label problem solving, particularly in European psychological research."
    },
    {
      "id": "8501",
      "title": "Distributed computing",
      "url": "https://en.wikipedia.org/wiki/Distributed_computing",
      "summary": "A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. Distributed computing is a field of computer science that studies distributed systems. \nThe components of a distributed system interact with one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.\nA computer program that runs within a distributed system is called  a distributed program, and distributed programming is the process of writing such programs. There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing."
    },
    {
      "id": "750101",
      "title": "Domain knowledge",
      "url": "https://en.wikipedia.org/wiki/Domain_knowledge",
      "summary": "Domain knowledge is knowledge of a specific discipline or field  in contrast to general (or domain-independent) knowledge.  The term is often used in reference to a more general discipline\u2014for example, in describing a software engineer who has general knowledge of computer programming as well as domain knowledge about developing programs for a particular industry. People with domain knowledge are often regarded as specialists or experts in their field.\n\n"
    },
    {
      "id": "9545",
      "title": "Empirical research",
      "url": "https://en.wikipedia.org/wiki/Empirical_research",
      "summary": "Empirical research is research using empirical evidence. It is also a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values some research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions that cannot be studied in laboratory settings, particularly in the social sciences and in education.\nIn some fields, quantitative research may begin with a research question (e.g., \"Does listening to vocal music during the learning of a word list have an effect on later memory for these words?\") which is tested through experimentation. Usually, the researcher has a certain theory regarding the topic under investigation. Based on this theory, statements or hypotheses will be proposed (e.g., \"Listening to vocal music has a negative effect on learning a word list.\"). From these hypotheses, predictions about specific events are derived (e.g., \"People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence.\"). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.\n\n"
    },
    {
      "id": "72826783",
      "title": "Exploration",
      "url": "https://en.wikipedia.org/wiki/Exploration",
      "summary": "Exploration is the process of exploring, an activity which has some expectation of discovery. Organised exploration is largely a human activity, but exploratory activity is common to most organisms capable of directed locomotion and the ability to learn, and has been described in, amongst others, social insects foraging behaviour, where feedback from returning individuals affects the activity of other members of the group.Exploration has been defined as:\n\nTo travel somewhere in search of discovery.\nTo examine or investigate something systematically.\nTo examine diagnostically.\nTo (seek) experience first hand.\nTo wander without any particular aim or purpose.In all these definitions there is an implication of novelty, or unfamiliarity or the expectation of discovery in the exploration, whereas a survey implies directed examination, but not necessarily discovery of any previously unknown or unexpected information. The activities are not mutually exclusive, and often occur simultaneously to a variable extent. The same field of investigation or region may be explored at different times by different explorers with different motivations, who may make similar or different discoveries.\nIntrinsic exploration involves activity that is not directed towards a specific goal other than the activity itself.Extrinsic exploration has the same meaning as appetitive behavior. It is directed towards a specific goal.\n\n"
    },
    {
      "id": "46363781",
      "title": "Extract, load, transform",
      "url": "https://en.wikipedia.org/wiki/Extract,_load,_transform",
      "summary": "Extract, load, transform (ELT) is an alternative to extract, transform, load (ETL) used with data lake implementations. In contrast to ETL, in ELT models the data is not transformed on entry to the data lake, but stored in its original raw format. This enables faster loading times. However, ELT requires sufficient processing power within the data processing engine to carry out the transformation on demand, to return the results in a timely manner. Since the data is not processed on entry to the data lake, the query and schema do not need to be defined a priori (although often the schema will be available during load since many data sources are extracts from databases or similar structured data systems and hence have an associated schema). ELT is a data pipeline model.\n\n"
    },
    {
      "id": "46207323",
      "title": "Feature engineering",
      "url": "https://en.wikipedia.org/wiki/Feature_engineering",
      "summary": "Feature engineering or feature extraction  or feature discovery is the process of extracting features (characteristics, properties, attributes) from raw data to support training a downstream statistical model.Other examples of features in physics include the construction of dimensionless numbers such as the Reynolds number in fluid dynamics, the Nusselt number in heat transfer, the Archimedes number in sedimentation, and construction of first approximations of the solution such as analytical strength of materials solutions in mechanics."
    },
    {
      "id": "294894",
      "title": "Forbes",
      "url": "https://en.wikipedia.org/wiki/Forbes",
      "summary": "Forbes () is an American business magazine founded  by B.C. Forbes in 1917 and owned by Hong Kong-based investment group Integrated Whale Media Investments since 2014. Its chairperson and editor-in-chief is Steve Forbes, and its CEO is Mike Federle. It is based in Jersey City, New Jersey. Competitors in the national business magazine category include Fortune and Bloomberg Businessweek. \nPublished eight times a year, Forbes features articles on finance, industry, investing, and marketing topics. It also reports on related subjects such as technology, communications, science, politics, and law. It has an international edition in Asia as well as editions produced under license in 27 countries and regions worldwide. The magazine is known for its lists and rankings, including of the richest Americans (the Forbes 400), the 30 most notable young people under the age of 30 (Forbes 30 under 30), America's Wealthiest Celebrities, the world's top companies (the Forbes Global 2000), Forbes list of the World's Most Powerful People, and The World's Billionaires. The motto of Forbes magazine is \"Change the World\"."
    },
    {
      "id": "12799",
      "title": "Graphic design",
      "url": "https://en.wikipedia.org/wiki/Graphic_design",
      "summary": "Graphic design is a profession, academic discipline and applied art whose activity consists in projecting visual communications intended to transmit specific messages to social groups, with specific objectives. Graphic design is an interdisciplinary branch of design and of the fine arts. Its practice involves creativity, innovation and lateral thinking using manual or digital tools, where it is usual to use text and graphics to communicate visually.\nThe role of the graphic designer in the communication process is that of encoder or interpreter of the message. They work on the interpretation, ordering, and presentation of visual messages. Usually, graphic design uses the aesthetics of typography and the compositional arrangement of the text, ornamentation, and imagery to convey ideas, feelings, and attitudes beyond what language alone expresses. The design work can be based on a customer's demand, a demand that ends up being established linguistically, either orally or in writing, that is, that graphic design transforms a linguistic message into a graphic manifestation.Graphic design has, as a field of application, different areas of knowledge focused on any visual communication system. For example, it can be applied in advertising strategies, or it can also be applied in the aviation world or space exploration. In this sense, in some countries graphic design is related as only associated with the production of sketches and drawings, this is incorrect, since visual communication is a small part of a huge range of types and classes where it can be applied.\nWith origins in Antiquity and the Middle Ages, graphic design as applied art was initially linked to the boom of rise of printing in Europe in the 15th century and the growth of consumer culture in the Industrial Revolution. From there it emerged as a distinct profession in the West, closely associated with advertising in the 19th century and its evolution allowed its consolidation in the 20th century. Given the rapid and massive growth in information exchange today, the demand for experienced designers is greater than ever, particularly because of the development of new technologies and the need to pay attention to human factors beyond the competence of the engineers who develop them."
    },
    {
      "id": "1822696",
      "title": "Harvard Business Review",
      "url": "https://en.wikipedia.org/wiki/Harvard_Business_Review",
      "summary": "Harvard Business Review (HBR) is a general management magazine  published by Harvard Business Publishing, a not-for-profit, independent corporation that is an affiliate of Harvard Business School. HBR is published six times a year and is headquartered in Brighton, Massachusetts.\nHBR covers a wide range of topics that are relevant to various industries, management functions, and geographic locations. These include leadership, negotiation, strategy, operations, marketing, and finance.Harvard Business Review has published articles by Clayton Christensen, Peter F. Drucker, Justin Fox, Michael E. Porter, Rosabeth Moss Kanter, John Hagel III, Thomas H. Davenport, Gary Hamel, C. K. Prahalad, Vijay Govindarajan, Robert S. Kaplan, Rita Gunther McGrath and others. Several management concepts and business terms were first given prominence in HBR.\nHarvard Business Review's worldwide English-language circulation is 250,000. HBR licenses its content for publication in nine international editions."
    },
    {
      "id": "23534602",
      "title": "Human\u2013computer interaction",
      "url": "https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction",
      "summary": "Human\u2013computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a \"Human-computer Interface (HCI)\".\nAs a field of research, human\u2013computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human\u2013Computer Interaction. The first known use was in 1975 by Carlisle. The term is intended to convey that, unlike other tools with specific and limited uses, computers have many uses which often involve an open-ended dialogue between the user and the computer. The notion of dialogue likens human\u2013computer interaction to human-to-human interaction: an analogy that is crucial to theoretical considerations in the field."
    },
    {
      "id": "14919",
      "title": "ISBN",
      "url": "https://en.wikipedia.org/wiki/ISBN",
      "summary": "The International Standard Book Number (ISBN) is a numeric commercial book identifier that is intended to be unique. Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency.An ISBN is assigned to each separate edition and variation (except reprintings) of a publication. For example, an e-book, a paperback and a hardcover edition of the same book must each have a different ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.\nThe initial ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the 9-digit SBN code can be converted to a 10-digit ISBN by prefixing it with a zero).\nPrivately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns such books ISBNs on its own initiative.Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.\n\n"
    },
    {
      "id": "234930",
      "title": "ISSN",
      "url": "https://en.wikipedia.org/wiki/ISSN",
      "summary": "An International Standard Serial Number (ISSN) is an eight-digit serial number used to uniquely identify a serial publication (periodical), such as a magazine. The ISSN is especially helpful in distinguishing between serials with the same title. ISSNs are used in ordering, cataloging, interlibrary loans, and other practices in connection with serial literature.The ISSN system was first drafted as an International Organization for Standardization (ISO) international standard in 1971 and published as ISO 3297 in 1975. ISO subcommittee TC 46/SC 9 is responsible for maintaining the standard.\nWhen a serial with the same content is published in more than one media type, a different ISSN is assigned to each media type. For example, many serials are published both in print and electronic media. The ISSN system refers to these types as print ISSN (p-ISSN) and electronic ISSN (e-ISSN). Consequently, as defined in ISO 3297:2007, every serial in the ISSN system is also assigned a linking ISSN (ISSN-L), typically the same as the ISSN assigned to the serial in its first published medium, which links together all ISSNs assigned to the serial in every medium.\n\n"
    },
    {
      "id": "2862975",
      "title": "Informatik",
      "url": "https://en.wikipedia.org/wiki/Informatik",
      "summary": "Informatik formerly known as Inform\u00e4tik is an electro-industrial/futurepop duo from Boston that was formed in 1993 and is represented by Metropolis Records in the US and Dependent Records in Europe. The band were repeat contributors to the \"Mind/Body\" compilation series organized by participants of the rec.music.industrial Usenet group in the mid-nineties. Both members are vegans."
    },
    {
      "id": "14774",
      "title": "Information explosion",
      "url": "https://en.wikipedia.org/wiki/Information_explosion",
      "summary": "The information explosion is the rapid increase in the amount of published information or data and the effects of this abundance. As the amount of available data grows, the problem of managing the information becomes more difficult, which can lead to information overload. The Online Oxford English Dictionary indicates use of the phrase in a March 1964 New Statesman article. The New York Times first used the phrase in its editorial content in an article by Walter Sullivan on June 7, 1964, in which he described the phrase as \"much discussed\". (p11.)  The earliest known use of the phrase was in a speech about television by NBC president Pat Weaver at the Institute of Practitioners of Advertising in London on September 27, 1955. The speech was rebroadcast on radio station WSUI in Iowa and excerpted in the Daily Iowan newspaper two months later.Many sectors are seeing this rapid increase in the amount of information available such as healthcare, supermarkets, and governments. Another sector that is being affected by this phenomenon is journalism. Such a profession, which in the past was responsible for the dissemination of information, may be suppressed by the overabundance of information today.Techniques to gather knowledge from an overabundance of electronic information (e.g., data fusion may help in data mining) have existed since the 1970s. Another common technique to deal with such amount of information is qualitative research. Such approaches aim to organize the information, synthesizing, categorizing and systematizing in order to be more usable and easier to search.\n\n"
    },
    {
      "id": "149354",
      "title": "Information science",
      "url": "https://en.wikipedia.org/wiki/Information_science",
      "summary": "Information science (also known as information studies) is an academic field which is primarily concerned with analysis, collection, classification, manipulation, storage, retrieval, movement, dissemination, and protection of information. Practitioners within and outside the field study the  application and the usage of knowledge in organizations in addition to the interaction between people, organizations, and any existing information systems with the aim of creating, replacing, improving, or understanding the information systems.\nHistorically, information science (informatics) is associated with computer science, data science, psychology, technology, library science, healthcare, and intelligence agencies. However, information science also incorporates aspects of diverse fields such as archival science, cognitive science, commerce, law, linguistics, museology, management, mathematics, philosophy, public policy, and social sciences."
    },
    {
      "id": "15201",
      "title": "Interdisciplinarity",
      "url": "https://en.wikipedia.org/wiki/Interdisciplinarity",
      "summary": "Interdisciplinarity or interdisciplinary studies involves the combination of multiple academic disciplines into one activity (e.g., a research project). It draws knowledge from several other fields like sociology, anthropology, psychology, economics, etc.  It is about creating something by thinking across boundaries. It is related to an interdiscipline or an interdisciplinary field, which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. Large engineering teams are usually interdisciplinary, as a power station or mobile phone or other project requires the melding of several specialties. However, the term \"interdisciplinary\" is sometimes confined to academic settings.\nThe term interdisciplinary is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies\u2014along with their specific perspectives\u2014in the pursuit of a common task. The epidemiology of HIV/AIDS or global warming requires understanding of diverse disciplines to solve complex problems. Interdisciplinary may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields.\nThe adjective interdisciplinary is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines.  For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.\n\n"
    },
    {
      "id": "68833",
      "title": "Iteration",
      "url": "https://en.wikipedia.org/wiki/Iteration",
      "summary": "Iteration is the repetition of a process in order to generate a (possibly unbounded) sequence of outcomes. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration. \nIn mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms."
    },
    {
      "id": "48996671",
      "title": "Jeff Hammerbacher",
      "url": "https://en.wikipedia.org/wiki/Jeff_Hammerbacher",
      "summary": "Jeff Hammerbacher (born 1982 or 1983) is a data scientist. He was chief scientist and cofounder at Cloudera and later served on the faculty of the Icahn School of Medicine at Mount Sinai.\n\n"
    },
    {
      "id": "308367",
      "title": "Jim Gray (computer scientist)",
      "url": "https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist)",
      "summary": "James Nicholas Gray (1944 \u2013 declared dead in absentia 2012) was an American computer scientist who received the Turing Award in 1998 \"for seminal contributions to database and transaction processing research and technical leadership in system implementation\"."
    },
    {
      "id": "31074852",
      "title": "Journal of Computational and Graphical Statistics",
      "url": "https://en.wikipedia.org/wiki/Journal_of_Computational_and_Graphical_Statistics",
      "summary": "The Journal of Computational and Graphical Statistics is a quarterly peer-reviewed scientific journal published by Taylor & Francis on behalf of the American Statistical Association. Established in 1992, the journal covers the use of computational and graphical methods in statistics and data analysis, including numerical methods, graphical displays and methods, and perception. It is published jointly with the Institute of Mathematical Statistics and the Interface Foundation of North America. According to the Journal Citation Reports, the journal has a 2021 impact factor of 1.884."
    },
    {
      "id": "243391",
      "title": "Knowledge",
      "url": "https://en.wikipedia.org/wiki/Knowledge",
      "summary": "Knowledge is an awareness of facts, a familiarity with individuals and situations, or a practical skill. Knowledge of facts, also called propositional knowledge, is often characterized as true belief that is distinct from opinion or guesswork by virtue of justification. While there is wide agreement among philosophers that propositional knowledge is a form of true belief, many controversies focus on justification. This includes questions like how to understand justification, whether it is needed at all, and whether something else besides it is needed. These controversies intensified in the latter half of the 20th century due to a series of thought experiments that provoked alternative definitions.\nKnowledge can be produced in many ways. The main source of empirical knowledge is perception, which involves the usage of the senses to learn about the external world. Introspection allows people to learn about their internal mental states and processes. Other sources of knowledge include memory, rational intuition, inference, and testimony. According to foundationalism, some of these sources are basic in that they can justify beliefs, without depending on other mental states. Coherentists reject this claim and contend that a sufficient degree of coherence among all the mental states of the believer is necessary for knowledge. According to infinitism, an infinite chain of beliefs is needed.\nThe main discipline investigating knowledge is epistemology, which studies what people know, how they come to know it, and what it means to know something. It discusses the value of knowledge and the thesis of philosophical skepticism, which questions the possibility of knowledge. Knowledge is relevant to many fields like the sciences, which aim to acquire knowledge using the scientific method based on repeatable experimentation, observation, and measurement. Various religions hold that humans should seek knowledge and that God or the divine is the source of knowledge. The anthropology of knowledge studies how knowledge is acquired, stored, retrieved, and communicated in different cultures. The sociology of knowledge examines under what sociohistorical circumstances knowledge arises, and what sociological consequences it has. The history of knowledge investigates how knowledge in different fields has developed, and evolved, in the course of history.\n\n"
    },
    {
      "id": "31002435",
      "title": "Knowledge extraction",
      "url": "https://en.wikipedia.org/wiki/Knowledge_extraction",
      "summary": "Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\nThe RDB2RDF W3C group  is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).\n\n"
    },
    {
      "id": "233488",
      "title": "Machine learning",
      "url": "https://en.wikipedia.org/wiki/Machine_learning",
      "summary": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to many fields including large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\n\n"
    },
    {
      "id": "3606435",
      "title": "Montpellier 2 University",
      "url": "https://en.wikipedia.org/wiki/Montpellier_2_University",
      "summary": "Montpellier 2 University (Universit\u00e9 Montpellier 2) was a French university in the acad\u00e9mie of Montpellier. It was one of the three universities formed in 1970 from the original University of Montpellier. Its main campus neighbors the Montpellier 3 University's main campus, and for this reason the nearest tramway station is named \"Universities of Sciences and Literature\" rather than \"University of Sciences\". In January 2015, Montpellier 1 University and Montpellier 2 University merged into the Montpellier University (Universit\u00e9 de Montpellier)."
    },
    {
      "id": "62500799",
      "title": "Nathan Yau",
      "url": "https://en.wikipedia.org/wiki/Nathan_Yau",
      "summary": "Nathan Yau is an American statistician and data visualization expert."
    },
    {
      "id": "3156699",
      "title": "National Science Board",
      "url": "https://en.wikipedia.org/wiki/National_Science_Board",
      "summary": "The National Science Board (NSB) of the United States establishes the policies of the National Science Foundation (NSF) within the framework of applicable national policies set forth by the president and the Congress. The NSB also serves as an independent policy advisory body to the president and Congress on science and engineering research and education issues. The board has a statutory obligation to \"...render to the President and to the Congress reports on specific, individual policy matters related to science and engineering and education in science engineering, as Congress or the President determines the need for such reports,\" (e.g. Science and Engineering Indicators; Report to Congress on Mid-scale Instrumentation at the National Science Foundation). All board members are presidential appointees. NSF's director serves as an ex officio 25th member and is appointed by the president and confirmed by the US Senate.\n\n"
    },
    {
      "id": "883885",
      "title": "OCLC",
      "url": "https://en.wikipedia.org/wiki/OCLC",
      "summary": "OCLC, Inc., doing business as OCLC, is an American nonprofit cooperative organization \"that provides shared technology services, original research, and community programs for its membership and the library community at large\". It was founded in 1967 as the Ohio College Library Center, then became the Online Computer Library Center as it expanded. In 2017, the name was formally changed to OCLC, Inc. OCLC and thousands of its member libraries cooperatively produce and maintain WorldCat, the largest online public access catalog in the world. OCLC is funded mainly by the fees that libraries pay (around $217.8 million annually in total as of 2021) for the many different services it offers. OCLC also maintains the Dewey Decimal Classification system."
    },
    {
      "id": "503009",
      "title": "PubMed",
      "url": "https://en.wikipedia.org/wiki/PubMed",
      "summary": "PubMed is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. The United States National Library of Medicine (NLM) at the National Institutes of Health maintain the database as part of the Entrez system of information retrieval.From 1971 to 1997, online access to the MEDLINE database had been primarily through institutional facilities, such as university libraries. PubMed, first released in January 1996, ushered in the era of private, free, home- and office-based MEDLINE searching. The PubMed system was offered free to the public starting in June 1997.\n\n"
    },
    {
      "id": "58731",
      "title": "Peter Naur",
      "url": "https://en.wikipedia.org/wiki/Peter_Naur",
      "summary": "Peter Naur (25 October 1928 \u2013 3 January 2016) was a Danish computer science pioneer and 2005 Turing award winner. He is best remembered as a contributor, with John Backus, to the Backus\u2013Naur form (BNF) notation used in describing the syntax for most programming languages. He also contributed to creating the language ALGOL 60.\n\n"
    },
    {
      "id": "60931",
      "title": "Phenomenon",
      "url": "https://en.wikipedia.org/wiki/Phenomenon",
      "summary": "A phenomenon  (pl.: phenomena), sometimes spelled phaenomenon, is an observable event. The term came into its modern philosophical usage through Immanuel Kant, who contrasted it with the noumenon, which cannot be directly observed. Kant was heavily influenced by Gottfried Wilhelm Leibniz in this part of his philosophy, in which phenomenon and noumenon serve as interrelated technical terms. Far predating this, the ancient Greek Pyrrhonist philosopher Sextus Empiricus also used phenomenon and noumenon as interrelated technical terms."
    },
    {
      "id": "1467948",
      "title": "Problem solving",
      "url": "https://en.wikipedia.org/wiki/Problem_solving",
      "summary": "Problem solving is the process of achieving a goal by overcoming obstacles, a frequent part of most activities. Problems in need of solutions range from simple personal tasks (e.g. how to turn on an appliance) to complex issues in business and technical fields. The former is an example of simple problem solving (SPS) addressing one issue, whereas the latter is complex problem solving (CPS) with multiple interrelated obstacles. Another classification of problem-solving tasks is into well-defined problems with specific obstacles and goals, and ill-defined problems in which the current situation is troublesome but it is not clear what kind of resolution to aim for. Similarly, one may distinguish formal or fact-based problems requiring psychometric intelligence, versus socio-emotional problems which depend on the changeable emotions of individuals or groups, such as tactful behavior, fashion, or gift choices.Solutions require sufficient resources and knowledge to attain the goal. Professionals such as lawyers, doctors, programmers, and consultants are largely problem solvers for issues that require technical skills and knowledge beyond general competence. Many businesses have found profitable markets by recognizing a problem and creating a solution: the more widespread and inconvenient the problem, the greater the opportunity to develop a scalable solution.\nThere are many specialized problem-solving techniques and methods in fields such as engineering, business, medicine, mathematics, computer science, philosophy, and social organization. The mental techniques to identify, analyze, and solve problems are studied in psychology and cognitive sciences. Also widely researched are the mental obstacles that prevent people from finding solutions; problem-solving impediments include confirmation bias, mental set, and functional fixedness."
    },
    {
      "id": "376707",
      "title": "R (programming language)",
      "url": "https://en.wikipedia.org/wiki/R_(programming_language)",
      "summary": "R is a programming language for statistical computing and data visualization. It has been adopted in the fields of data mining, bioinformatics, and data analysis.The core R language is augmented by a large number of extension packages, containing reusable code, documentation, and sample data.\nR software is open-source and free software. It is licensed by the GNU Project and available under the GNU General Public License. It is written primarily in C, Fortran, and R itself. Precompiled executables are provided for various operating systems.\nAs an interpreted language, R has a native command line interface. Moreover, multiple third-party graphical user interfaces are available, such as RStudio\u2014an integrated development environment\u2014and Jupyter\u2014a notebook interface."
    },
    {
      "id": "46951032",
      "title": "Scientific Data (journal)",
      "url": "https://en.wikipedia.org/wiki/Scientific_Data_(journal)",
      "summary": "Scientific Data is a peer-reviewed open access scientific journal published by Nature Research since 2014. It focuses on descriptions of data sets relevant to the natural sciences, medicine, engineering and social sciences, which are provided as machine-readable data, complemented with a human oriented narrative. The journal was not the first to publish data papers, but is one of a few journals whose content consists primarily of data papers. The journal is abstracted and indexed by Index Medicus/MEDLINE/PubMed.\n\n"
    },
    {
      "id": "26833",
      "title": "Scientific method",
      "url": "https://en.wikipedia.org/wiki/Scientific_method",
      "summary": "The scientific method is an empirical method for acquiring knowledge that has characterized the development of science since at least the 17th century. (For notable practitioners in previous centuries, see history of scientific method.) \nThe scientific method involves careful observation coupled with rigorous scepticism, because cognitive assumptions can distort the interpretation of the observation. Scientific inquiry includes creating a hypothesis through inductive reasoning, testing it through experiments and statistical analysis, and adjusting or discarding the hypothesis based on the results. \nThe above mentioned are principles of the scientific method, a definitive series of steps applicable to all scientific enterprises.Although procedures vary from one field of inquiry to another, the underlying process is frequently the same. The process in the scientific method involves making conjectures (hypothetical explanations), deriving predictions from the hypotheses as logical consequences, and then carrying out experiments or empirical observations based on those predictions. A hypothesis is a conjecture based on knowledge obtained while seeking answers to the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments or studies. A scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.The purpose of an experiment is to determine whether observations  agree or disagree with hypothesis.Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always in the same order."
    },
    {
      "id": "29006",
      "title": "Space telescope",
      "url": "https://en.wikipedia.org/wiki/Space_telescope",
      "summary": "A space telescope or space observatory is a telescope in outer space used to observe astronomical objects. Suggested by Lyman Spitzer in 1946, the first operational telescopes were the American Orbiting Astronomical Observatory, OAO-2 launched in 1968, and the Soviet Orion 1 ultraviolet telescope aboard space station Salyut 1 in 1971. Space telescopes avoid the filtering and distortion (scintillation) of electromagnetic radiation which they observe, and avoid light pollution which ground-based observatories encounter. They are divided into two types: Satellites which map the entire sky (astronomical survey), and satellites which focus on selected astronomical objects or parts of the sky and beyond. Space telescopes are distinct from Earth imaging satellites, which point toward Earth for satellite imaging, applied for weather analysis, espionage, and other types of information gathering.\n\n"
    },
    {
      "id": "8151410",
      "title": "Thomas H. Davenport",
      "url": "https://en.wikipedia.org/wiki/Thomas_H._Davenport",
      "summary": "Thomas Hayes \"Tom\" Davenport, Jr. (born October 17, 1954) is an American academic and author specializing in analytics, business process innovation, knowledge management, and artificial intelligence. He is currently the President\u2019s Distinguished Professor in Information Technology and Management at Babson College, a Fellow of the MIT Initiative on the Digital Economy, Co-founder of the International Institute for Analytics, and a Senior Advisor to Deloitte Analytics.\nDavenport has written, coauthored, or edited twenty books, including the first books on analytical competition, business process reengineering and achieving value from enterprise systems, and the best seller, Working Knowledge (with Larry Prusak), on knowledge management. He has written more than one hundred articles for such publications as Harvard Business Review, MIT Sloan Management Review, California Management Review, the Financial Times, and many other publications. Davenport has also been a columnist for The Wall Street Journal, CIO, InformationWeek, and Forbes magazines.\nIn 2003, Davenport was named one of the world\u2019s 'Top 25 Consultants' by Consulting magazine, and in 2005 was named one of the world\u2019s top three analysts of business and technology by readers of Optimize magazine. In 2012 he was named one of the world's \"Top 50 Business School Professors\" by Poets and Quants and Fortune Magazine.\nOne of his most popular books (coauthored with Jeanne Harris), Competing on Analytics: The New Science of Winning, provides guidelines for basing competitive strategies on the analysis of business data, and highlights several firms that do so.\nOne of his sons, Hayes Davenport, is a television comedy writer and podcaster living in Los Angeles.\nHis other son, Chase Davenport, makes surfboards and researches artificial intelligence in San Francisco."
    },
    {
      "id": "35263738",
      "title": "Vasant Dhar",
      "url": "https://en.wikipedia.org/wiki/Vasant_Dhar",
      "summary": "Vasant Dhar is a professor at the Stern School of Business and the Center for Data Science at New York University, former editor-in-chief of the journal Big Data and the founder of SCT Capital, one of the first machine-learning-based hedge funds in New York City in the 1990s. His research focuses on building scalable decision-making systems from large sources of data using techniques and principles from the disciplines of artificial intelligence and machine learning.\n\n"
    },
    {
      "id": "6390647",
      "title": "Wide-field Infrared Survey Explorer",
      "url": "https://en.wikipedia.org/wiki/Wide-field_Infrared_Survey_Explorer",
      "summary": "Wide-field Infrared Survey Explorer (WISE, observatory code C51, Explorer 92 and SMEX-6) is a NASA infrared astronomy space telescope in the Explorers Program launched in December 2009. WISE discovered thousands of minor planets and numerous star clusters. Its observations also supported the discovery of the first Y-type brown dwarf and Earth trojan asteroid.\nWISE performed an all-sky astronomical survey with images in 3.4, 4.6, 12 and 22 \u03bcm wavelength range bands, over ten months using a 40 cm (16 in) diameter infrared telescope in Earth orbit.After its solid hydrogen coolant depleted, it was placed in hibernation mode in February 2011.\nIn 2013, NASA reactivated the WISE telescope to search for near-Earth objects (NEO), such as comets and asteroids, that could collide with Earth. \nThe reactivation mission was called Near-Earth Object Wide-field Infrared Survey Explorer (NEOWISE). \nAs of August 2023, NEOWISE is 40% through the 20th coverage of the full sky.\nScience operations and data processing for WISE and NEOWISE take place at the Infrared Processing and Analysis Center at the California Institute of Technology in Pasadena, California.\nThe WISE All-Sky (WISEA) data, including processed images, source catalogs and raw data, was released to the public on 14 March 2012, and is available at the Infrared Science Archive.The NEOWISE mission is expected to end by early-2025 and the satellite to reentry some time after."
    },
    {
      "id": "46374359",
      "title": "William S. Cleveland",
      "url": "https://en.wikipedia.org/wiki/William_S._Cleveland",
      "summary": "William Swain Cleveland II (born 1943) is an American computer scientist and Professor of Statistics and Professor of Computer Science at Purdue University, known for his work on data visualization, particularly on nonparametric regression and local regression.\n\n"
    },
    {
      "id": "70358320",
      "title": "Women in Data",
      "url": "https://en.wikipedia.org/wiki/Women_in_Data",
      "summary": "Women in Data is an organisation and movement that aims to empower women and support them through the various stages of their careers in data. \nAlthough women comprise about 50% of the United Kingdom (UK) population, only 20% of professionals in artificial intelligence and data in the UK are women.Underrepresentation of women in data science can result in serious issues and in some cases, e.g., automobile crashes, actual harm.\nWith a membership of over 25,000, the organisation seeks to improve the representation of women and girls in data and technology, address some of the key issues, and connect data professionals with partner companies.\n\n"
    }
  ],
  "links": [
    {
      "source": "455220",
      "target": "2381958"
    },
    {
      "source": "455220",
      "target": "639009"
    },
    {
      "source": "455220",
      "target": "775"
    },
    {
      "source": "455220",
      "target": "90451"
    },
    {
      "source": "455220",
      "target": "61928892"
    },
    {
      "source": "455220",
      "target": "42164234"
    },
    {
      "source": "455220",
      "target": "1164"
    },
    {
      "source": "455220",
      "target": "27051151"
    },
    {
      "source": "455220",
      "target": "1364506"
    },
    {
      "source": "455220",
      "target": "2371482"
    },
    {
      "source": "455220",
      "target": "168387"
    },
    {
      "source": "455220",
      "target": "1040299"
    },
    {
      "source": "455220",
      "target": "19541494"
    },
    {
      "source": "455220",
      "target": "5300"
    },
    {
      "source": "455220",
      "target": "7077"
    },
    {
      "source": "455220",
      "target": "5323"
    },
    {
      "source": "455220",
      "target": "7398"
    },
    {
      "source": "455220",
      "target": "2720954"
    },
    {
      "source": "455220",
      "target": "34296790"
    },
    {
      "source": "455220",
      "target": "46626475"
    },
    {
      "source": "455220",
      "target": "82871"
    },
    {
      "source": "455220",
      "target": "759422"
    },
    {
      "source": "455220",
      "target": "41961"
    },
    {
      "source": "455220",
      "target": "237536"
    },
    {
      "source": "455220",
      "target": "35458904"
    },
    {
      "source": "455220",
      "target": "7990"
    },
    {
      "source": "455220",
      "target": "8078610"
    },
    {
      "source": "455220",
      "target": "1040387"
    },
    {
      "source": "455220",
      "target": "8377"
    },
    {
      "source": "455220",
      "target": "1079396"
    },
    {
      "source": "455220",
      "target": "32472154"
    },
    {
      "source": "455220",
      "target": "204002"
    },
    {
      "source": "455220",
      "target": "19721986"
    },
    {
      "source": "455220",
      "target": "422994"
    },
    {
      "source": "455220",
      "target": "239516"
    },
    {
      "source": "455220",
      "target": "239516"
    },
    {
      "source": "455220",
      "target": "5603080"
    },
    {
      "source": "455220",
      "target": "1010280"
    },
    {
      "source": "455220",
      "target": "13777"
    },
    {
      "source": "455220",
      "target": "185529"
    },
    {
      "source": "455220",
      "target": "56938"
    },
    {
      "source": "455220",
      "target": "20276016"
    },
    {
      "source": "455220",
      "target": "58644759"
    },
    {
      "source": "455220",
      "target": "36674345"
    },
    {
      "source": "455220",
      "target": "146738"
    },
    {
      "source": "455220",
      "target": "1716320"
    },
    {
      "source": "455220",
      "target": "59252"
    },
    {
      "source": "455220",
      "target": "18831"
    },
    {
      "source": "455220",
      "target": "18933632"
    },
    {
      "source": "455220",
      "target": "19001"
    },
    {
      "source": "455220",
      "target": "175537"
    },
    {
      "source": "455220",
      "target": "37256799"
    },
    {
      "source": "455220",
      "target": "23968131"
    },
    {
      "source": "455220",
      "target": "2063278"
    },
    {
      "source": "455220",
      "target": "40572678"
    },
    {
      "source": "455220",
      "target": "189239"
    },
    {
      "source": "455220",
      "target": "2329992"
    },
    {
      "source": "455220",
      "target": "23862"
    },
    {
      "source": "455220",
      "target": "25873"
    },
    {
      "source": "455220",
      "target": "29414838"
    },
    {
      "source": "455220",
      "target": "48455863"
    },
    {
      "source": "455220",
      "target": "1393991"
    },
    {
      "source": "455220",
      "target": "69894"
    },
    {
      "source": "455220",
      "target": "455220"
    },
    {
      "source": "455220",
      "target": "3254510"
    },
    {
      "source": "455220",
      "target": "22135297"
    },
    {
      "source": "455220",
      "target": "5309"
    },
    {
      "source": "455220",
      "target": "27010"
    },
    {
      "source": "455220",
      "target": "7366298"
    },
    {
      "source": "455220",
      "target": "26685"
    },
    {
      "source": "455220",
      "target": "82871"
    },
    {
      "source": "455220",
      "target": "8286675"
    },
    {
      "source": "455220",
      "target": "12098689"
    },
    {
      "source": "455220",
      "target": "21101351"
    },
    {
      "source": "455220",
      "target": "651800"
    },
    {
      "source": "455220",
      "target": "1079396"
    },
    {
      "source": "455220",
      "target": "45554839"
    },
    {
      "source": "455220",
      "target": "82871"
    },
    {
      "source": "455220",
      "target": "23538754"
    },
    {
      "source": "455220",
      "target": "12506378"
    },
    {
      "source": "35458904",
      "target": "14924067"
    },
    {
      "source": "35458904",
      "target": "775"
    },
    {
      "source": "35458904",
      "target": "341988"
    },
    {
      "source": "35458904",
      "target": "1134"
    },
    {
      "source": "35458904",
      "target": "25745941"
    },
    {
      "source": "35458904",
      "target": "38751"
    },
    {
      "source": "35458904",
      "target": "1673865"
    },
    {
      "source": "35458904",
      "target": "30863191"
    },
    {
      "source": "35458904",
      "target": "20877649"
    },
    {
      "source": "35458904",
      "target": "27051151"
    },
    {
      "source": "35458904",
      "target": "24437894"
    },
    {
      "source": "35458904",
      "target": "30068"
    },
    {
      "source": "35458904",
      "target": "39206"
    },
    {
      "source": "35458904",
      "target": "158442"
    },
    {
      "source": "35458904",
      "target": "26880450"
    },
    {
      "source": "35458904",
      "target": "6310"
    },
    {
      "source": "35458904",
      "target": "63552467"
    },
    {
      "source": "35458904",
      "target": "7671"
    },
    {
      "source": "35458904",
      "target": "5177"
    },
    {
      "source": "35458904",
      "target": "37438"
    },
    {
      "source": "35458904",
      "target": "1181008"
    },
    {
      "source": "35458904",
      "target": "5311"
    },
    {
      "source": "35458904",
      "target": "5323"
    },
    {
      "source": "35458904",
      "target": "45443335"
    },
    {
      "source": "35458904",
      "target": "2234333"
    },
    {
      "source": "35458904",
      "target": "2720954"
    },
    {
      "source": "35458904",
      "target": "2720954"
    },
    {
      "source": "35458904",
      "target": "2588620"
    },
    {
      "source": "35458904",
      "target": "51443362"
    },
    {
      "source": "35458904",
      "target": "3575651"
    },
    {
      "source": "35458904",
      "target": "3575651"
    },
    {
      "source": "35458904",
      "target": "11501746"
    },
    {
      "source": "35458904",
      "target": "8013"
    },
    {
      "source": "35458904",
      "target": "1040512"
    },
    {
      "source": "35458904",
      "target": "23943140"
    },
    {
      "source": "35458904",
      "target": "372381"
    },
    {
      "source": "35458904",
      "target": "42906439"
    },
    {
      "source": "35458904",
      "target": "455220"
    },
    {
      "source": "35458904",
      "target": "12097860"
    },
    {
      "source": "35458904",
      "target": "2690471"
    },
    {
      "source": "35458904",
      "target": "9328883"
    },
    {
      "source": "35458904",
      "target": "7176679"
    },
    {
      "source": "35458904",
      "target": "4780372"
    },
    {
      "source": "35458904",
      "target": "40995"
    },
    {
      "source": "35458904",
      "target": "24810701"
    },
    {
      "source": "35458904",
      "target": "44783487"
    },
    {
      "source": "35458904",
      "target": "239516"
    },
    {
      "source": "35458904",
      "target": "1381282"
    },
    {
      "source": "35458904",
      "target": "759312"
    },
    {
      "source": "35458904",
      "target": "1135408"
    },
    {
      "source": "35458904",
      "target": "49882988"
    },
    {
      "source": "35458904",
      "target": "12386904"
    },
    {
      "source": "35458904",
      "target": "12386904"
    },
    {
      "source": "35458904",
      "target": "55810614"
    },
    {
      "source": "35458904",
      "target": "41961"
    },
    {
      "source": "35458904",
      "target": "42813835"
    },
    {
      "source": "35458904",
      "target": "1609808"
    },
    {
      "source": "35458904",
      "target": "2160183"
    },
    {
      "source": "35458904",
      "target": "23789529"
    },
    {
      "source": "35458904",
      "target": "3095080"
    },
    {
      "source": "35458904",
      "target": "46471245"
    },
    {
      "source": "35458904",
      "target": "3575656"
    },
    {
      "source": "35458904",
      "target": "1157832"
    },
    {
      "source": "35458904",
      "target": "8495"
    },
    {
      "source": "35458904",
      "target": "60798295"
    },
    {
      "source": "35458904",
      "target": "6212365"
    },
    {
      "source": "35458904",
      "target": "28174"
    },
    {
      "source": "35458904",
      "target": "7360695"
    },
    {
      "source": "35458904",
      "target": "4080917"
    },
    {
      "source": "35458904",
      "target": "1705399"
    },
    {
      "source": "35458904",
      "target": "3461736"
    },
    {
      "source": "35458904",
      "target": "7990"
    },
    {
      "source": "35458904",
      "target": "12487489"
    },
    {
      "source": "35458904",
      "target": "8377"
    },
    {
      "source": "35458904",
      "target": "265752"
    },
    {
      "source": "35458904",
      "target": "8501"
    },
    {
      "source": "35458904",
      "target": "422994"
    },
    {
      "source": "35458904",
      "target": "750101"
    },
    {
      "source": "35458904",
      "target": "9545"
    },
    {
      "source": "35458904",
      "target": "72826783"
    },
    {
      "source": "35458904",
      "target": "46363781"
    },
    {
      "source": "35458904",
      "target": "239516"
    },
    {
      "source": "35458904",
      "target": "46207323"
    },
    {
      "source": "35458904",
      "target": "294894"
    },
    {
      "source": "35458904",
      "target": "12799"
    },
    {
      "source": "35458904",
      "target": "1822696"
    },
    {
      "source": "35458904",
      "target": "23534602"
    },
    {
      "source": "35458904",
      "target": "14919"
    },
    {
      "source": "35458904",
      "target": "234930"
    },
    {
      "source": "35458904",
      "target": "2862975"
    },
    {
      "source": "35458904",
      "target": "14774"
    },
    {
      "source": "35458904",
      "target": "237536"
    },
    {
      "source": "35458904",
      "target": "149354"
    },
    {
      "source": "35458904",
      "target": "36674345"
    },
    {
      "source": "35458904",
      "target": "3461736"
    },
    {
      "source": "35458904",
      "target": "15201"
    },
    {
      "source": "35458904",
      "target": "15201"
    },
    {
      "source": "35458904",
      "target": "15201"
    },
    {
      "source": "35458904",
      "target": "68833"
    },
    {
      "source": "35458904",
      "target": "48996671"
    },
    {
      "source": "35458904",
      "target": "308367"
    },
    {
      "source": "35458904",
      "target": "31074852"
    },
    {
      "source": "35458904",
      "target": "243391"
    },
    {
      "source": "35458904",
      "target": "31002435"
    },
    {
      "source": "35458904",
      "target": "233488"
    },
    {
      "source": "35458904",
      "target": "18831"
    },
    {
      "source": "35458904",
      "target": "3606435"
    },
    {
      "source": "35458904",
      "target": "62500799"
    },
    {
      "source": "35458904",
      "target": "3156699"
    },
    {
      "source": "35458904",
      "target": "883885"
    },
    {
      "source": "35458904",
      "target": "503009"
    },
    {
      "source": "35458904",
      "target": "58731"
    },
    {
      "source": "35458904",
      "target": "60931"
    },
    {
      "source": "35458904",
      "target": "1467948"
    },
    {
      "source": "35458904",
      "target": "23862"
    },
    {
      "source": "35458904",
      "target": "376707"
    },
    {
      "source": "35458904",
      "target": "48455863"
    },
    {
      "source": "35458904",
      "target": "46951032"
    },
    {
      "source": "35458904",
      "target": "1181008"
    },
    {
      "source": "35458904",
      "target": "26833"
    },
    {
      "source": "35458904",
      "target": "29006"
    },
    {
      "source": "35458904",
      "target": "26685"
    },
    {
      "source": "35458904",
      "target": "26685"
    },
    {
      "source": "35458904",
      "target": "82871"
    },
    {
      "source": "35458904",
      "target": "8151410"
    },
    {
      "source": "35458904",
      "target": "82871"
    },
    {
      "source": "35458904",
      "target": "35263738"
    },
    {
      "source": "35458904",
      "target": "6390647"
    },
    {
      "source": "35458904",
      "target": "46374359"
    },
    {
      "source": "35458904",
      "target": "70358320"
    }
  ]
}